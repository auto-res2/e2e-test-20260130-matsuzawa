{
  "research_topic": "Proposing an improved version of the Chain-of-Thought based on human thinking methods",
  "queries": [
    "human-like reasoning CoT",
    "cognitive process prompting",
    "thought scaffolding LLM"
  ],
  "research_study_list": [
    {
      "title": "CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language",
      "full_text": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language Kevin Ellis Cornell University kellis@cornell.edu Abstract A core tension in models of concept learning is that the model must carefully balance the tractability of inference against the expressivity of the hypothesis class. Humans, however, can efficiently learn a broad range of concepts. We introduce a model of inductive learning that seeks to be human-like in that sense. It implements a Bayesian reasoning process where a language model first proposes candidate hypotheses expressed in natural language, which are then re-weighed by a prior and a likelihood. By estimating the prior from human data, we can predict human judgments on learning problems involving numbers and sets, spanning concepts that are generative, discriminative, propositional, and higher-order. 1 Introduction Human learning is rapid and broad. Consider a child learning a new routine like ‘high-five’: given just 1 or 2 examples, they can learn that move, and even generalize to variations like low-fives. Or consider learning the basics of a game like Pacman, or an AI researcher learning a new technique like ‘few-shot prompting’, or a mathematician extrapolating ‘1, 4, 16, 64, ...’. In each of these cases, the relevant routine, rules, or concept can be learned from little data and only seconds to minutes of experience. Furthermore, the space of possible concepts is effectively infinite, because concepts can compose to yield more complex constructs like ‘high-five followed by a fist bump’. Thus a key computational challenge is to understand how an intelligent system can acquire a wide range of new concepts, given modest data, and granted a modest computational budget. Building AI systems that can efficiently master many concepts is also practically valuable, because data-efficiency and broad generalization remain some of the most salient gaps between human and machine intelligence [1]. Fundamentally, we are concerned with the problem of induction: Inferring a generalizable pattern, rule, trend, or law from specific examples. A classic approach to induction is to start with a hypothesis space of possible concepts, and then probabilistically infer which hypothesis most likely generated the observed data using Bayes’ Rule. This Bayesian paradigm has proved widely applicable across both cognitive science and artificial intelligence [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. On its own, however, the Bayesian paradigm leaves important questions unanswered. Acquiring a broad range of possible concepts demands an expressive hypothesis space, but inference over a rich space of concepts comes at steep computational cost. For increased expressivity, many Bayesian models design hypothesis spaces that resemble a programming language [15, 16, 17, 18, 19, 20]. Posterior inference then corresponds to constructing high-probability programs. Each of these program-learning models requires a custom domain-specific programming language. Despite confining themselves to a domain-specific language, these models still require specialized inference machinery, such as heuristically-designed search moves [10], or exorbitant compute budgets [21]. Our goal is to build a model of humanlike concept learning that makes progress toward resolving the tension between intractable inference and expressive hypothesis classes. We propose a new model 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.02797v3  [cs.CL]  29 Sep 2023that expresses its concepts in natural language–even when the learning problem does not involve natural language. We do this for two reasons. First, language is an effective representation for many human concepts. It is compositional, richly expressive, and regularizes the learner toward natural generalizations. Second, we find we can efficiently infer natural language concepts using modern large language models [22, 23] based on the transformer architecture [24]. Like any Bayesian model, we will first define a prior over concepts, which in our case exerts a top-down pressure for naturally-structured language. Our model also has a bottom-up mechanism for efficiently inferring possible hypotheses, analogous to a recognition network [25]. The interaction of these top-down and bottom-up models is surprisingly effective as a model of humanlike learning: Given around 50 samples from the recognition network, our model can account for human patterns of generalization for concepts that are generative or discriminative, and propositional or first-order. We show the model can also capture fine-grained structure in human judgements: both subtle gradations of uncertainty, and also the dynamics of learning starting from a few examples and going to dozens of examples. Finally, a key reason why humans can learn so efficiently is because they have a good inductive bias or prior [26, 27]. Our model can fine-tune its prior to human judgements, effectively extracting a human-like prior from behavioral data. We find this gives a more faithful model of human generalization, and also improves average accuracy on concept-learning tasks. We focus on abstract symbolic concepts, such as ‘prime numbers less than 30’, which we think are well-suited to natural language. We do not consider concepts grounded in perception and actuation, such as ‘dog’ or ‘chewing’, and hence do not provide a single unified account of inductive learning. However, for these abstract concepts, we provide a rational process model [28]. Following the Marr levels of analysis [29], this means we propose algorithmic mechanisms for concept learning that rationally approximate optimal inference, subject to bounds on computation (sampling). This contrasts with computational-level models, which characterize the goal of the learner, without committing to a theory of how the learner mechanistically accomplishes that goal [ 29]. Most Bayesian concept learning models operate at the computational level, avoiding issues of intractability [3, 15] (cf. [30]). We contribute (1) a model of symbolic concept learning that supports efficient inference over a flexible hypothesis class; (2) an evaluation on human data from two different concept learning experiments; and (3) a simple recipe for extracting a humanlike prior over concepts, given raw behavioral data. 2 Background and Related Work Bayesian Program Learning (BPL: [10]) models concept learning as Bayesian inference over latent symbolic programs. BPL models first define a domain-specific programming language spanning a space of possible concepts, and then infer a posterior over concepts C given training data D via Bayes’ Rule: p(C∣D) ∝p(D∣C)p(C). Although the set of possible concepts remains hardcoded, the prior p(C) can be learned through hierarchical Bayesian methods. Given parameters θ indexing possible priors, and a collection of datasets D, the prior can be learned by approximately solving [21]: θ∗ =arg max θ p(θ∣D) =arg max θ p(θ) ∏ D∈D ∑ C p(D∣C)pθ(C) Bayesian models can learn from few examples, because the prior regularizes them toward reasonable generalizations. They also produce nuanced uncertainty estimates, because they represent the posterior. Program learners can also acquire any concept expressible in their domain-specific language, but this language must be appropriately circumscribed for inference to remain tractable. Systems such as DreamCoder [21] partly address this concern by growing the language and training neural networks to aid inference, but even then, general-purpose programming languages remain out of scope. We next turn to latent language, which considers the broad hypothesis space of all natural language. Latent Language. Using language as an internal representation for nonlinguistic tasks was introduced by the Latent Language framework [31, 32]. These systems perform few-shot learning by searching for the language which minimizes a loss on the training examples. Given training input-output examples {(x, y)}, the latent language approach infers the language C∗ minimizing C∗ =arg min C∈Σ∗ ∑ (x,y) Loss(y, fθ(x; C)) where fθ is a neural network and Σ∗ is the set of all strings. Because there are infinitely many strings, another neural network samples a finite pool of candidate concepts. Relative to Bayesian learners, 2latent language models use maximum likelihood estimation to infer a single concept, rather than construct a posterior distribution. Like our approach, latent language uses language as an intermediate representation, combined with a bottom-up concept proposal process. Our work adds additional Bayesian mechanics, and shows how to learn a prior on C from human judgments. Learning the prior proves important for both modeling human judgments and achieving high task performance. Induction, Abduction, and Deduction. We address inductive problems: inferring a general pattern from specific examples [ 33]. Abduction is a related process where the reasoner infers an explanation for a specific observation. Abductive reasoning using modern language models has received much recent attention [34, 35], which has solidified natural language as a promising candidate for representing abductive explanations. Deduction—logically inferring the truth of a proposition—has also have been similarly revisited in the context of modern language models [36, 37]. 3 Model We start with a basic Bayesian approach. A latent conceptC generates K observed examples, notated X1, . . . , XK, according to an IID process. We abbreviate X1, . . . , XK as X1∶K. For our model, C is an utterance in natural language. The learner’s posterior beliefs are given by Bayes’s Rule, p(C∣X1∶K) ∝p(C) ∏ 1≤k≤K p(Xk∣C) (1) The prior p(C) comes from a neural model. The likelihood p(X∣C) is domain-specific because it depends on the structure of the examples. We assume the posterior in Eq. 1 is intractable. We model tasks that do not involve externalized language, thus beliefs overC play a fundamentally auxiliary role. We instead care about the probability that a test example Xtest belongs to the same concept as the training examples X1∶K. This posterior predictive quantity is p(Xtest ∈C∣X1∶K) =∑ C p(C∣X1∶K)1[Xtest ∈C] (2) To make the above tractable, we introduce a proposal distribution q(C∣X1∶K). We draw from q a modest number of sampled concepts (tens to hundreds), writing those samples as C(1), . . . , C(S). By only considering concepts proposed by q, the infinite sum over C in Eq. 2 becomes a finite sum over S samples. Provided those proposals account for much of the posterior mass, this is a good approximation. Conventionally, q is used to construct an importance sampler [38]: p(Xtest ∈C∣X1∶K) = E C∼p(⋅∣X1∶K) 1[Xtest ∈C] ≈ ∑ 1≤s≤S w(s)1[Xtest ∈C(s)], where w(s) = ˜w(s) ∑s′ ˜w(s) and ˜w(s) = p(C(s))p(X1∶K∣C(s)) q(C(s)∣X1∶K) (3) The above Monte Carlo estimate requires evaluating q(C(s)∣X1∶K). The most powerful proposal dis- tributions at our disposal, such as GPT-4 [22], do not expose this functionality, so we heuristically ap- proximate importance sampling by deduplicating the samples and weighing each byp(C)p(X1∶K∣C): p(Xtest ∈C∣X1∶K) ≈ ∑ C∈{C(1),...,C(S)} w(C)1[Xtest ∈C], where w(C) = ˜w(C) ∑C′ ˜w(C′) and ˜w(C) =p(C)p(X1∶K∣C)1[C ∈{C(1), . . . , C(S)}] (4) Ultimately, the distribution p(C) should reflect the prior beliefs of human learners. To tune the prior to reflect human patterns of generalization, we assume access to a dataset of human judgments consisting of triples (X1∶K, Xtest, r), where r is the ratio of humans who judged Xtest as belonging to the same concept as X1∶K. More generally, r could be any average human rating in [0, 1]. If θ parametrizes the prior, then we match the prior to the human data by solving arg max θ ∑ (X1∶K,Xtest,r) r log pθ(Xtest ∈C∣X1∶K)+(1 −r)log (1 −pθ(Xtest ∈C∣X1∶K)) (5) where pθ(Xtest ∈C∣X1∶K) =∑ C 1[Xtest ∈C]pθ(C∣X1∶K), approximated via Eq. 3-4 As a process model, the approach claims that a bottom-up generator proposes a bounded number of candidate concepts, which are re-weighed by re-examining each datapoint, while being biased by a prior. This can be seen as a dual-process model [39], as well as a model of bounded rationality [40]. 3Comparing hypotheses to data. The likelihood p(X∣C) and the prediction 1[X ∈C] both require comparing natural language C against a nonlinguistic datumX. For the domains we consider it is con- venient to implement this comparison by translating the conceptC into Python using an LLM—which we do deterministically, via greedy decoding—and then execute the Python program onX, similar to [41, 42]. We do not view this latent program as essential to the general framework: Alternatively, another neural network could serve to link X to C, which would allow fuzzier comparisons between hypotheses and data. Fig. 1 diagrams these details as a Bayesian network. Concept C Program Data Xk proposal q k ∈[K] Figure 1: Model as Bayesian network. The program is a deterministic function of C. 4 The Number Game The Number Game is a few-shot concept learning setup covered by classic textbooks and disserta- tions [5, 43]. Participants playing The Number Game are given a few example numbers belonging to a hidden concept, and then rate how likely it is that other numbers also belong to the same concept. Given just the example number 16, the concept could be ‘square numbers’, ‘powers of two’, ‘evens’, ‘odds but also 16’, ‘97 and 16’, ‘numbers ending in 6’, or infinitely many other concepts. With more examples such as 16, 8, 2, 64 , humans consistently rate powers of two as almost certainly in the concept, but gradations of uncertainty remain: other evens like 24 could plausibly belong in the concept, but humans rate odds like 23 as extremely unlikely. Examining human judgments for these and other concepts reveal a variety of belief states, including sharp, all-or-none concepts like ‘powers of two,’ but also soft graded concepts like ‘numbers around 20’ (Fig. 2, blue bars). Human Data. We take human data from [43]. Eight human participants rated test numbers on a scale of 1-7, given different training sets of example numbers. We model the average human rating for each test number, on each training set of example numbers. 0 10 20 30 40 50 60 70 80 90 100 0 1 between/uni00A030/uni00A0and/uni00A045 between/uni00A015/uni00A0and/uni00A020 a/uni00A0power/uni00A0of/uni00A02 a/uni00A0multiple/uni00A0of/uni00A016 less/uni00A0than/uni00A020 training/uni00A0examples:/uni00A016 0 10 20 30 40 50 60 70 80 90 100 0 1prob./uni00A0test/uni00A0is/uni00A0in/uni00A0concept divisible/uni00A0by/uni00A08 power/uni00A0of/uni00A02 evenly/uni00A0divisible/uni00A0by/uni00A02 a/uni00A0power/uni00A0of/uni00A0two a/uni00A0power/uni00A0of/uni00A02. training/uni00A0examples:/uni00A016,/uni00A08,/uni00A02,/uni00A064 0 10 20 30 40 50 60 70 80 90 100 test/uni00A0number 0 1 less/uni00A0than/uni00A030. between/uni00A015/uni00A0and/uni00A030 between/uni00A016/uni00A0and/uni00A023 between/uni00A015/uni00A0and/uni00A025 greater/uni00A0than/uni00A015/uni00A0and/uni00A0le... training/uni00A0examples:/uni00A016,/uni00A023,/uni00A019,/uni00A020 model human Figure 2: Number Game human judgments (blue bars) for different example data. For instance, the top plot shows that after seeing that 16 belongs to the concept, humans rate 64 as around 50% likely to belong to that same concept. Bars at zero correspond to missing data. Orange curves show our model’s predictions. The text to the right of each plot shows 5 samples from our model’s approximate posterior after proposing 100 concepts using q. Human data from [43]. See also Appendix Fig. 7 4Prior. We consider two different prior distributions. The pretrained prior scores the log likelihood of each concept C using an open source language model, specifically, CodeGen 350M [ 44]. The tuned prior first extracts features of C using a pretrained sentence feature extractor ϕ, specifically all-MiniLM-L6 [45], which outputs a 384-dimensional feature vector. The tuned prior maps those features to an unnormalized log probability via a linear layer with parameters θ: Tuned prior: pθ(C) ∝exp (θ ⋅ϕ (C)) (6) Likelihood. Evaluating p(X∣C) requires first determining which numbers belong to the concept C. To efficiently enumerate those numbers, we translate C from natural language to Python using Codex code-davinci-002 [23], a language model trained on source code. We run the Python code on the numbers 1..100 to determine the members of C. Given the members of C, we assume numbers are sampled uniformly from C with probability (1 −ϵ), and otherwise sampled uniformly from 1..100: p(X∣C) =(1 −ϵ)1[X ∈C] ∣C∣ +ϵ 1 100 (7) Parameter fitting. We want a single parameter setting that works for all of the training example sets, so we fit the above parameters to the average human judgment for each test number, and for each set of examples. When comparing model predictions against the average human judgment on a particular test number, we always holdout that particular human data from the parameter fitting. We use Adam [46] to perform maximum likelihood estimation of the parameters, following Eq. 5. Temperature, Platt transform. Because human subjects rated on a scale of 1-7, we introduce a learnable Platt transform between the model’s predicted probabilities and the human judgments [47]. We also place a learnable temperature parameter on the posterior. Proposal distribution. We implement q using Codex code-davinci-002 [23]. We prompt Codex by adapting the cover story given to the human subjects, then append the training example numbers X1∶K and have it complete the natural language description of the hidden concept. Alternative models. We compare against GPT-4 to understand the few-shot learning abilities of a bleeding-edge large language model. Prompted with example numbers belonging to a concept, together with a test number, we measure the probability that GPT-4 responds “yes” to the test number belonging to the concept, vs responding “no”. We fit a Platt transform to those probabilities. We also compare against DreamCoder [21], a recent Bayesian Program Learning system. Starting with a Number Game DSL, DreamCoder learns a prior and trains a neural proposal distribution. DreamCoder considers 104 hypotheses at test time—two orders of magnitude more than our model—and trains its neural network on 105 synthetic number concepts (“dreams”). We further contrast against Latent Language [31], using the same Codex-based proposal and likelihood distributions, and the same learnable Platt transform. Finally, we consider versions of our model that encode hypotheses in Python instead of natural language (‘code prior’), as well as a version of our model that ablates the proposal distribution by generating concepts unconditioned on X1∶K (‘no proposal dist.’). Results. Fig. 3 shows that an out-of-the-box pretrained natural language prior offers a decent fit to the human data after proposing 50-100 hypotheses. Our tuned prior achieves a very close fit to the human data, again using 50-100 samples. Switching from English to Python significantly degrades model fit, even when the Python prior is learned (‘tuned code prior’). Ablating the proposal distribution–sampling hypotheses from the prior–also provides a poor fit: the space of possible number hypotheses is too vast to be randomly guessed without looking at the data. These results establish that a language-generating proposal distribution can support efficient inference, and accurately model the average of a small pool of human participants. Recall that we are modeling the average of 8 human judgements, and a good fit to this average requires 50-100 samples. Therefore, our model suggests that each human might only need to draw a couple dozen samples, which we think is psychologically plausible (and practical, from an engineering perspective). In contrast, the original Number Game model considered over 5000 hypotheses [43], and a typical BPL system such as DreamCoder still lags our model, even when it considers orders of magnitude more hypotheses. Last, GPT-4’s judgments are decidedly different from humans. This does not mean that GPT-4 is wrong in its predictions: there is no ground-truth for whether the number 30 should belong to the same concept as the number 60. To better understand how humans and models stack up against each other, we next consider a richer domain where accuracy can be more objectively measured. 5100 101 102 num/uni00A0samples 0.00 0.25 0.50 0.75 1.00model/uni00ADhuman/uni00A0response/uni00A0R/uni00B2 tuned/uni00A0prior pretrained/uni00A0prior tuned/uni00A0code/uni00A0prior no/uni00A0proposal/uni00A0dist. latent/uni00A0language DreamCoder GPT/uni00AD4 0.0 0.5 1.0 model/uni00A0prediction 0.00 0.25 0.50 0.75 1.00human/uni00A0rating R/uni00B2=0.95 tuned/uni00A0prior,/uni00A0100/uni00A0samples Figure 3: How well different models predict held-out human judgments (R2), as a function of the sampling budget (left panel, X-axis, log scale). Error bars: ±SEM over 3 runs with different seeds. Variance across runs decreases with number of samples. See Appendix Fig. 8-9 for further results. 5 Logical Concepts We next consider concepts with more complex logical structure. Consider a concept such asbachelor, defined as “unmarried man”, or Valedictorian, defined as “the person, within a single school, with the highest GPA”. Using the primitives of propositional logic, we can definebachelor: (Male∧¬Married). Using the more expressive language of first-order logic, which includes quantifiers, we can define valedictorian as Valedictorian(x) ⇐⇒(∀y ∶School(x) = School(y) /Leftr⫯g⊸tl⫯ne⇒GPA(x) ≥ GPA(y)). Discovering the discrete logical structure that best explains a dataset of examples is a well-known AI challenge [48, 49, 50]. Within the cognitive sciences, understanding how people come to grasp logical relations has been proposed to be a key component of understanding how people comprehend number systems, geometry, causal processes, social and kinship relations, and other domains [51, 52, 53, 54]. For our modeling, we consider an online learning setup from [55] where a learner observes a stream of examples of a unknown logical concept. On each example, the learner observes a fresh batch of 1-5 objects, and must pick out which objects belong to the hidden concept. The learner then gets feedback on which objects in the batch actually belonged to the concept, and then the process repeats for a new batch. Fig. 4 illustrates this experimental setup: each object is a shape defined by its size (small, medium, large), color (green, yellow, blue), and shape (triangle, rectangle, circle). Recording each human response to each shape on each batch gives a fine-grained learning curve capturing how learning unfolds over dozens of examples. These learning curves signal what concepts people readily learn, what patterns of mistakes they make, and what concepts remain essentially unlearnable from even dozens of examples. We obtain this human data from [55], which covers 112 concepts, collecting judgements from 1,596 human participants as they attempt to learn each concept over 25 batches of examples. These 25 batches corresponds to ≈75 examples/concept, and each concept run on ≈20 participants. Most human learning happens over the first dozen batches, so we take the first 15/25 batches, which conveniently also nearly halves the cost and time of running the model. Figure 4: Concept learning experiment from [55] (illustration used with permission). On each batch, participants label which shapes they think belong to a new concept (called wudsy). Previous batches are shown with the ground truth positive examples surrounded by a square. From these examples, participants might infer a simple concept like “green triangles”, and select the second test object. 6Model. Our modeling approach is similar to The Number Game, except we now have a discriminative learning problem instead of a generative one, and an online learning setup where the learner observes a stream of examples. To model online learning, we draw fresh proposals from q for each training batch, and perform Bayesian inference over all proposals drawn so far. To model discriminative learning, each example is now a triple (B, T, Y), where B is a batch of shapes, T is a test shape in that batch, and Y is zero or one depending on whether T in B is an example of the concept. Our likelihood model assumes human subjects predict according to C with probability (1 −ϵ) and otherwise predict randomly with a base rate α of labeling an example as positive. Following [55, 56] we model a memory decay process where the relative importance of earlier observations falls off according to a power law with parameter β: p(Y =1∣B, T, C) =(1 −ϵ)1[(B, T) ∈C]+ϵα (8) log p(X1∶K∣C) = ∑ (Bk,Tk,Yk)∈X1∶K (1 +K −k)−β log p(Y ∣B, T, C) (9) As before, we translate hypotheses from natural language into Python using code-davinci-002, and evaluate the likelihood term above by running Python code. We consider both pretrained and tuned priors. Our proposal distribution q comes from running GPT-4 on prompts that illustrate previous batches, either as truth tables (for propositional concepts) or as a raw list of previous observed batches (for higher-order concepts). We again place a learnable temperature on the posterior. Bayesian Program Learning Baseline (BPL). We contrast with a strong BPL baseline. It uses a grammar over expressions in first-order logic, plus predicates for shape, color, and size, totaling 28 primitives that were selected by the creator of the logical concept learning dataset (A.3.3). The BPL baseline uses the same memory-decay likelihood model, and fits (tunes) its prior by estimating probabilities for each logical primitive. It is implemented in Fleet [ 57], the state-of-the-art in fast parallelized MCMC over grammatically structured hypothesis spaces. Our model differs in two important ways. First, the baseline is given first-order primitives that were chosen specifically for this dataset. While our model can use natural language expressing first-order concepts (e.g., the only for ∃!), it can also express concepts like objects with the least common color that are unrepresentable by the baseline, and which are not in the dataset. The second difference is that our model supports efficient inference via bottom-up proposals, while this baseline performs a stochastic search over hypotheses (MCMC), requiring many more samples. It takes 106 Metropolis-Hastings proposals per batch, and per learning curve, totaling ≈109 proposals, which are deduplicated to yield ≈45, 000 unique concepts, which provide the support of subsequent posterior inference. This means every BPL posterior is informed by the total ≈109 sampling moves. Results. Our model’s predictions generally align well with human judgments (Fig. 5). Using 100 proposals per batch, our model explains 81% of the variance in human responses (R2 =.81), which is much higher than GPT-4 on its own. The model is also more accurate at the actual task than GPT-4, and within 3% of human-level (Fig. 6C). The model also fits the human data somewhat better than the BPL baseline, even when that baseline is granted an exorbitant sampling budget. Tuning the prior proves critical, possibly because these logical concepts are more complex than the earlier number concepts, and so can be expressed in a wider variety of syntactic forms. The pretrained model is highly sensitive to syntax and cannot learn to attend to semantic features, unlike the tuned model. 100 101 102 num samples per batch 0.2 0.4 0.6 0.8model-human response R² tuned prior pretrained prior no proposal dist. latent language GPT-4 BPL (10 -10  samples) 0.0 0.5 1.0 model/uni00A0response 0.0 0.5 1.0human/uni00A0response R/uni00B2=0.81 tuned/uni00A0prior,/uni00A0100/uni00A0samples Figure 5: Model fits on holdout data. Error bars: ±SEM over 3 runs. (Error bars often close to zero) 7Although our model explains most of the variation in the human responses, nontrivial variation remains. One possible reason is that we are modeling the responses of many human subjects, and different subject groups per concept, so it is difficult to capture the full variability of the human responses: Building a model that accurately reflects the judgments of a population of humans may be much more difficult, and require a higher sampling budget, than building a model of a single person. The model also slightly underperforms humans, and so a higher fidelity model might come from simply performing the task better. (Although one can slightly surpass human accuracy by optimizing purely for task performance, doing so degrades model fit. See Fig. 6C, ‘tune for accuracy’.) More fundamentally, the data come from many successive trials, so they likely contain memory and order effects such as garden-pathing [58] and anchoring [39] which are not well accounted for by the pure probabilistic inference our model approximates, nor by our simple memory decay model. At the same time, our model does account for many fine-grained details in the behavioral data, including predicting specific patterns of successes and failures (Fig. 6A). Because our approach is manifestly interpretable—it explicitly verbalizes its hypotheses in human language—we can inspect its maximum a posteriori concept at the end of each learning episode, and observe that its successes typically occur because its verbalization of the concept describes the correct first-order law. Conversely, when humans make highly selective failures, we can probe the model to suggest what alternative hypotheses humans may have incorrectly inferred (Fig. 6A, top right). 0 10 20 30 40 response number 0.0 0.5 1.0accuracy x.shape = circle  x.color blue a circle (unless it is blue). human tuned prior 0 10 20 30 40 50 response number y S. y.shape=x.shape  x.size y.size  it is the largest object with its shape in the example 0 10 20 30 40 response number x.color = blue  y S. x.size y.size  it is the largest blue object in the example A 0 10 20 30 response number 0.0 0.5 1.0accuracy y S. x.size > y.size  it is larger than the smallest object in the example 0 10 20 30 40 response number y S. (x y  y.shape = x.shape)  it shares the same shape as another object in the example 0 10 20 30 40 50 response number y S. (x y  y.color = x.color)  the total number of objects with the same color is odd B human tune prior pretrain prior GPT-4 tune for accuracy 0.5 0.6 0.7 0.8average accuracy 0.74 0.71 0.68 0.60 0.79 R²=0.74 C 0 20 40 60 80 response number 0.0 0.5 1.0accuracy  the total number of objects with its color is greater than the total number of objects with any other color BPL 0 20 40 60 80 response number  the total number of objects of its color is less than or equal to the total number of objects of any other color D Figure 6: A. Left and middle: learning concepts isomorphic to bachelor and valedictorian. Dark ticks delimit batches. Above each plot is the ground-truth logical expression and the predicted natural language description. Right: the model can explain human mistakes by verbalizing what other concept humans might have been misled into learning. Both humans and the model seem to have learned largest blue instead of the largest and it also happens to be blue. B. Further learning curves, including somewhat worse fits, and getting the right answer despite having a slightly odd solution (rightmost). C. The model is close to human performance after extracting a prior from human judgments. D. New concepts run on new participants. All results on holdout learning curves. 8Modeling new concepts. To test the flexibility of our model, we created two new concepts for evaluating both the model and a new group of human participants. These two concepts were shapes with the majority color, and shapes with the least common color. 16 human subjects participated in an IRB-approved study (A.2). Our study finds that humans rapidly learn these concepts (Fig. 6D.) We also test our model on these new concepts, but using the prior estimated earlier on the data in [59]. Our model correctly predicts that humans will learn the concept of majority color after just 2 batches of examples. It predicts learning minority color after 4 batches, while humans need just 3, but the primary result—that this concept is very learnable—holds for both humans and the model. Although these two new concepts are easy to explain in words, and could be expressed in first-order logic with the right primitives—set cardinality and quantification over colors—neither concepts are learnable by the BPL baseline. This is because both concepts are simply unrepresentable: despite being equipped with 28 primitives, those primitives were designed without anticipating these new concepts. This highlights the fact that it is difficult to preprogram a sufficiently broad set of primitives that can efficiently encode all the different concepts people might learn. 6 Discussion Putting humanlike inductive biases into machines. Humans excel at rapidly mastering new tasks and understanding new concepts in part because they have a good inductive bias or prior [60, 61]. Imparting similar priors upon machines is therefore an important problem. Our work gives a recipe for training such a prior by fitting it directly to human judgments, marginalizing out the natural language, meaning we never need to elicit natural language from human subjects. Because our approach simply tunes a small network on top of a large open-source pretrained model, and because it can be fit to raw human judgments, we hope that it can serve as a broadly applicable engineering strategy for extracting and using human priors. Recent work has also explored complimentary strategies for instilling a humanlike prior upon a machine learning system. For example, Kumar et al. 2022 [ 9] show that training neural networks with auxiliary linguistic tasks, and with auxiliary programming tasks, causes their representations to better align with human priors. Bayesian Program Learning (BPL). Our model has similar motivations to BPL. Because we translate the natural language into Python to compute the likelihood, it is possible to see our approach as BPL with an unusual prior: p(program) = ∑NL p(NL)p(program∣NL). In that sense, our work provides a new prior for BPL, together with the demonstration that it is possible and practical to do BPL over a Turing-complete language like Python. Relatedly, recent BPL modeling has found synergies between natural language and program representations for cognitive AI models [9, 62]. Beyond BPL, combining probabilistic reasoning with expressive symbolic representations has long been an appealing paradigm [63, 64, 65], although the expressivity of the symbolic language must be balanced against the tractability of inference [66, 67]. Guiding inference with a neural model is a natural choice, but this is hard because of the lack of natural training data (though synthetic data can help: [68, 69, 70]). Encoding knowledge in natural language allows pretrained neural models to guide inference, and it could be fruitful to examine statistical-relational AI [71] in light of that fact. Large Language Models. Our work suggests that an out-of-the-box large language model is not an effective approach to inductive reasoning, at least on its own. Bayesian mechanics are needed to dampen the unpredictability of the language model. To the extent that being Bayesian is a normative account of rational behavior, our work offers a framework for enabling language models to draw more rational inferences, and ultimately generalize in more predictable and human-like ways by tuning their prior beliefs to match human data. The Language of Thought. Our work connects to the Language of Thought Hypothesis [ 72], which says that human learning and thinking relies on an inner symbolic language. This has been a productive framework for computational modeling [ 15, 16, 18, 73]. In its most literal forms, language-of-thought models are afflicted by the curse of a compositional mind (Spelke 2022 [74]): the free-form recombination of concepts yields a combinatorial explosion, which here we address by using pretrained knowledge from language to guide the learner. Whatever the true Language of Thought looks like, however, it must have a wide array of composable basic concepts in order to explain the breadth, flexibility, and generality of human thinking. Natural language, even if it is not actually the same as our inner mental language, acts as a vast reservoir of human concepts, 9and provides a flexible algebra for combining them. Therefore a reasonable near-term strategy for modeling human thinking may be to use natural language as a heuristic approximation to an inner Language of Thought, despite strong evidence that human thinking need not engage language- processing brain regions [75]. Rational Process Modeling. As a rational process model, our account of concept learning bridges the computational and algorithmic levels of the Marr hierarchy [28, 29]: We commit to a Bayesian computational-level theory, and a particular Monte Carlo algorithm as a rational approximation to that theory. One important sense in which our account is inadequate is that we do not actually explain how the bottom-up process works, or how it came to be learned. We merely require that it is stochastic and unreliable, but occasionally correct enough to not need many millions of proposals. Given those requirements, a modern large language model is a reasonable surrogate for this bottom-up process, even if it its inner workings might differ greatly from human bottom-up proposal processes. Generalizability of the theoretical framework. The basics of the model make few commitments, yet instantiating it requires selecting specific language models, engineering prompts and likelihoods, etc. More broadly, a high-resolution cognitive model, particularly a structured Bayesian one, requires domain-specific modeling choices. How much credit should we assign to the general theoretical framing, as opposed to particular engineering decisions? Although our paradigm introduces new degrees of freedom, such as which LLMs/prompts to use, it removes others, such as the grammatical structure of the symbolic hypothesis space. On balance, we are cautiously optimistic that the framework will generalize with less domain-specific tinkering, at least for abstract symbolic domains, because it replaces hand-designed symbolic hypothesis spaces with pretrained neural models, and because reasonable “default” neural networks worked well across our experiments. Bayes, Natural Language, and Recursive Reasoning. There is a rich literature on natural language and Bayesian inference from a social or communicative perspective, such as modeling pragmatic inference [76]. These models often use a recursive reasoning formulation where each agent represents the beliefs of other agents, including beliefs about themselves, and so on, recursively. Our work does not consider the communicative function of natural language, and so does not engage with these issues. However, models of pedagogical inductive learning—where a helpful teacher selects the training examples—benefit from similar forms of recursive reasoning, because the teacher and learner must reason about each other’s beliefs [77]. These insights could be important for building versions of our model that might simulate aspects of how humans learn from each other. Limitations. Our model performs induction via discrete structure learning. Given the combinatorial difficulty of structure learning, it is unclear whether our approach can scale to inferring complex systems of symbolic rules. We believe recent work on iteratively refining language model outputs may be promising here [78, 79]. The present form of our model is also limited to processing discrete symbolic input-outputs. Actual human thinking connects with the messy perceptual world. It would be valuable to understand whether this limitation can be addressed using multimodal language models [80], or approaches that interface separate language and vision modules [81, 82]. It is worth noting that BPL models can straightforwardly interoperate with perceptional data [10, 16, 18], and that many outstanding challenge problems within AI have at their core a perceptual-reasoning process, such as Bongard problems [83] and the Abstraction and Reasoning Corpus [1]. Currently, the model relies on costly, energy-intensive models for its proposal distribution, a constraint that might be mitigated by open-source models and network compression [84]. Last, a strong justification for formal representations is that they allow specifying knowledge precisely and unambiguously [85]. Natural language is usually imprecise and ambiguous, which we deferred addressing by translating language into Python. It remains to be seen whether language models can be coaxed into producing sufficiently precise language to support representing knowledge solely in natural language, or if refinement into precise languages like Python offers the better scaling route. Code and data available at: https://github.com/ellisk42/humanlike_fewshot_learning Acknowledgements. We are grateful to Steven Piantadosi for providing the raw human data and Fleet results for the logical concepts, as well as Joshua Tenenbaum for providing his Number Game data, and Mathias Sablé-Meyer for comments on the manuscript and work. 10References [1] François Chollet. On the measure of intelligence, 2019. [2] Nick Chater and Mike Oaksford. The probabilistic mind: Prospects for Bayesian cognitive science. Oxford University Press, USA, 2008. [3] Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):1279–1285, 2011. [4] Ray J Solomonoff. A formal theory of inductive inference. Information and control, 7(1):1–22, 1964. [5] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012. [6] David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. [7] Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel Lázaro-Gredilla, C. C. Laan, Bhaskara Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Hua-Yan Wang, Alexander Lavin, and D. Scott Phoenix. A generative vision model that trains with high data efficiency and breaks text-based captchas. Science, 358, 2017. [8] Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452, 2015. [9] Sreejan Kumar, Carlos G Correa, Ishita Dasgupta, Raja Marjieh, Michael Hu, Robert D. Hawkins, Jonathan Cohen, Nathaniel Daw, Karthik R Narasimhan, and Thomas L. Griffiths. Using natural language and program abstractions to instill human inductive biases in machines. In NeurIPS, 2022. [10] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. [11] Kevin Ellis, Adam Albright, Armando Solar-Lezama, Joshua B Tenenbaum, and Timothy J O’Donnell. Synthesizing theories of human language with bayesian program induction. Nature communications, 13(1):5024, 2022. [12] Pedro A Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Samuel J Gershman, and Joshua B Tenenbaum. Human-level reinforcement learning through theory-based modeling, exploration, and planning. arXiv preprint arXiv:2107.12544, 2021. [13] Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of general- ization. Advances in neural information processing systems, 33:4697–4708, 2020. [14] Noah D Goodman, Joshua B. Tenenbaum, and The ProbMods Contributors. Probabilistic Models of Cognition. http://probmods.org/v2, 2016. Accessed: 2023-9-23. [15] Steven Thomas Piantadosi. Learning and the language of thought. PhD thesis, MIT, 2011. [16] Goker Erdogan, Ilker Yildirim, and Robert A Jacobs. From sensory signals to modality-independent conceptual representations: A probabilistic language of thought approach. PLoS computational biology, 11(11):e1004610, 2015. [17] Mathias Sablé-Meyer, Kevin Ellis, Josh Tenenbaum, and Stanislas Dehaene. A language of thought for the mental representation of geometric shapes. Cognitive Psychology, 139:101527, 2022. [18] Lucas Tian, Kevin Ellis, Marta Kryven, and Josh Tenenbaum. Learning abstract structure for drawing by efficient motor program induction. Advances in Neural Information Processing Systems, 33:2686–2697, 2020. [19] Feras A Saad, Marco F Cusumano-Towner, Ulrich Schaechtle, Martin C Rinard, and Vikash K Mansinghka. Bayesian synthesis of probabilistic programs for automatic data modeling. Proceedings of the ACM on Programming Languages, 3(POPL):1–32, 2019. [20] Percy Liang, Michael I. Jordan, and Dan Klein. Learning programs: A hierarchical bayesian approach. In ICML, 2010. [21] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In PLDI, 2021. [22] OpenAI. Gpt-4 technical report, 2023. 11[23] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [25] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The \"wake-sleep\" algorithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995. [26] Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89–96, 2007. [27] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [28] Thomas L Griffiths, Edward Vul, and Adam N Sanborn. Bridging levels of analysis for probabilistic models of cognition. Current Directions in Psychological Science, 21(4):263–268, 2012. [29] David Marr. Vision: A computational investigation into the human representation and processing of visual information. MIT press, 2010. [30] Thomas L. Griffiths, Falk Lieder, and Noah D. Goodman. Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in Cognitive Science, 7(2):217–229, 2015. [31] Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2166–2179, 2018. [32] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1713–1726, 2022. [33] David Hume. An enquiry concerning human understanding. 1748. [34] Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning.arXiv preprint arXiv:1908.05739, 2019. [35] Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762–4779, Florence, Italy, July 2019. Association for Computational Linguistics. [36] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1266–1279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [37] Eric Mitchell, Joseph Noh, Siyan Li, Will Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn, and Christopher Manning. Enhancing self-consistency and performance of pre-trained language models through natural language inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1754–1768, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [38] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006. [39] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. [40] Herbert A Simon. A behavioral model of rational choice. The quarterly journal of economics , pages 99–118, 1955. [41] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764–10799. PMLR, 2023. 12[42] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.arXiv preprint arXiv:2211.12588, 2022. [43] Joshua Brett Tenenbaum. A Bayesian framework for concept learning. PhD thesis, Massachusetts Institute of Technology, 1999. [44] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint, 2022. [45] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. [46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [47] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625–632, 2005. [48] Stephen Muggleton. Inductive logic programming. New generation computing, 8:295–318, 1991. [49] Andrew Cropper, Sebastijan Duman ˇci´c, Richard Evans, and Stephen H Muggleton. Inductive logic programming at 30. Machine Learning, pages 1–26, 2022. [50] Stanley Kok and Pedro Domingos. Learning the structure of markov logic networks. In Proceedings of the 22nd international conference on Machine learning, pages 441–448, 2005. [51] Charles Kemp and Terry Regier. Kinship categories across languages reflect general communicative principles. Science, 336(6084):1049–1054, 2012. [52] Marie Amalric, Liping Wang, Pierre Pica, Santiago Figueira, Mariano Sigman, and Stanislas Dehaene. The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers. PLoS computational biology, 13(1):e1005273, 2017. [53] Steven T Piantadosi. The computational origin of representation. Minds and machines, 31:1–58, 2021. [54] Lisa Feigenson, Stanislas Dehaene, and Elizabeth Spelke. Core systems of number. Trends in cognitive sciences, 8(7):307–314, 2004. [55] Steven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman. The logical primitives of thought: Empirical foundations for compositional cognitive models. Psychological review, 123(4):392, 2016. [56] John R Anderson and Lael J Schooler. Reflections of the environment in memory. Psychological science, 2(6):396–408, 1991. [57] Steven Piantadosi. Fleet. https://github.com/piantado/Fleet/, 2023. [Online GitHub repository]. [58] Maryellen C MacDonald. Probabilistic constraints and syntactic ambiguity resolution. Language and cognitive processes, 9(2):157–201, 1994. [59] Steven T Piantadosi. The computational origin of representation and conceptual change. 2016. [60] Jeanne Ellis Ormrod. Human learning. Pearson Higher Ed, 2016. [61] Charles Kemp and Joshua B Tenenbaum. The discovery of structural form. Proceedings of the National Academy of Sciences, 105(31):10687–10692, 2008. [62] Catherine Wong, Kevin Ellis, Joshua B. Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In International Conference on Machine Learning, 2021. [63] Taisuke Sato and Yoshitaka Kameya. Prism: a language for symbolic-statistical modeling. In IJCAI, volume 97, pages 1330–1339, 1997. [64] Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62:107–136, 2006. [65] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In NIPS, 2018. 13[66] Pedro Domingos and William Webb. A tractable first-order probabilistic logic. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages 1902–1909, 2012. [67] Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. Church: a language for generative models. In UAI, pages 220–229, 2008. [68] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. ICML, 2017. [69] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A probabilistic programming language for scene perception. In CVPR, 2015. [70] Tuan Anh Le, Atılım Güne¸ s Baydin, and Frank Wood. Inference Compilation and Universal Probabilistic Programming. In AISTATS, 2017. [71] Lise Getoor and Ben Taskar. Introduction to statistical relational learning. MIT press, 2007. [72] Jerry A Fodor. The language of thought, volume 5. Harvard University Press, 1975. [73] Noah D Goodman, Joshua B Tenenbaum, Jacob Feldman, and Thomas L Griffiths. A rational analysis of rule-based concept learning. Cognitive science, 32(1):108–154, 2008. [74] Elizabeth S Spelke. What Babies Know: Core Knowledge and Composition Volume 1, volume 1. Oxford University Press, 2022. [75] Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specificity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–16433, 2011. [76] Noah D Goodman and Michael C Frank. Pragmatic language interpretation as probabilistic inference. Trends in cognitive sciences, 20(11):818–829, 2016. [77] Patrick Shafto, Noah D Goodman, and Thomas L Griffiths. A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive psychology, 71:55–89, 2014. [78] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. [79] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. ICLR, 2023. [80] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583–5594. PMLR, 2021. [81] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. [82] Wai Keen V ong and Brenden M Lake. Cross-situational word learning with multimodal neural networks. Cognitive science, 46(4):e13122, 2022. [83] M. M. Bongard. Pattern Recognition. Spartan Books, 1970. [84] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [85] Hector J Levesque. Knowledge representation and reasoning.Annual review of computer science, 1(1):255– 287, 1986. [86] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 14A Appendix A.1 Supplemental Results Fig. 7 illustrates model predictions across every Number Game concept in [43]. 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 an/uni00A0even/uni00A0number/uni00A0and/uni00A0bet... divisible/uni00A0by/uni00A08 between/uni00A08/uni00A0and/uni00A022 divisible/uni00A0by/uni00A04 even/uni00A0and/uni00A0less/uni00A0than/uni00A030 training/uni00A0examples:/uni00A016 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0prob./uni00A0test/uni00A0is/uni00A0in/uni00A0concept a/uni00A0power/uni00A0of/uni00A02/uni00A0(it/uni00A0is/uni00A0no... an/uni00A0even/uni00A0power/uni00A0of/uni00A02/uni00A0 an/uni00A0integer/uni00A0power/uni00A0of/uni00A02/uni00A0... a/uni00A0precorrect/uni00A0power/uni00A0of/uni00A02 a/uni00A0power/uni00A0of/uni00A02./uni00A0 training/uni00A0examples:/uni00A016,/uni00A08,/uni00A02,/uni00A064 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 either/uni00A016,/uni00A023,/uni00A019,/uni00A0or/uni00A020 a/uni00A0multiple/uni00A0//uni00A0factor/uni00A0of... either/uni00A016,/uni00A023,/uni00A019,/uni00A0or/uni00A020 between/uni00A020/uni00A0and/uni00A025,/uni00A0or/uni00A0... between/uni00A016/uni00A0and/uni00A023. training/uni00A0examples:/uni00A016,/uni00A023,/uni00A019,/uni00A020 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 even/uni00A0and/uni00A060 between/uni00A040/uni00A0and/uni00A080 divisible/uni00A0by/uni00A0both/uni00A02/uni00A0and/uni00A04 between/uni00A050/uni00A0and/uni00A070 divisible/uni00A0by/uni00A03/uni00A0or/uni00A05. training/uni00A0examples:/uni00A060 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 evenly/uni00A0divisible/uni00A0by/uni00A010... greater/uni00A0than/uni00A00,/uni00A0less/uni00A0t... divided/uni00A0by/uni00A010/uni00A0or/uni00A020 a/uni00A0multiple/uni00A0of/uni00A010/uni00A0and/uni00A0i... divisible/uni00A0by/uni00A060,/uni00A080,/uni00A01... training/uni00A0examples:/uni00A060,/uni00A080,/uni00A010,/uni00A030 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 50/uni00A0/uni00AD/uni00A060 50/uni00A0/uni00AD/uni00A060 60,/uni00A0or/uni00A052,/uni00A0or/uni00A057,/uni00A0or/uni00A055. between/uni00A040/uni00A0and/uni00A078 10/uni00A0more/uni00A0than/uni00A050,/uni00A060,/uni00A05... training/uni00A0examples:/uni00A060,/uni00A052,/uni00A057,/uni00A055 0 10 20 30 40 50 60 70 80 90 100 0.0 0.5 1.0 above/uni00A080 either/uni00A098,/uni00A081,/uni00A086,/uni00A0or/uni00A093 between/uni00A080/uni00A0and/uni00A0100/uni00A0** either/uni00A098,/uni00A081,/uni00A086,/uni00A0or/uni00A093 greater/uni00A0than/uni00A0equal/uni00A0to/uni00A0... training/uni00A0examples:/uni00A098,/uni00A081,/uni00A086,/uni00A093 0 10 20 30 40 50 60 70 80 90 100 test/uni00A0number 0.0 0.5 1.0 a/uni00A0square/uni00A0(1,/uni00A04,/uni00A09...) perfect/uni00A0square/uni00A0(has/uni00A0an... a/uni00A0perfect/uni00A0square/uni00A0(see/uni00A0... a/uni00A0square/uni00A0number/uni00A0(integer) a/uni00A0square/uni00A0(non/uni00ADnegative... training/uni00A0examples:/uni00A025,/uni00A04,/uni00A036,/uni00A081 model human Figure 7: Model predictions across every Number Game concept in [43], using 1000 q samples 15Recall that we deduplicated the proposals instead of performing actual importance sampling. Fig. 8 contrasts model fit for importance sampling and deduplication. We originally did deduplication simply because importance sampling is not possible with GPT-4, and GPT-4 proved necessary for the logical concepts. On number concepts we used code-davinci-002, from which we can construct an importance sampler because it exposes the log probability of its samples. On number concepts deduplication provides a fit that is on-par (actually slightly better) compared to importance sampling (Fig. 8). 0.0 0.5 1.0 model/uni00A0prediction 0.0 0.5 1.0human/uni00A0rating R/uni00B2=0.95 deduplication 0.0 0.5 1.0 model/uni00A0prediction 0.0 0.5 1.0human/uni00A0rating R/uni00B2=0.90 importance/uni00A0sampling Figure 8: Monte Carlo inference using deduplication instead of importance sampling does not harm model fit to human data. The above figures show Number Game models using a learned prior and 100 samples, and show predictions only on holdout data. We also studied the effect of replacing the closed-source code-davinci-002 (Codex) with the open- source 70B LLaMA2 [86]. This helps in understanding if open LLMs can be used to build models like ours. Additionally, as the open models are presumably weaker, swapping out Codex for LLaMA2 serves as a way of softly ablating either the likelihood or proposal distribution, both of which use an LLM. We find that the (presumably weaker) LLaMA2 can substitute fully as a likelihood. Using LLaMA2 as a proposal distribution impairs model fit in the low-sample regime (Fig. 9), but with enough samples, the weaker model comes close to ‘catching up’. This highlights the fact that an explicitly Bayesian reasoning process can compensate for deficiencies in the LLM, given enough Monte Carlo samples. 100 101 102 num/uni00A0samples 0.00 0.25 0.50 0.75 1.00model/uni00ADhuman/uni00A0response/uni00A0R/uni00B2 tuned/uni00A0prior,/uni00A0codex/uni00A0for/uni00A0proposals/uni00A0and/uni00A0likelihood tuned/uni00A0prior,/uni00A0llama/uni00AD2/uni00A0for/uni00A0proposals/uni00A0and/uni00A0likelihood tuned/uni00A0prior,/uni00A0llama/uni00AD2/uni00A0for/uni00A0likelihood/uni00A0only tuned/uni00A0prior,/uni00A0llama/uni00AD2/uni00A0for/uni00A0proposals/uni00A0only Figure 9: Comparing versions of our model that swap out the closed-source Codex model for the open-source LLaMA2 70B model, for both the likelihood and the proposal distributions. A.2 Human Study 16 participants were recruited primarily through a Slack message sent to a channel populated by members of our academic department. Participants had an average age 28.1 (stddev 13.6, all over 1618), and were 7 male/5 female/1 nonbinary/3 declined to answer. Participants were randomly split between the concepts of most common color / least common color. Each participant went through 15 trials, and took an average of 294s to complete those 15 trials. In exchange for participating in the study, participants received $10 in Amazon gift cards. Fig. 10 illustrates the web interface shown to our human participants, including the cover story. Figure 10: Cover story and web interface for our version of the logical concept learning study, which is based on [59] A.3 Modeling A.3.1 Temperature and Platt Transform Adding a temperature parameter T to a model corresponds to computing the posterior via pTemp(Xtest ∈C∣X1∶K) ≈ ∑ C∈{C(1),...,C(S)} w(C)1[Xtest ∈C], where w(C) = (˜w(C)) 1/T ∑C′ (˜w(C′)) 1/T and ˜w(C) =p(C)p(X1∶K∣C)1[C ∈{C(1), . . . , C(S)}] (10) Adjusting the predictions of a model using a Platt transform corresponds to introducing parameters a and b which transform the predictions according to pPlatt(Xtest ∈C∣X1∶K) =Logistic(b +a ×Logistic−1 (p(Xtest ∈C∣X1∶K))) (11) 17For the number game, every model has its outputs transformed by a learned Platt transform. This is because we are modeling human ratings instead of human responses. We expect that the ratings correspond to some monotonic transformation of the human’s subjective probability estimates, and so this transformation gives some extra flexibility by inferring the correspondence between probabilities and ratings. Logical concept models do not use Platt transforms. A.3.2 Parameter fitting Training consists of fitting the parameters T, θ (for the prior), ϵ (for the likelihood), α and β (for the logical concepts likelihood), and Platt transform parameters a, b (for the Number Game). In practice, this amounts to around 400 parameters, almost all of which come from θ. We fit these parameters using Adam with a learning rate of 0.001. We perform 1000 epochs of training for the Number Game, and 100 epochs for logical concepts. There is a tenfold difference in the number of concepts, so this way they take about the same number of gradient steps. For the number game we do 10-fold cross validation to calculate holdout predictions. For logical concepts we use the train-test split introduced in [59], which involves running different groups of human subjects on each concept twice, with different random examples. One sequence of random examples is arbitrarily designated as training data, and the other as holdout data. All model were trained on a laptop using no GPUs. Training takes between a few minutes and an hour, depending on the domain and the model. Some of the parameters that we fit, namely ϵ, α, β, cannot be negative. To enforce this we actually optimize the inverse logistic of those parameters. A.3.3 MCMC over Logical Expressions Fleet was used1 to perform MCMC over logical expressions with the domain-specific primitives in this file, which include: true, false ∶boolean blue, yellow, green ∶object →boolean rectangle, circle, triangle ∶object →boolean small, medium, large ∶object →boolean and, or, ⇐⇒, /Leftr⫯g⊸tl⫯ne⇒∶boolean ×boolean →boolean ∀, ∃∶ (shape →boolean)×2object →boolean filter ∶(object →boolean)×2object →2object ∈∶object ×2object →boolean ι ∶2object →object ∪{/⊙◇⊞}, unique set element empty ∶2object →boolean same_shape, same_color, same_size ∶object ×object →boolean size<, size≤, size>, size≥, ∶object ×object →boolean The model first constructed a large hypothesis space by performing MCMC for 1 minute per batch, and per learning curve. In one minute, Fleet makes approximately 106 MH proposals. There are a little more than 200 learning curves, and 25 batches per curve, for a total of about 5 billion MCMC proposals. In the main text, we abbreviate this analysis by referring to 109 proposals. The top 10 samples per batch and per learning curve were retained. These top 10 samples samples were then deduplicated to yield 45 thousand hypotheses. Parameter fitting and posterior estimation was performed solely over those 45 thousand hypotheses. Quantitatively, these are vastly more proposals than the models introduced in this paper. Quantitatively, these proposals are also derived in a very different way: the hypothesis space for the BPL learner is 1Running the model was graciously performed by the authors of [59], who provided us with the raw data. 18actually informed by data on other learning curves, and also on the same learning curve, but in the future batches. It is in this sense that the BPL model is a computational-level theory, and not a process model, because human subjects could not be proposing hypotheses using data that is going to be seen in the future, or on other learning curves. However, the above strategy for proposing hypotheses is a very reasonable heuristic for constructing the support of the space of plausible logical hypotheses that a human learner might ever think of. A.3.4 DreamCoder baseline DreamCoder was initialized with the following domain-specific primitives: 1, 2, 3, ...,100 ∶int squares ∶2int singleton ∶int →2int interval ∶int ×int →2int intersect, union ∶2int ×2int →2int powersof, multiplesof ∶int →2int Each of the 8 stimuli from [43] was converted into a single task. The discrete structure of the prior was held fixed because there was no learnable common structure across the 8 tasks, but the continuous parameters of the prior were updated at each wake-sleep cycle. We estimated those parameters from the topK=100 highest posterior programs discovered for each task at each wake-sleep cycle. The neural recognition model took as input a 100-dimensional binary vector encoding the support of a number concept, which was then processed by an MLP with 64 hidden units. The recognition model was trained on a 50/50 split of replays/dreams for approximately 105 training examples per wake-sleep cycle. During the waking phase, program synthesis was provided with a timeout of 60 seconds per task per cycle, enumerating approximately 104 programs per task. We confirmed by manual inspection that high-likelihood programs were discovered for of the eight tasks. Therefore the difference between DreamCoder and our model cannot be attributed to a failure on the part of DreamCoder to discover valid programs. Instead, we attribute the difference to differences of representation (natural language vs lambda calculus), together with differences in the ways that parameters are estimated (maximizing the marginal likelihood of the tasks, vs directly fitting human judgments). A.4 Prompting A.4.1 Proposing hypotheses For the number game we use the following prompt for code-davinci-002 to generate candidate concepts in natural language, given examples X1∶K. The example number concepts given in the prompt come from the cover score given to human participants [43]: # Python 3 # Here are a few example number concepts : # -- The number is even # -- The number is between 30 and 45 # -- The number is a power of 3 # -- The number is less than 10 # # Here are some random examples of numbers belonging to a different ⤦ /r⇣urve⇡rrowsenumber concept : # X1∶K # The above are examples of the following number concept : # -- The number is where X1∶K is formatted by listing the numbers with a comma and a space between them. 19For the number game we used the following prompt to generate candidate concepts in python (code baseline): # Python 3 # Here are a few example number concepts : # -- The number is even # -- The number is between 30 and 45 # -- The number is a power of 3 # -- The number is less than 10 # # Here are some random examples of numbers belonging to a different ⤦ /r⇣urve⇡rrowsenumber concept : # X1∶K # Write a python function that returns true if ‘num ‘ belongs to ⤦ /r⇣urve⇡rrowsethis number concept . def check_if_in_concept ( num ): return For logical concepts we used the following few-shot prompt for GPT-4 to generate candidate concepts: Here three simple concepts , which specify when an object is ⤦ /r⇣urve⇡rrowse’positive ’ relative to an example collection of other ⤦ /r⇣urve⇡rrowseobjects . Before giving the rule for each concept , we give ⤦ /r⇣urve⇡rrowseexamples of collections of objects , and which objects in the ⤦ /r⇣urve⇡rrowsecollection are ’positive ’. Concept #1: An Example of Concept #1: POSITIVES : ( big yellow rectangle ) NEGATIVES : ( big green circle ), ( medium yellow rectangle ) Another Example of Concept #1: POSITIVES : ( medium yellow rectangle ) NEGATIVES : ( big red circle ), ( small green circle ) Rule for Concept #1: Something is positive if it is the biggest ⤦ /r⇣urve⇡rrowseyellow object in the example . Concept #2: An Example of Concept #2: POSITIVES : ( small yellow circle ), ( medium yellow rectangle ) NEGATIVES : ( big green circle ), ( big blue rectangle ) Another Example of Concept #2: POSITIVES : ( big blue circle ), ( medium blue rectangle ) NEGATIVES : ( small green circle ), ( medium yellow rectangle ), Rule for Concept #2: Something is positive if there is another ⤦ /r⇣urve⇡rrowseobject with the same color in the example . Concept #3: An Example of Concept #3: POSITIVES : ( small yellow circle ), ( medium yellow rectangle ) NEGATIVES : ( big green circle ), ( big blue rectangle ) Another Example of Concept #3: POSITIVES : ( small blue circle ), ( small blue triangle ), ⤦ /r⇣urve⇡rrowse( medium blue rectangle ) NEGATIVES : ( medium green triangle ), ( big yellow rectangle ) Another Example of Concept #3: POSITIVES : ( big red rectangle ), ( medium red rectangle ), ⤦ /r⇣urve⇡rrowse( big red triangle ) NEGATIVES : ( medium green triangle ), ( big yellow rectangle ) Rule for Concept #3: Something is positive if it is the same color ⤦ /r⇣urve⇡rrowseas the smallest triangle in the example . Now here are some examples of another concept called Concept #4 , ⤦ /r⇣urve⇡rrowsebut this time we don ’t know the rule . Infer ten different ⤦ /r⇣urve⇡rrowsepossible rules , and make those ten rules as simple and ⤦ /r⇣urve⇡rrowsegeneral as you can . Your simple general rules might talk ⤦ 20/r⇣urve⇡rrowseabout shapes , colors , and sizes , and might make comparisons ⤦ /r⇣urve⇡rrowsebetween these features within a single example , but it ⤦ /r⇣urve⇡rrowsedoesn ’t have to. Remember that a rule should say when ⤦ /r⇣urve⇡rrowsesomething is positive , and should mention the other objects ⤦ /r⇣urve⇡rrowsein the example , and should be consisting with what you see ⤦ /r⇣urve⇡rrowsebelow . Concept #4: X1∶K Rule for Concept #4: Something is positive if ... Now make a numbered list of 10 possible rules for Concept #4. Start ⤦ /r⇣urve⇡rrowseby writing \"1. Something is positive if \". End each line with ⤦ /r⇣urve⇡rrowsea period . Each sample from the above prompt generates 10 possible concepts formatted as a numbered list. We draw 10 times at temperature=1 to construct 100 hypotheses. To obtain fewer than 100 hypotheses we take hypotheses from each sampled list in round-robin fashion. We found that asking it to generate a list of hypotheses generated greater diversity without sacrificing quality, compared to repeatedly sampling a single hypothesis. The above prompt provides in-context examples of first-order rules. We also tried using a different prompt for propositional concepts that illustrated the examples as a truth table, and gave in-context example rules that were propositional: Here are some example concepts defined by a logical rule : Rule : a triangle . Rule : a green rectangle . Rule : big or a rectangle ( unless that rectangle is blue ). Rule : not both big and green . Rule : either big or green , but not both . Rule : either a rectangle or not yellow . Rule : a circle . Now please produce a logical rule for a new concept . Your rule ⤦ /r⇣urve⇡rrowseshould be consistent with the following examples . It must be ⤦ /r⇣urve⇡rrowsetrue of all the positive examples , and not true of all the ⤦ /r⇣urve⇡rrowsenegative examples . The examples are organized into a table ⤦ /r⇣urve⇡rrowsewith one column for each feature (size , color , shape ): X1∶K Please produce a simple rule that is consistent with the above ⤦ /r⇣urve⇡rrowsetable . Make your rule as SHORT , SIMPLE , and GENERAL as ⤦ /r⇣urve⇡rrowsepossible . Do NOT make it more complicated than it has to be , ⤦ /r⇣urve⇡rrowseor refer to features that you absolutely do not have to refer ⤦ /r⇣urve⇡rrowseto. Begin by writing \" Rule : \" and then the rule , followed by ⤦ /r⇣urve⇡rrowsea period . Using the first order prompt for every concept gives aR2 =.80 fit to the human responses. Using both prompts gives the R2 =.81 result in the main paper: the propositional prompt for the propositional problems, and the first order prompt for the higher order problems. We strongly suspect that a single prompt that just showed both propositional and higher-order in-context examples would work equally well, given that a single first-order prompt works about as well also, but we did not try that it would have required rerunning all of our GPT-4 queries, which would have had high cost. On the first batch, the learner has not observed any examples. Therefore the above prompts do not apply, and we use a different prompt to construct an initial hypothesis space: Here are some example concepts defined by a logical rule : Rule : color is purple . Rule : shape is not a hexagon . 21Rule : color is purple and size is small . Rule : size is tiny or shape is square . Rule : size is not enormous . Rule : color is red . Please propose a some new concepts , defined by a logical rule . ⤦ /r⇣urve⇡rrowseThese new concepts can only refer to the following features : - shape : triangle , rectangle , circle - color : green , blue , yellow - size : small , medium , large Please make your rules short and simple , and please write your ⤦ /r⇣urve⇡rrowseresponse on a single line that begins with the text \" Rule : \". ⤦ /r⇣urve⇡rrowseProvide 100 possible rules . We generate from the above prompt at temperature=0, and split on line breaks to obtain candidate rules. A.4.2 Translating from natural language to Python We translate Number Game concepts from English to Python via the following prompt for code- davinci-002, and generate at temperature=0 until linebreak: # Write a python function to check if a number is C. def check_number ( num ): return We translate logical concepts from English to Python using a series of in-context examples, again generating with temperature=0 until the text #DONE is produced.2 def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it is not a small object , and not ⤦ /r⇣urve⇡rrowsea green object . \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it is not a small object , and not a ⤦ /r⇣urve⇡rrowsegreen object . # START return ( not this_size == 1) and ( not this_color == \" green \") # DONE def check_object ( this_object , other_objects ): \"\"\" 2This prompt is pretty long, and probably could be much shorter. Preliminary experiments suggested that a few in-context examples were very helpful, and so to increase the odds of the model working without much time spent prompt-engineering, we provided a large number of in-context examples. 22this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it is bigger than every other object \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it is bigger than every other object # START return all ( this_size > other_object [2] for other_object in ⤦ /r⇣urve⇡rrowseother_objects ) # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it is one of the largest \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it is one of the largest # START return all ( this_size >= other_object [2] for all_example_object ⤦ /r⇣urve⇡rrowsein all_example_objects ) # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it is smaller than every yellow ⤦ /r⇣urve⇡rrowseobject \"\"\" 23# shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it is smaller than every yellow object # START return all ( this_size < other_object [2] for other_object in ⤦ /r⇣urve⇡rrowseother_objects if other_object [1] == \" yellow \" ) # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if there is another object with the ⤦ /r⇣urve⇡rrowsesame color \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if there is another object with the ⤦ /r⇣urve⇡rrowsesame color # START return any ( this_color == other_object [1] for other_object in ⤦ /r⇣urve⇡rrowseother_objects ) # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it has a unique combination of ⤦ /r⇣urve⇡rrowsecolor and shape \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ 24# to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it has a unique combination of color ⤦ /r⇣urve⇡rrowseand shape # START return all ( this_shape != other_object [0] or this_color != ⤦ /r⇣urve⇡rrowseother_object [1] for other_object in other_objects ) # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if it has the same color as the ⤦ /r⇣urve⇡rrowsemajority of objects \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if it has the same color as the ⤦ /r⇣urve⇡rrowsemajority of objects # START majority_color = max ([\" yellow \", \" green \", \" blue \"] , key = lambda ⤦ /r⇣urve⇡rrowsecolor : sum (1 for obj in all_example_objects if obj [1] == ⤦ /r⇣urve⇡rrowsecolor )) return this_color == majority_color # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : Something is positive if there are at least two other ⤦ /r⇣urve⇡rrowseobjects with the same shape \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ 25# be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # Something is positive if there are at least two other objects ⤦ /r⇣urve⇡rrowsewith the same shape # START return sum (1 for other_object in other_objects if ⤦ /r⇣urve⇡rrowseother_object [0] == this_shape ) >= 2 # DONE def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : C \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # C # START When modeling the concepts of majority/minority color we used a different prompt for converting natural language to python, because the above includes majority color as one of its in-context examples. We gave GPT-4 the following prompt for those concepts:3 Please write a python function to check if an object obeys a ⤦ /r⇣urve⇡rrowselogical rule . The logical rule talks about the following ⤦ /r⇣urve⇡rrowsefeatures : shape : a string , either \" circle \", \" rectangle \", or \" triangle \" color : a string , either \" yellow \", \" green \", or \" blue \" size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) The python function should be called ‘ check_object ‘, and inputs : ‘this_object ‘: a tuple of ( shape , color , size ) ‘ other_objects ‘: a list of tuples of ( shape , color , size ) The logical rule should check if ‘this_object ‘ has a certain ⤦ /r⇣urve⇡rrowserelationship with ‘ other_objects ‘. Collectively , ⤦ /r⇣urve⇡rrowse‘[ this_object ]+ other_objects ‘ correspond to all of the ⤦ /r⇣urve⇡rrowseobjects , so if the rule references the whole examples , it is ⤦ /r⇣urve⇡rrowsetalking about that structure . The logical rule is: C 3We strongly suspect that GPT-4 with the following prompt is strictly better then Codex. The high cost of using GPT-4 makes it more practical to use the Codex prompt for the main experiments however. 26Please start your response by writing the following code , and then ⤦ /r⇣urve⇡rrowsecomplete the function body so that it returns ‘True ‘ if and ⤦ /r⇣urve⇡rrowseonly if the logical rule above holds . ‘‘‘ def check_object ( this_object , other_objects ): \"\"\" this_object : a tuple of ( shape , color , size ) other_objects : a list of tuples of ( shape , color , size ) returns : True if ‘this_object ‘ is positive according to the ⤦ /r⇣urve⇡rrowsefollowing rule : C \"\"\" # shape : a string , either \" circle \", \" rectangle \", or \" triangle \" # color : a string , either \" yellow \", \" green \", or \" blue \" # size : an int , either 1 ( small ), 2 ( medium ), or 3 ( large ) this_shape , this_color , this_size = this_object # ‘this_object ‘ is not a part of ‘ other_objects ‘ # to get all of the examples , you can use ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘, defined as ‘ other_objects + ⤦ /r⇣urve⇡rrowse[ this_object ]‘ # be careful as to whether you should be using ⤦ /r⇣urve⇡rrowse‘ all_example_objects ‘ or ‘ other_objects ‘ in your code all_example_objects = other_objects + [ this_object ] # return True if and only if: # C ‘‘‘ A.5 GPT-4 Baselines Our GPT-4 baseline for each domain presented the examples X1∶K in string form and then asked GPT-4 to respond Yes/No as to whether a test exampleXtest belonged to the same concept. GPT-4 was then queried at temperature=1 to collect 10 samples. Samples not beginning with ‘y’/‘n’ were discarded, and the ratio of remaining samples that began with ‘y’ was computed (case insensitive). We show below example prompts for the number and logic domains. Here are a few example number concepts : -- The number is even -- The number is between 30 and 45 -- The number is a power of 3 -- The number is less than 10 Here are some random examples of numbers belonging to a possibly ⤦ /r⇣urve⇡rrowsedifferent number concept : 98, 81, 86, 93 Question : Does the number 42 belong to the same concept as the ⤦ /r⇣urve⇡rrowseabove numbers ? Answer ( one word , yes /no): Logical concept example prompt: Here are some example concepts defined by a logical rule : Rule for Concept #1: Something is positive if it is the biggest ⤦ /r⇣urve⇡rrowseyellow object in the example Rule for Concept #2: Something is positive if there is another ⤦ /r⇣urve⇡rrowseobject with the same color in the example Rule for Concept #3: Something is positive if it is the same color ⤦ /r⇣urve⇡rrowseas the smallest triangle in the example 27Now please look at the following examples for a new logical rule . An Example of Concept #4: POSITIVES : none NEGATIVES : ( large yellow circle ), ( small green circle ), ⤦ /r⇣urve⇡rrowse( medium green circle ), ( small yellow triangle ) Another Example of Concept #4: POSITIVES : ( small green circle ), ( large green circle ) NEGATIVES : ( large yellow circle ), ( medium blue circle ) Another Example of Concept #4: POSITIVES : ( small green rectangle ) NEGATIVES : ( medium yellow circle ), ( medium blue rectangle ), ⤦ /r⇣urve⇡rrowse( large green circle ), ( medium green circle ) Another Example of Concept #4: POSITIVES : ( medium green rectangle ) NEGATIVES : ( medium yellow circle ), ( small yellow ⤦ /r⇣urve⇡rrowserectangle ), ( medium yellow rectangle ), ( medium blue ⤦ /r⇣urve⇡rrowserectangle ) Another Example of Concept #4: POSITIVES : ( small green rectangle ) NEGATIVES : ( large yellow rectangle ), ( small yellow ⤦ /r⇣urve⇡rrowsetriangle ), ( medium green circle ), ( small blue rectangle ) Another Example of Concept #4: POSITIVES : ( medium green triangle ) NEGATIVES : ( medium blue triangle ), ( medium blue rectangle ), ⤦ /r⇣urve⇡rrowse( large blue triangle ), ( small yellow triangle ) Another Example of Concept #4: POSITIVES : none NEGATIVES : ( small yellow circle ), ( large blue circle ) Another Example of Concept #4: POSITIVES : none NEGATIVES : ( large green circle ), ( small blue rectangle ), ⤦ /r⇣urve⇡rrowse( small green triangle ), ( medium blue rectangle ) Another Example of Concept #4: POSITIVES : ( small green rectangle ) NEGATIVES : ( small yellow circle ), ( large blue rectangle ) Now we get a new collection of examples for Concept #4: ( medium blue triangle ) ( large yellow triangle ) ( small blue ⤦ /r⇣urve⇡rrowserectangle ) ( large blue circle ) ( small yellow circle ) Question : Based on the above example , is a ( small yellow circle ) in ⤦ /r⇣urve⇡rrowsethe concept ? Answer ( one word , just write yes /no): A.6 Latent Language Baseline For fair comparison, we designed our latent language baseline to be as similar to our system as possible. It performs maximum likelihood estimation of a single concept, rather than estimate a full posterior, but uses the exact same prompts and likelihood functions as our model. The most important difference from the original latent language paper [ 31] is that instead of training our own neural models for language interpretation and language generation, we use pretrained models (Codex/code-davinci-002 and GPT-4). A.7 Ablation of the proposal distribution We ablate the proposal distribution by proposing hypotheses unconditioned on X1∶K. We accomplish this by drawing concepts from the following alternative prompt, which is designed to resemble the prompt used by the full model except that it does not include X1∶K: # Python 3 # Here are a few example number concepts : # -- The number is even # -- The number is between 30 and 45 28# -- The number is a power of 3 # -- The number is less than 10 # -- The number is A.8 Pretrained prior Our pretrained prior comes from the opensource model CodeGen [44], which was trained on source code. We chose this model because we suspected that pretraining on source code would give better density estimation for text describing precise rules. We formatted the rules as a natural language comment and prefixed it with a small amount of domain-specific text in order to prime the model to put probability mass on rules that correctly talk about numbers or shapes. For the number game, we would query CodeGen for the probability of p(C) via # Here is an example number concept : # The number is C For the number game’s code baseline, we would query CodeGen for the probability of p(C) via # Python 3 # Let ’s think of a number concept . # Write a python function that returns true if ‘num ‘ belongs to ⤦ /r⇣urve⇡rrowsethis number concept . def check_if_in_concept ( num ): return C For logical concepts we would query CodeGen for the probability of p(C) via # Here are some simple example shape concepts : # 1. neither a triangle nor a green rectangle # 2. not blue and large . # 3. if it is large , then it must be yellow . # 4. small and blue # 5. either big or green . # 6. C Because the proposal distribution would generate rules beginning with the prefix “Something is positive if...” we would remove that text before computing p(C) as above. 29",
      "references": [
        "On the measure of intelligence",
        "The probabilistic mind: Prospects for Bayesian cognitive science",
        "How to grow a mind: Statistics, structure, and abstraction",
        "A formal theory of inductive inference",
        "Machine learning: a probabilistic perspective",
        "Information theory, inference and learning algorithms",
        "A generative vision model that trains with high data efficiency and breaks text-based captchas",
        "Probabilistic machine learning and artificial intelligence",
        "Using natural language and program abstractions to instill human inductive biases in machines",
        "Human-level concept learning through probabilistic program induction",
        "Synthesizing theories of human language with bayesian program induction",
        "Human-level reinforcement learning through theory-based modeling, exploration, and planning",
        "Bayesian deep learning and a probabilistic perspective of general- ization",
        "Probabilistic Models of Cognition",
        "Learning and the language of thought",
        "From sensory signals to modality-independent conceptual representations: A probabilistic language of thought approach",
        "A language of thought for the mental representation of geometric shapes",
        "Learning abstract structure for drawing by efficient motor program induction",
        "Bayesian synthesis of probabilistic programs for automatic data modeling",
        "Learning programs: A hierarchical bayesian approach",
        "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
        "Gpt-4 technical report",
        "Evaluating large language models trained on code",
        "Attention is all you need",
        "The \"wake-sleep\" algorithm for unsupervised neural networks",
        "Core knowledge",
        "Building machines that learn and think like people",
        "Bridging levels of analysis for probabilistic models of cognition",
        "Vision: A computational investigation into the human representation and processing of visual information",
        "Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic",
        "Learning with latent language",
        "Skill induction and planning with latent language",
        "An enquiry concerning human understanding",
        "Abductive commonsense reasoning.arXiv preprint arXiv:1908.05739, 2019",
        "COMET: Commonsense transformers for automatic knowledge graph construction",
        "Maieutic prompting: Logically consistent reasoning with recursive explanations",
        "Enhancing self-consistency and performance of pre-trained language models through natural language inference",
        "Pattern Recognition and Machine Learning",
        "Thinking, fast and slow",
        "A behavioral model of rational choice",
        "Pal: Program-aided language models",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "A Bayesian framework for concept learning",
        "Codegen: An open large language model for code with multi-turn program synthesis",
        "Sentence-bert: Sentence embeddings using siamese bert-networks",
        "Predicting good probabilities with supervised learning",
        "Inductive logic programming",
        "Inductive logic programming at 30",
        "Learning the structure of markov logic networks",
        "Kinship categories across languages reflect general communicative principles",
        "The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers",
        "The computational origin of representation",
        "Core systems of number",
        "The logical primitives of thought: Empirical foundations for compositional cognitive models",
        "Reflections of the environment in memory",
        "Fleet",
        "Probabilistic constraints and syntactic ambiguity resolution",
        "The computational origin of representation and conceptual change",
        "Human learning",
        "The discovery of structural form",
        "Leveraging language to learn program abstractions and search heuristics",
        "Prism: a language for symbolic-statistical modeling",
        "Markov logic networks",
        "Deepproblog: Neural probabilistic logic programming",
        "A tractable first-order probabilistic logic",
        "Church: a language for generative models",
        "Robustfill: Neural program learning under noisy i/o",
        "Picture: A probabilistic programming language for scene perception",
        "Inference Compilation and Universal Probabilistic Programming",
        "Introduction to statistical relational learning",
        "The language of thought",
        "A rational analysis of rule-based concept learning",
        "What Babies Know: Core Knowledge and Composition Volume 1, volume 1",
        "Functional specificity for high-level linguistic processing in the human brain",
        "Pragmatic language interpretation as probabilistic inference",
        "A rational account of pedagogical reasoning: Teaching by, and learning from, examples",
        "Teaching large language models to self-debug",
        "Generating sequences by learning to self-correct",
        "Vilt: Vision-and-language transformer without convolution or region supervision",
        "Socratic models: Composing zero-shot multimodal reasoning with language",
        "Cross-situational word learning with multimodal neural networks",
        "Pattern Recognition",
        "Llm. int8 (): 8-bit matrix multiplication for transformers at scale",
        "Knowledge representation and reasoning",
        "Llama 2: Open foundation and fine-tuned chat models"
      ],
      "meta_data": {
        "arxiv_id": "2306.02797v3",
        "authors": [
          "Kevin Ellis"
        ],
        "published_date": "2023-06-05T11:46:45Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces a human-like few-shot learning model that balances tractable inference with an expressive hypothesis space. The model utilizes Bayesian reasoning where a language model proposes natural language hypotheses, which are then re-weighed by a prior and a likelihood. The key contributions include: (1) a symbolic concept learning model supporting efficient inference over a flexible hypothesis class, (2) evaluation on human data from two concept learning experiments (Number Game and Logical Concepts), and (3) a method for extracting a human-like prior over concepts from behavioral data. The model can predict human judgments on learning problems involving numbers and sets, spanning generative, discriminative, propositional, and higher-order concepts, and captures fine-grained structures in human judgments.",
        "methodology": "The model is built upon a basic Bayesian approach where a latent concept C generates observed examples. The core idea is to represent concepts in natural language. A neural model defines the prior p(C), and the likelihood p(X|C) is domain-specific. Since the posterior is often intractable, a proposal distribution q(C|X1:K) is introduced to sample candidate concepts. These sampled concepts are then re-weighed by p(C)p(X1:K|C) to approximate the posterior predictive distribution. To compare natural language concepts with nonlinguistic data, the natural language concept C is translated into Python code using a large language model (LLM) and executed on the data. The model distinguishes between a \"pretrained prior\" (using an open-source language model to score concept log-likelihood) and a \"tuned prior\" (learning a linear mapping from sentence features to an unnormalized log probability, fine-tuned to human judgments). The proposal distribution (q) is implemented using an LLM to complete natural language descriptions of concepts based on training examples.",
        "experimental_setup": "The model was evaluated on two main concept learning tasks: The Number Game and Logical Concepts. For the Number Game, human data from a previous study [43] was used, where participants rated the likelihood of test numbers belonging to a concept given example numbers. The model aimed to predict the average human rating for each test number. For Logical Concepts, human data was taken from an online learning setup where learners observed streams of examples of unknown logical concepts [55]. These concepts included propositional and first-order logic structures. The model was evaluated on its ability to predict human learning curves over dozens of examples. New concepts (\"shapes with the majority color\" and \"shapes with the least common color\") were also created and tested on a new group of human participants (16 subjects). Model predictions were compared against baselines including GPT-4, DreamCoder (a Bayesian Program Learning system), and Latent Language models. Parameter fitting utilized Adam optimization and 10-fold cross-validation for the Number Game, and a train-test split from [59] for logical concepts.",
        "limitations": "The model performs induction via discrete structure learning, and it is unclear if this approach can scale to inferring complex systems of symbolic rules. The current model is limited to processing discrete symbolic input-outputs and does not handle messy perceptual data, a limitation that BPL models can address. The model relies on costly and energy-intensive LLMs for its proposal distribution. Natural language, being inherently imprecise and ambiguous, required translation into Python for precise evaluation, raising questions about whether LLMs can generate sufficiently precise language or if refinement into precise languages like Python is a better long-term strategy.",
        "future_research_directions": "Future work could explore iteratively refining language model outputs to scale the model to more complex symbolic rule systems. Investigating the integration of multimodal language models or approaches that interface separate language and vision modules could address the limitation of processing only discrete symbolic inputs and outputs, allowing the model to connect with perceptual data. Reducing the reliance on costly LLMs by exploring open-source models and network compression techniques is another promising direction. Further research could examine how statistical-relational AI can benefit from encoding knowledge in natural language, given its use of pretrained neural models to guide inference. Additionally, exploring how insights from models of pedagogical inductive learning and recursive reasoning could be incorporated to simulate aspects of human-to-human learning is suggested.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Unifying Image Processing as Visual Prompting Question Answering",
      "full_text": "Unifying Image Processing as Visual Prompting Question Answering Yihao Liu * 1 2 Xiangyu Chen * 1 2 3 Xianzheng Ma * 1 Xintao Wang4 Jiantao Zhou 3 Yu Qiao1 2 Chao Dong 2 1 Abstract Image processing is a fundamental task in com- puter vision, which aims at enhancing image qual- ity and extracting essential features for subsequent vision applications. Traditionally, task-specific models are developed for individual tasks and designing such models requires distinct exper- tise. Building upon the success of large language models (LLMs) in natural language processing (NLP), there is a similar trend in computer vi- sion, which focuses on developing large-scale models through pretraining and in-context learn- ing. This paradigm shift reduces the reliance on task-specific models, yielding a powerful uni- fied model to deal with various tasks. However, these advances have predominantly concentrated on high-level vision tasks, with less attention paid to low-level vision tasks. To address this issue, we propose a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, etc. Our proposed framework, named PromptGIP, uni- fies these diverse image processing tasks within a universal framework. Inspired by NLP ques- tion answering (QA) techniques, we employ a visual prompting question answering paradigm. Specifically, we treat the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem. PromptGIP can under- take diverse cross-domain tasks using provided visual prompts, eliminating the need for task- specific finetuning. Capable of handling up to 15 different image processing tasks, PromptGIP represents a versatile and adaptive approach to general image processing. While PromptGIP has demonstrated a certain degree of out-of-domain task generalization, further research is expected to fully explore its more powerful capability. *Equal contribution 1Shanghai Artificial Intelligence Labo- ratory 2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 3University of Macau 4ARC Lab, Tencent PCG. Correspondence to: Chao Dong <chao.dong@siat.ac.cn>. Preprint. Copyright 2024 by the author(s). Question-Answer Prompts Input Question Predicted Answer GT Inpainting Rain LLF Low-light Canny Laplacian L0 smooth RestorationEnhancementEdge DetectionStyle Transfer Figure 1.PromptGIP is a universal framework for general image processing. It can accomplish diverse tasks with distinct output domains, including image restoration, enhancement and edge de- tection. It has demonstrated a certain level of generalization for out-of-domain tasks (marked in dashed lines). 1. Introduction Image processing encompasses a set of fundamental tasks that are aimed at direct manipulation and enhancement of image pixel-level information. These tasks are primarily focused on improving image quality and extracting basic image features, including but not limited to image restora- tion, image enhancement, image filtering, and image feature extraction. They provide a solid foundation for subsequent analysis, recognition, and comprehension of visual content within images. To address diverse image processing require- ments, practitioners have traditionally resorted to developing specialized task-specific models. Consequently, achieving a particular objective often demands the utilization of differ- ent independent or combined models. 1 arXiv:2310.10513v2  [cs.CV]  21 Feb 2024Unifying Image Processing as Visual Prompting Question Answering I love hamburgers J'adore les hamburgers I like sports J'aime le sport MAE-VQGAN Painter PromptGIP Q A Q A Task Prompt Query Input I J'adore love les Hamburgers hamburgers I J'aime like le sports sport I love hamburgers I like sports J'adore les hamburgers J'aime le sport I love hamburgers I like sports J'adore les hamburgers J'aime le sport I love hamburgers J'adore les hamburgers I like sports J'aime le sport I love hamburgers J'adore les hamburgers I like sports J'aime le sport An image is equal to  one sentence Image processing as  prompting QA English-French Translation Figure 2.Analogous to NLP tasks, various image processing tasks can be unified into a general visual prompting QA paradigm: given a pair of image prompt, the model can process the query image based on the prompts. MAE-VQGAN fragments image tokens and arrange them in an interleaved fashion. It disrupts the continuity and contextual understanding of the image content. Painter adopts a Q-Q-A-A organizational structure, which is not aligned with the QA paradigm. This misalignment can lead to inefficiencies in learning. In recent years, a significant trend has emerged towards the development of general large-scale models. This paradigm shift involves extensive pretraining on massive datasets and interactive in-context learning techniques, leading to the creation of a unified, powerful model capable of handling multiple tasks. For example, large language models (LLMs), especially the GPT series models (Radford et al., 2019; Brown et al., 2020), have successfully unified most tasks in the natural language processing (NLP) field and achieved exceptional performance. Similar exploration has also been observed in the field of computer vision. Meta AI Research introduced a Segment Anything Model (SAM) (Kirillov et al., 2023) for image segmentation. Through large-scale pretraining, SAM achieves remarkable zero-shot generaliza- tion performance in various scenarios. In other computer vision fields, a quantity of large foundation models have also been proposed, such as Inpainting Anything Model (IAM) (Yu et al., 2023), Track Anything Model (TAM) (Yang et al., 2023), InternImage (Wang et al., 2023a), and InternVideo (Wang et al., 2022a). These advancements carry profound implications for the realization of artificial general intelli- gence (AGI). However, current focus of large models primarily lies in the domain of high-level vision. Low-level vision has received relatively little attention. While some newly-proposed meth- ods, e.g., MAE-VQGAN (Bar et al., 2022) and Painter (Wang et al., 2023b), have involved a few classic low-level vision tasks, their main focus remains on high-level vision tasks. Furthermore, these methods encounter challenges in dataset selection, model design, and training paradigms, making them unable to directly adapt to the low-level vision. In this paper, we present a universal model for general im- age processingby thoroughly examining the characteristics of low-level vision tasks and analyzing the limitations of existing in-context learning models in computer vision. Un- like prior literature that predominantly focused on image restoration tasks, our proposed model expands its scope to encompass image restoration, image enhancement, and image feature extraction. These tasks all belong to the do- main of image processing, but their objectives and output domains are distinct. Specifically, image restoration aims to recover the original clean and natural image from a de- graded image, such as denoising and deblurring. Image enhancement focuses on improving the visual quality of the image by enhancing contrast, brightness, color tones, and textures. Image feature extraction, like edge detection, fo- cuses on extracting the basic features from the image. Due to the different output representations, conventional image restoration models cannot accomplish these diverse cross- domain tasks by simply expanding the training data within a streamlined framework. To mitigate the ambiguity across different output domains, substantial task-specific retraining is needed. To address the diverse challenges of general im- age processing tasks, we adopt a visual prompting question answering paradigm, which utilizes paired visual prompts to precisely indicate the tasks to be accomplished. Our uni- versal model, namely PromptGIP, can effectively handle up to 15 various image processing tasks, providing a more versatile solution for low-level vision. The experiments also indicate that in-context learning enables the model to exhibit preliminary generalization for out-of-domain tasks. 2. Related Work Image Restoration and Beyond. Over the past decade, single-purpose image restoration methods, dedicated to re- cover the original clean and natural image from degraded observation, have garnered substantial research attention. 2Unifying Image Processing as Visual Prompting Question Answering Numerous representative approaches have found applica- tions across various domains, including denoising (Zhang et al., 2017), deblurring (Kupyn et al., 2018), and deraining (Zamir et al., 2021), among others. However, the inher- ent limitation of these techniques lies in their reliance on specialized datasets and the tailored network architectures. Consequently, their generalization ability remains unsatis- factory, falling notably short of generality. Moreover, image enhancement algorithms, like low-light enhancement (Wei et al., 2018), present significant demands and applications. Paradoxically, most researchers tend to concentrate solely on a specific augmentation methodology, such as simply enlarging the training data, to seek for more robust gen- eralization. Differently, we advocate for a paradigm shift, expanding the purview beyond image restoration to em- brace image enhancement and other image processing tasks. Besides, we propose a unified framework capable of col- lectively tackling all these tasks. This pioneering approach markedly enhances the universality of low-level vision foun- dation models, bridging the gap between disparate domains. Visual In-Context Learning. In NLP, the GPT (Genera- tive Pretrained Transformer) series models, such as GPT-2 and GPT-3 (Radford et al., 2019; Brown et al., 2020), have achieved significant success in unifying various NLP tasks. By providing a prompt or designing an in-context example, which is usually a task-specific instruction or question, GPT can be transformed into a task-specific question-answering model without the need for extensive retraining or fine- tuning. In vision, a few works – MAE-VQGAN (Bar et al., 2022) and Painter (Wang et al., 2023b), have begun harness- ing the flexibility afforded by in-context learning to unify diverse vision tasks. By constructing grid-like prompts, they exhibit commendable performance on high-level tasks like semantic segmentation. However, their efficacy has been less pronounced in low-level domains, failing to ex- ploit the full potential of in-context learning. We claim that this discrepancy may be attributed to the distinct nature of low-level vision tasks, which involve pixel-wise image ma- nipulation, in contrast to the high-level tasks that demand comprehension across varying levels of abstraction. Multi-task Learning for Image Processing. Multi-Task Learning (MTL) aims to train a single model to concurrently handle multiple image processing tasks. Traditionally, MTL approaches have predominantly focused on image restora- tion, and they can be broadly categorized into two streams. BSRGAN (Zhang et al., 2021) and RealESRGAN (Wang et al., 2021b) adopt a data-centric approach. They propose to employ models with significant parameter complexity and utilize complicated degradation models to generate am- ple training data. DASR (Wang et al., 2021a) and AirNet (Li et al., 2022), on the other hand, adopt a model-centric approach. They design specialized modules to implicitly capture diverse degradations and exploit them as conditions for achieving MTL. Beyond these approaches, ProRes (Ma et al., 2023) and PromptIR (Potlapalli et al., 2023) lever- age prompts as a form of guidance or condition, enabling MTL for three (denoising, rain removal, and fog removal) tasks, or five (denoising, deraining, deblurring, low-light enhancement, and defogging) tasks. Despite these contribu- tions, existing methodologies remain limited in their ability to tackle a modest number of MTL tasks, typically up to five. In contrast, our proposed approach breaks this ceil- ing by achieving MTL across more than ten distinct tasks (denoising, deblurring, deJPEG, dering, deraining, defog- ging, deraining, inpainting, low-light enhancement, local Laplacian filtering, and edge detection). 3. Method 3.1. Image Processing as Visual Question Answering Compared to high-level vision tasks, low-level vision tasks necessitate meticulous pixel-level adjustments, demanding architectures that excel in processing intricate details. These tasks encounter diverse input/output domains, characterized by various degradations and complex operations. These challenges underscore the complexity and non-trivial nature of general image processing. Inspired by the success of prompting in NLP (Liu et al., 2023), we propose to unify the general image processing problem as the visual prompting question answering (QA) paradigm, as illustrated in Fig. 2. In QA, the objective is to process a given context, such as a paragraph or document, and accurately generate the correct answer in response to specific questions related to that context. Building upon this concept, we adapt the QA paradigm to image processing. In our design, we view an image as a “question” (Q) or an “answer” (A). When inference, the model G is initially provided with input-output image pairs (PQ and PA), which serve as essential task prompts, much like the given context in QA tasks. These image pairs play a pivotal role in guiding the model’s image processing operations. To process a new targeted input image XQ, we encode it as the query “question” to be answered. The provided input-output image pairs then serve as contextual prompts, enabling the model to gain insightful cues to generate the desired output. With this knowledge, the model executes the appropriate image processing operations to produce the predicted “answer” YA: YA = G(XQ|{PQ, PA}). (1) An illustrative example is shown in Fig. 3. The content of the prompts for the model is represented in the form of “question”-“answer” image pairs. For instance, when the input prompt is a “rainy”-“rain-free” image pair, the model will perform rain removal on the target input image. If the answer in the prompt is related to image edges, the model 3Unifying Image Processing as Visual Prompting Question Answering Patch Embedding Random Masking Transformer Block Loss Q A Q A Training Phase Patch Embedding Adding Mask Transformer Block Q A Q Inference Phase Predicted patchMasked patch ? Figure 3.We structure the input and output images as a “Q-A-Q-A” sequence. During training, the answer images (A) are randonly masked and predicted. For inference, PromptGIP can execute proper processing to the question image according to the prompt pairs. will conduct edge detection operations on the query image, producing the corresponding edge image as the output. Notably, PromptGIP is capable of handling tasks with dis- tinct output domains, which was not achievable with previ- ous image restoration methods. The output domain of image restoration is the natural image space; image enhancement involves transformations in image brightness, color tones, or styles; while image edge detection outputs edge features, not the RGB image space. Our approach unifies these different tasks within a unified framework. 3.2. Masked Visual Prompting Paradigm Masked image modeling has emerged as a promising self- supervised technique for learning valuable visual represen- tations. Following (He et al., 2022), we implement a similar masked autoencoding approach in our training process. As depicted in Fig. 3, we initially structure the input and output images within a “Q-A-Q-A” sequence. Then, we introduce random masking to certain portions of the answer images, prompting the model to reconstruct these masked patches from the unmasked counterparts. This procedure employs a mask ratio of 85%. It is pivotal to note that our organiza- tional framework distinguishes itself from prior works (Bar et al., 2022; Wang et al., 2023b) in its more rational and effective design. More analyses are described in Sec. 3.3. During the training phase, our approach leverages a diverse dataset comprising input-output image pairs, where each pair corresponds to a distinct image processing goal, includ- ing restoration, enhancement, and edge detection. Notably, each primary task encompasses various sub-tasks that fur- ther enrich the model’s understanding. Throughout this process, the model is trained to grasp the intrinsic correla- tions between the Q-A image pairs. During the inference stage, we assemble an input-output pair as a task prompt, guiding the model to execute tailored operations. By pro- viding an input question image alongside a fully masked image, the model generates the intended answer image in correspondence with the question image. 3.3. Further Discussion Comparison with image restoration models. Earlier re- search primarily focused on crafting specialized models tailored to specific tasks, such as SRCNN (Dong et al., 2015) for super-resolution, DnCNN (Zhang et al., 2017) for denoising, and Deblur-GAN (Kupyn et al., 2018) for deblurring. While effective within constrained scenarios, these task-specific models possess limited generalization capability. Recent attention has pivoted toward all-in-one restoration methods (Li et al., 2022; Wang et al., 2021b). These approaches leverage multi-task learning techniques to construct models that are able to handle diverse restoration tasks, thereby circumventing the need for task-specific fine- tuning. Nonetheless, these models are often limited within predefined application domains. They fall short in produc- ing alternative representations like stylistic images or image edges. Several concurrent works (Ma et al., 2023; Potlapalli et al., 2023) have embraced the concept of prompt learn- ing, but still concentrate on image restoration tasks. They propose to incorporate learnable prompts as degradation embeddings to guide the restoration process. However, it is worth noting that general image processing encompasses more than just restoration tasks. In this context, Prompt- GIP demonstrates a remarkable adaptability across a wide spectrum of low-level vision tasks, liberating it from the constraints of a singular output domain. 4Unifying Image Processing as Visual Prompting Question Answering Painter Input prompts  Input prompts Query  Query Output  Output MAE-VQGAN Input prompts: Denoising  Input prompts: Depth estimation Query  Query Output：Segmentation  Output：Segmentation Figure 4.The drawbacks of existing methods. MAE-VQGAN fails to produce high-quality images. The prompts of Painter do not actually work well. Comparison with existing visual prompting models. Two novel visual prompting techniques, MAE-VQGAN (Bar et al., 2022) and Painter (Wang et al., 2023b), have emerged for addressing various tasks. MAE-VQGAN employs a masked autoencoder for pretraining. Unlike predicting masked pixels, it predicts visual tokens from a pretrained VQGAN codebook. The training process involves the Im- ageNet dataset and the collected CVF dataset, compris- ing a diverse array of figures from computer vision papers. Painter combines pairs of images to predict the output do- main through the masked image modeling. It encompasses high-level tasks and a few low-level tasks. Differences with MAE-VQGAN.MAE-VQGAN diverges from the question-answering paradigm. As illustrated in Fig. 2, MAE-VQGAN utilizes crude images extracted from the ImageNet/CVF dataset during MAE training, and stitches paired images as a whole image for inference. This straightforward and coarse data organization scheme de- viates from the QA framework. Specifically, during the training phase, the model lacks the capability to differenti- ate whether a given visual token corresponds to a “Question” or an “Answer”, leading to an interleaved and ambiguous input-output encoding. In contrast, our approach is firmly grounded in an explicit QA paradigm, enabling precise pixel- level prediction. In addition, MAE-VQGAN choose to pre- dict VQGAN tokens rather than pixels, which results in subpar fidelity of reconstructed content, as exemplified in Fig. 4. On the contrary, our framework excels in pixel-wise prediction with visually compelling outcomes. Differences with Painter.Painter predominantly targets high- level vision tasks, encompassing segmentation, depth es- timation, and keypoint detection, while it also addresses limited low-level task. Painter only draws training data from just seven specific datasets. This potentially induces a propensity for excessive alignment with these datasets, which can easily result in a concomitant risk of overfitting. Furthermore, the implementation of Painter employs a “Q- Q-A-A” sequence for encoding prompt and query images (see Fig. 2). Such a design yields unexpected behaviors in Painter’s response to task prompts. Extensive tests on Painter revealed disparities in its prompt mechanism com- pared to anticipated outcomes. As in Fig. 4, when provided prompts linked to denoising and depth estimation, it unex- pectedly executes segmentation tasks. This phenomenon hints at the model’s inclination to memorize specific datasets rather than effectively leveraging the provided prompts. We conjecture that this problem might stem from the limited range of training tasks. 4. Experiments and Analysis 4.1. Image Processing Task Settings To show the versatility of our proposed method, we incor- porate up to 15 tasks including diverse image restoration, image enhancement, and image edge detection tasks into our experiments. These tasks have their distinct output domains. Image restoration. We consider 10 degradation types: Gaussian noise, Gaussian blur, Poisson noise, salt & pep- per noise, jpeg compression, ringing artifacts, R-L algo- rithm (Richardson, 1972), inpainting, haze, and rain. For the first eight types, we directly introduce corresponding distortions to the ImageNet (Deng et al., 2009) dataset to create degraded-clean pairs. We collect a composed dataset (Common528) for testing, which consists of commonly- used datasets: Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), BSDS100 (Martin et al., 2001), Manga109 (Matsui et al., 2017), Urban100 (Huang et al., 2015), Gen- eral100 (Dong et al., 2016), and DIV2K-Valid (Agustsson & Timofte, 2017). For dehazing, we utilize the ITS training set of RESIDE dataset (Li et al., 2018). For rain removal, we employ two types of rain addition models: Simple Rain Model and Complex Rain Model. The former is a straight- forward additive rain model, directly synthesized on the ImageNet dataset; while the latter utilizes Rain13K (Zamir et al., 2021), including an assortment of diverse rain models. Image enhancement. We employ two enhancement tasks: low-light image enhancement (LLE) and local Laplacian filtering (LLF). For LLE, the LOL dataset (Wei et al., 2018) is adopted for training. For LLF, we apply local Laplacian filter (Aubry et al., 2014) on the expert-C retouched images of Adobe-MIT Fivek dataset (Bychkovsky et al., 2011), forming the requisite input-output pairs. LLF is a multi- scale operator for edge-preserving detail enhancement. 5Unifying Image Processing as Visual Prompting Question Answering Question-Answer Prompts Input Question Predicted Answer GT Gaussian Blur Gaussian Noise S&P Noise Jpeg Compression Ringing Artifacts Inpainting Haze Rain Figure 5.Visual results of PromptGIP on all-in-one multi-task restoration. 6Unifying Image Processing as Visual Prompting Question Answering Table 1.Quantitative results (PSNR/SSIM) on image restoration tasks. ⋆: trained with only restoration tasks. ♠: trained with all image processing tasks. †: public released model. Gaussian Noise Poisson Noise S&P Noise Gaussian Blur JPEG Ringing R-L Inpainting Simple Rain Complex Rain Haze Real-ESRGAN† 25.38/0.7997 26.57/0.8472 21.50/0.5884 21.49/0.6263 25.21/0.8058 24.64/0.7834 21.71/0.6548 14.06/0.7084 16.10/0.5989 21.01/0.6705 11.86/0.6346 Restormer⋆ 28.66/0.8731 31.31/0.9317 36.12/0.9851 24.24/0.7537 26.65/0.8391 27.14/0.8561 30.53/0.9306 27.77/0.9289 29.68/0.9476 24.26/0.8369 14.83/0.7382 ViT-large⋆ 24.67/0.7804 25.39/0.8152 23.71/0.7335 22.17/0.6413 24.76/0.7920 23.89/0.7463 24.09/0.7335 23.11/0.7662 23.21/0.7620 23.04/0.7788 24.91/0.8565 Restormer♠ 25.27/0.7634 27.22/0.8535 27.84/0.8811 21.71/0.6078 23.90/0.7606 23.61/0.7261 23.18/0.7120 24.19/0.8615 22.68/0.7879 20.39/0.6930 7.22/0.1395 Painter♠ 24.17/0.7468 24.63/0.7792 24.75/0.7903 22.36/0.6477 23.97/0.7458 24.21/0.7531 24.56/0.7728 22.95/0.7455 23.35/0.7493 22.81/0.7710 20.60/0.8250 PromptGIP♠ 26.22/0.8167 27.29/0.8590 27.49/0.8804 22.77/0.6911 25.38/0.7978 25.45/0.8079 26.79/0.8506 25.02/0.8401 25.46/0.8399 24.08/0.8322 24.32/0.9020 Table 2.Quantitative results on image enhancement and image edge detection. ⋆: single models trained with individual tasks. ♠: trained with all image processing tasks. LLE (LOL dataset) LLF Canny Laplacian PSNR↑ SSIM↑ PSNR↑ SSIM↑ MAE↓ MAE↓ ViT-large⋆ 13.37 0.4892 25.42 0.8948 36.5290 1.4655 Painter♠ 19.47 0.7491 23.87 0.8451 33.7188 5.4518 PromptGIP♠ 20.30 0.8026 26.11 0.9107 21.4376 3.7852 Image edge detection. Two acknowledged image edge detection operators, the Canny and Laplacian operators, are investigated. The ImageNet dataset forms the basis for creating input-output training pairs. All these 15 diverse tasks are amalgamated within a unified setting. PromptGIP excels in accommodating these tasks under a cohesive framework with one single training phase. 4.2. Implementation Details A vanilla vision Transformer (ViT-large) (Dosovitskiy et al., 2020) is adopted as the backbone architecture. During train- ing, the model processes sequences of four 256 × 256 im- ages in a “Q-A-Q-A” pattern, resulting in a 4 × 256 × 256 total input resolution. L1 loss is utilized as the loss function. For optimization, AdamW (Loshchilov & Hutter, 2017) op- timizer with a cosine learning rate scheduler is employed. The base learning rate is1e−4. The batch size is 48. We use 8 Tesla V100 GPUs for training. A total of 50 epochs are executed. For testing Painter and PromptGIP, we construct 20 image prompts for each task and report the best results. 4.3. Experiments Currently, there is no existing unified network that can com- prehensively address all the aforementioned tasks in an all-in-one manner. For instance, previous image restoration models are incapable of handling image edge detection task. For reference, we train a ViT-large model and a Restormer model (Zamir et al., 2022) using the same training policy on multiple restoration tasks. We retrain the Painter (Wang et al., 2023b) model with all tasks as PromptGIP. We also report the results of Real-ESRGAN (Wang et al., 2021b), which is proposed to handle various complex restoration. Due to differences in the performance of various architec- tures, absolute numerical comparisons would be unfair. We have opted for the simplest ViT structure, thus it is more fair to focus on a direct comparison with the ViT and Painter. Moreover, achieving state-of-the-art performance on every Question-Answer Prompts Input Question Predicted Answer GT Low-light LLF Canny Operator Laplacian  Operator Figure 6.Visual results of PromptGIP on image enhancement and edge detection tasks. task is not the purpose of this paper. Our primary focus revolves around examining the effects and capability of prompt learning in the context of general image processing. We can focus more on functional outcomes rather than nu- merical results. The metrics are evaluated on RGB channels. Results. Illustrated in Fig. 5 and 6, PromptGIP profi- ciently addresses a range of image processing tasks using different input prompts. These tasks encompass multiple- degradation restoration, enhancement, and edge detection. These tasks entail distinct output representations, a level of complexity that lies beyond the capability of existing image restoration methods. PromptGIP yields impressive visual results in diverse tasks. In the training process, we introduced mixed degradation scenarios to further challenge the model’s restoration capability, with results presented in Fig. 7. Quantitative results for restoration are detailed in Tab. 1, where PromptGIP demonstrates appealing performance across 10 restoration tasks using a vanilla ViT backbone. Compared to the original ViT model and Painter, prompt learning demonstrates a significant enhancement in model performance, resulting in improved restoration and multi- tasking capability. PromptGIP also surpasses the perfor- mance of Real-ESRGAN, a model specifically crafted for blind image restoration. PromptGIP achieves higher quanti- tative score than Restormer on complex derain and dehaze tasks. Restormer achieves superior quantitative scores on 7Unifying Image Processing as Visual Prompting Question Answering Table 3.Effectiveness of the proposed QA paradigm and masked training strategy. Encoding Paradigm Mask StrategyPoisson NoisePSNR↑ HazePSNR↑ LLFPSNR↑ LaplacianMAE↓Painter Q1-Q2-A1-A2 A1&A2 24.63 20.60 23.87 5.4518Direct predictingQ1-A1-Q2-A2 only A2 26.30 18.57 25.38 11.5553PromptGIPQ1-A1-Q2-A2 A1&A2 27.29 24.32 26.11 3.7852 other degradations. This can be attributed to Restormer’s advanced architecture tailored for restoration tasks. PromptGIP also succeeds in enhancing low-light images and emulating image operators. Notably, earlier methods struggle to simultaneously realize all these tasks within a single framework, due to variances in output domain rep- resentations. However, PromptGIP, when provided with proper prompts, effectively executes a wide spectrum of image processing tasks within a singular, streamlined net- work, as depicted in Fig. 6. For image edge detection, the Canny operator produces clear and well-defined edges, while Laplacian operator tends to produce thicker and nois- ier edges. Despite these intricacies, PromptGIP successfully discerns and faithfully simulates the distinct behaviors of both operators, underscoring its impressive adaptability. The numerical results are shown in Tab. 2. Query Input  Output  Query Input  Output Query Input  Output  Query Input  Output Figure 7.Results of mixed degraded images. Effectiveness of the QA paradigm and masked training. We further validate the efficacy of our newly proposed QA paradigm and the masked training strategy. Unlike the en- coding order of Q-Q-A-A used in Painter, our PromptGIP employs a Q-A-Q-A approach. Q-Q-A-A could dilute the model’s focus and impair its ability to directly map questions to their relevant answers. Our paradigm significantly im- proves performance, as in Tab. 1 and 2, which demonstrate superior outcomes in both image restoration and enhance- ment tasks. Additionally, we emphasize the necessity of our masked training strategy. During the training phase, Prompt- GIP randomly masks patches in the two “answer” images, in contrast to direct predicting where only the last “answer” image is masked. This methodology, as shown in Tab. 3, proves more effective across all tasks, particularly in image dehazing, where direct predicting struggles to yield satisfac- tory results. This outcome suggests that masked training not only enhances the model’s capability in handling diverse tasks but also contributes to its generalization and stability. Prompts: Colorization  Query  Output  GT Prompts : Style transfer  Query  Output  GT Out-of-distribution Tasks Prompts: Mixed degradation  Query  Output  GT Prompts: Mixed degradation  Query  Output  GT Figure 8.Although PromptGIP cannot perfectly deal with every out-of-distribution tasks, it has demonstrated a certain level of generalization capability. Exploration on out-of-distribution tasks. To evaluate the model’s capacity for generalization, we incorporate a set of diverse out-of-distribution tasks that are intentionally not encountered during the training phase, including mixed degradation restoration, colorization, and style transfer. The results are presented in Fig. 8. We employ L0 smooth fil- tering (Xu et al., 2011) to conduct style transfer experiment. As shown in Fig. 8, the model seems to understand the input prompt pair, yielding images with a discernible L0 smooth filter style. While it occasionally succeeds in producing visually appealing reconstructed images, it encounters dif- ficulties in effectively restoring unfamiliar mix degraded images when compared to seen degraded data. Additionally, we provide a grayscale-colorful image pair as a prompt, with the expectation that the model would apply colorization to the grayscale input. However, the model regrettably does not exhibit colorization behavior in response to this prompt. In summary, these observations highlight the model’s ca- pacity to discern the intended task from the prompt and endeavor to fulfill it, showcasing a certain level of general- ization. It is essential to emphasize that the model’s present capability does not extend to generating great “emergent” outcomes. These conclusions are in accordance with prior studies (Min et al., 2022; Wei et al., 2023). 5. Conclusion We present PromptGIP, a versatile model designed to ad- dress a wide spectrum of image processing tasks. By adopt- ing a unique visual prompting question answering paradigm, PromptGIP adeptly handles tasks like image restoration, enhancement, and edge detection. Our comprehensive ex- perimental evaluations affirm PromptGIP’s commendable capacity to interpret the implicit task cues embedded within visual prompts, yielding pertinent outputs that underscore a 8Unifying Image Processing as Visual Prompting Question Answering noteworthy level of generalization. This study sheds light on exploration and refinement of universal image processing models in the future. 6. Broader Impact The proposed PromptGIP framework represents a new ad- vancement in the field of image processing, offering a versa- tile and efficient solution for a wide range of low-level vi- sion tasks. By adapting the training data, PromptGIP could facilitate new applications in medical imaging, environmen- tal monitoring, and content creation, making high-quality image processing more accessible to non-experts and con- tributing positively to fields where image quality is crucial. Additionally, this research could inspire further interdisci- plinary work, integrating concepts from natural language processing and computer vision to solve complex problems in novel ways. References Agustsson, E. and Timofte, R. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 126–135, 2017. Aubry, M., Paris, S., Hasinoff, S. W., Kautz, J., and Durand, F. Fast local laplacian filters: Theory and applications. ACM Transactions on Graphics (TOG), 33(5):1–14, 2014. Bar, A., Gandelsman, Y ., Darrell, T., Globerson, A., and Efros, A. Visual prompting via image inpainting. Ad- vances in Neural Information Processing Systems, 35: 25005–25017, 2022. Bevilacqua, M., Roumy, A., Guillemot, C., and Alberi- Morel, M. L. Low-complexity single-image super- resolution based on nonnegative neighbor embedding. 2012. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901, 2020. Bychkovsky, V ., Paris, S., Chan, E., and Durand, F. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR 2011, pp. 97–104. IEEE, 2011. Chen, A., Yao, Y ., Chen, P.-Y ., Zhang, Y ., and Liu, S. Un- derstanding and improving visual prompting: A label- mapping perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19133–19143, 2023. Chen, H., Wang, Y ., Guo, T., Xu, C., Deng, Y ., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12299–12310, 2021. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Dong, C., Loy, C. C., He, K., and Tang, X. Image super- resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295–307, 2015. Dong, C., Loy, C. C., and Tang, X. Accelerating the super- resolution convolutional neural network. In Computer Vision–ECCV 2016: 14th European Conference, Amster- dam, The Netherlands, October 11-14, 2016, Proceed- ings, Part II 14, pp. 391–407. Springer, 2016. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. He, K., Chen, X., Xie, S., Li, Y ., Doll´ar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000–16009, 2022. Huang, J.-B., Singh, A., and Ahuja, N. Single image super- resolution from transformed self-exemplars. In Proceed- ings of the IEEE conference on computer vision and pat- tern recognition, pp. 5197–5206, 2015. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y ., et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Kupyn, O., Budzan, V ., Mykhailych, M., Mishkin, D., and Matas, J. Deblurgan: Blind motion deblurring using con- ditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8183–8192, 2018. Li, B., Ren, W., Fu, D., Tao, D., Feng, D., Zeng, W., and Wang, Z. Benchmarking single-image dehazing and be- yond. IEEE Transactions on Image Processing, 28(1): 492–505, 2018. Li, B., Liu, X., Hu, P., Wu, Z., Lv, J., and Peng, X. All- in-one image restoration for unknown corruption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17452–17462, 2022. 9Unifying Image Processing as Visual Prompting Question Answering Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., and Timofte, R. Swinir: Image restoration using swin trans- former. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1833–1844, 2021. Liu, J., Shen, D., Zhang, Y ., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt- 3? arXiv preprint arXiv:2101.06804, 2021. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023. Liu, Y ., He, J., Chen, X., Zhang, Z., Zhao, H., Dong, C., and Qiao, Y . Very lightweight photo retouching network with conditional sequential modulation. IEEE Transactions on Multimedia, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. arXiv preprint arXiv:1711.05101, 2017. Ma, J., Cheng, T., Wang, G., Zhang, Q., Wang, X., and Zhang, L. Prores: Exploring degradation-aware visual prompt for universal image restoration. arXiv preprint arXiv:2306.13653, 2023. Martin, D., Fowlkes, C., Tal, D., and Malik, J. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE Inter- national Conference on Computer Vision. ICCV 2001, volume 2, pp. 416–423. IEEE, 2001. Matsui, Y ., Ito, K., Aramaki, Y ., Fujimoto, A., Ogawa, T., Yamasaki, T., and Aizawa, K. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 76(20):21811–21838, 2017. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. Potlapalli, V ., Zamir, S. W., Khan, S., and Khan, F. S. Promptir: Prompting for all-in-one blind image restora- tion. arXiv preprint arXiv:2306.13090, 2023. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Richardson, W. H. Bayesian-based iterative method of image restoration. JoSA, 62(1):55–59, 1972. Sun, Y ., Chen, Q., Wang, J., Wang, J., and Li, Z. Exploring effective factors for improving visual in-context learning. arXiv preprint arXiv:2304.04748, 2023. Wang, L., Wang, Y ., Dong, X., Xu, Q., Yang, J., An, W., and Guo, Y . Unsupervised degradation representation learning for blind super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10581–10590, 2021a. Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 14408–14419, 2023a. Wang, X., Xie, L., Dong, C., and Shan, Y . Real-esrgan: Training real-world blind super-resolution with pure syn- thetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1905–1914, 2021b. Wang, X., Wang, W., Cao, Y ., Shen, C., and Huang, T. Im- ages speak in images: A generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 6830–6839, 2023b. Wang, Y ., Li, K., Li, Y ., He, Y ., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y ., Wang, Z., et al. Internvideo: General video foundation models via generative and discrimina- tive learning. arXiv preprint arXiv:2212.03191, 2022a. Wang, Z., Cun, X., Bao, J., Zhou, W., Liu, J., and Li, H. Uformer: A general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 17683–17693, 2022b. Wei, C., Wang, W., Yang, W., and Liu, J. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. Wei, J., Wei, J., Tay, Y ., Tran, D., Webson, A., Lu, Y ., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Xu, L., Lu, C., Xu, Y ., and Jia, J. Image smoothing via l 0 gradient minimization. In Proceedings of the 2011 SIGGRAPH Asia conference, pp. 1–12, 2011. Yang, J., Gao, M., Li, Z., Gao, S., Wang, F., and Zheng, F. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023. Yu, T., Feng, R., Feng, R., Liu, J., Jin, X., Zeng, W., and Chen, Z. Inpaint anything: Segment anything meets im- age inpainting. arXiv preprint arXiv:2304.06790, 2023. Zamir, S. W., Arora, A., Khan, S., Hayat, M., Khan, F. S., Yang, M.-H., and Shao, L. Multi-stage progressive image 10Unifying Image Processing as Visual Prompting Question Answering restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14821– 14831, 2021. Zamir, S. W., Arora, A., Khan, S., Hayat, M., Khan, F. S., and Yang, M.-H. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5728–5739, 2022. Zeyde, R., Elad, M., and Protter, M. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pp. 711–730. Springer, 2010. Zhang, K., Zuo, W., Chen, Y ., Meng, D., and Zhang, L. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142–3155, 2017. Zhang, K., Liang, J., Van Gool, L., and Timofte, R. Design- ing a practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pp. 4791–4800, 2021. Zhang, Y ., Zhou, K., and Liu, Z. What makes good ex- amples for visual in-context learning? arXiv preprint arXiv:2301.13670, 2023. 11Unifying Image Processing as Visual Prompting Question Answering A. Details of Image Processing Tasks A.1. Image Restoration Tasks For image restoration, we consider the following 10 different degradation types. Gaussian Noise. Gaussian noise is a type of random variation that affects the pixel values of an image. It is characterized by a probability distribution known as the Gaussian distribution or normal distribution. In images, Gaussian noise appears as a random variation in pixel values, where the noise values follow the Gaussian distribution pattern. We add Gaussian noise to the clean images to synthesize noisy images. The noise level is uniformly sampled from [10, 50]. Gaussian Blur. Gaussian blur is a common image filtering technique used to reduce high-frequency noise and details in an image. It employs a mathematical function known as the Gaussian kernel to apply weighted averaging to the pixel values within a specified neighborhood. We apply isotropic Gaussian blur kernel on the clean image to synthesize blurry images. The kernel width is uniformly sampled from [2, 4]. Poisson Noise. Poisson noise is manifested as random variations in pixel intensities, leading to a grainy appearance. It follows a Poisson distribution, where the variance is proportional to the mean intensity of the signal. Following the common settings, we set the noise level to 2. Salt & Pepper Noise. Salt and Pepper noise, also known as impulse noise, is a type of irregular interference commonly found in digital images. This noise is characterized by random occurrences of very bright (salt) and very dark (pepper) pixels. We simulate the salt and pepper noise with a signal noise ratio of 0.95. The probabilities of producing salt and pepper are both 50%. Jpeg Compression Artifacts. JPEG compression artifacts are distortions or anomalies that arise due to the lossy nature of the JPEG compression process. These artifacts typically manifest as blocky patterns, color shifts, and blurriness, especially in areas with high contrast or fine details. The visual quality factor is uniformly sampled from [10, 40]. Ringing Artifacts. Ringing artifacts are most noticeable as a pair of bright and dark bands adjacent to the edges of objects in the image. These bands result from the reconstruction process during compression or other image transformations. When a sharp edge is compressed or enhanced, the algorithm may introduce extra pixel values that weren’t present in the original image. This results in an overemphasis of the edge and the creation of the distinct bright and dark bands. We employ the implementation of ring artifacts from Real-ESRGAN (Wang et al., 2021b). R-L Algorithm. The Richardson-Lucy (R-L) algorithm aims to estimate the original, sharp image from a degraded version by iteratively updating the estimate based on the observed degraded image and a point spread function (PSF) that characterizes the blurring. It iteratively refines the estimated image to minimize the difference between the observed and estimated images, while accounting for the effects of blurring and noise. We directly employ python built-in function skimage.restoration.richardson lucy to attain the processed results. Inpainting. We randomly add masked streaks in the clean image, obtaining masked images to be fulfilled. The number of streaks ranges from 5 to 10 and the thickness is sampled from [5, 10]. Haze. For image dehazing, we utilize the ITS training set of RESIDE dataset (Li et al., 2018) for training and the SOTS-indoor dataset for testing. Rain. We employ two types of rain addition models: Simple Rain Model and Complex Rain Model. The Simple Rain Model is a straightforward additive rain model, and we directly synthesize it on the clean images. The Complex Rain Model utilizes Rain13K dataset (Zamir et al., 2021), including an assortment of diverse rain models. Test100 dataset is adopted for evaluation. A.2. Image Enhancement Tasks Low-light Image Enhancement. The goal of low-light image enhancement is to adjust the image’s brightness, contrast, and color balance while preserving important details and minimizing noise. We adopt the commonly-used LOL dataset (Wei et al., 2018) for training and testing. Local Laplacian Filtering. In local Laplacian filtering (Aubry et al., 2014), a Laplacian pyramid is constructed for the input image, representing different scales of details. This pyramid is modified using a control grid that adjusts the appearance of image regions based on their contrast and brightness levels. By applying different filters to the pyramid’s levels, the method 12Unifying Image Processing as Visual Prompting Question Answering enhances the image’s finer details while respecting its global structure. Following (Liu et al., 2022), we apply this operator on images retouched by expert C of the MIT-Adobe FiveK dataset (Bychkovsky et al., 2011). The training and testing sets of MIT-Adobe FiveK dataset are adopted. A.3. Image Edge Detection Canny Operator. The Canny edge detection operator is a widely used method in image processing for detecting edges in digital images. It aims to identify significant changes in intensity within an image, which often correspond to object boundaries or important features. The Canny operator is known for its ability to detect edges accurately, suppressing noise and responding well to significant changes in intensity. We set the threshold 1 and the threshold 2 of Canny operator as 50 and 200. Laplacian Operator. The Laplacian operator is a mathematical filter commonly used for detecting regions of rapid intensity changes. Mathematically, the Laplacian operator calculates the second derivative of the image intensity with respect to its spatial coordinates (x and y). B. Limitations and Prospectives PromptGIP exhibits considerable potential in addressing a diverse range of image processing tasks through its innovative visual prompting question-answering paradigm. However, there are certain limitations and areas for further exploration that merit attention. These limitations and potential areas for improvement could provide valuable insights for future research and development efforts. PromptGIP excels at tasks guided by explicit prompts, but its current scope does not extend to generating unexpected or emergent outcomes. Our findings suggest that the model still lacks the ability to generate novel solutions beyond what it has learned from training data. This observation is consistent with previous investigations of language models (Min et al., 2022). Recent works demonstrate that the effectiveness of in-context learning for large language models heavily relies on the quality, diversity, and quantity of the training data (Wei et al., 2023). Inadequate or biased training data can lead to suboptimal performance on certain tasks or scenarios. Due to constrained computational resources and limited training data, we are currently unable to conduct extensive large scaling-up experiments. However, we believe that in-context learning still has the potential to show more impressive effects. Another limitation lies in the current backbone choice of ViT. ViT (Dosovitskiy et al., 2020) splits the input image into 16×16 patches and transforms them into a sequence of linear embeddings. The rough patch-splitting strategy would cause a significant loss of high-frequency information, such as edges, textures and structures, leading to severe artifacts and over-smoothed results. Consequently, current Transformer-based low-level models (Zamir et al., 2022; Wang et al., 2022b; Liang et al., 2021; Chen et al., 2021) still adopt CNN for pre/post-processing. In this paper, we mainly focus on validating the effectiveness of prompt learning in multitask image processing, thus we simply adopt the basic ViT architecure to conduct experiments. However, the original ViT backbone cannot achieve supreme performance on low-level vision tasks, leading to subpar quantitative results compared to state-of-the art Restormer backbone. This issue could be addressed by adopting stronger backbone models in the future work. C. Effects of Visual Prompts C.1. Validating the Efficacy of Visual Prompts in PromptGIP As discussed in the main paper, we find that the utilization of prompts in Painter does not yield the anticipated outcomes. Instead, the model tends to exhibit suboptimal behavior by overly adapting to specific tasks and the corresponding datasets. Specifically, when presented with a noisy image pair, the model is intended to perform denoising on the input query image. However, a noteworthy observation is that when the input query image portrays an indoor bedroom scene, the model erroneously engages in depth estimation rather than denoising. This behavior arises due to the fact that the training data for depth estimation tasks predominantly consist of indoor scene images. Consequently, the model associates depth estimation tasks with indoor images. As a result, when the input query image resembles an indoor setting, the model instinctively conducts depth estimation processing. Consequently, it becomes evident that the utilization of visual prompts in Painter does not yield the intended results. To assess the potency of visual prompts within the PromptGIP framework, we conducted a series of tests. Illustrated in Fig. 13Unifying Image Processing as Visual Prompting Question Answering Question-Answer Prompts Input Question Predicted Answer S&P Noise Inpainting S&P Noise Input prompts: Canny Input prompts: Laplacian Input prompts: Inpainting Input prompts: Denoise Input prompts: Low-light Enhancement Gaussian Blur Gaussian Blur Figure 9.Effectiveness of visual prompts in PromptGIP. PromptGIP can correctly identity and execute the designed tasks, instead of memorizing the types of input image. 14Unifying Image Processing as Visual Prompting Question Answering 9, our experiments span different scenarios. When tasked with a canny operator prompt, PromptGIP adeptly applies the Canny operator to input query images afflicted with salt and pepper noise. Notably, the model’s performance is not driven by data memorization for denoising operations based on the query image content. Further evaluations reveal intriguing dynamics. In instances where the task prompt involves denoising and the query image exhibits Gaussian blur, PromptGIP appropriately refrains from processing the image. This behavior underscores the model’s grasp of the task prompt’s intent, resisting unnecessary processing when incongruities are detected. Moreover, when presented with a pair of low-light and high-light images in a question-answer format, PromptGIP deftly heightens the brightness of the query image. It is intriguing to observe that this occurs even when the input query image bears blurriness. Notably, the model abstains from undertaking deblurring actions, signaling its grasp of the task prompt’s essence. These comprehensive assessments affirm that PromptGIP possesses the ability to comprehend and act upon task prompts without succumbing to the perils of overfitting to specific training data. >  > >  > Gaussian NoiseGaussian Blur Figure 10.Different visual prompts will lead to different results. The figure above ranks the effects of different visual prompts on Gaussian denoising and Gaussian deblurring tasks. Prompt ID PSNR SSIM Idx 0 26.1142 0.8131 Idx 1 25.9345 0.8043 Idx 2 25.9343 0.8097 Idx 3 26.1386 0.8127 Idx 4 25.8980 0.8099 Idx 5 25.8063 0.8101 Idx 6 26.0166 0.8102 Idx 7 25.9953 0.8087 Idx 8 25.9991 0.8121 Idx 9 26.2194 0.8167 Idx 10 26.0864 0.8105 Idx 11 26.0188 0.8110 Idx 12 26.0002 0.8115 Idx 13 26.0299 0.8143 Idx 14 26.0116 0.8129 Idx 15 25.6136 0.8048 Idx 16 26.1333 0.8124 Idx 17 25.8457 0.7930 Idx 18 23.2508 0.7458 Idx 19 26.0260 0.8061 Avg. 25.8536 0.8064 Std. 0.6110 0.0147 Table 4.Influence of employing different visual prompts on Gaussian denoise task. Prompt ID PSNR SSIM Idx 0 22.0729 0.6280 Idx 1 22.7198 0.6901 Idx 2 22.4821 0.6810 Idx 3 22.4858 0.6894 Idx 4 21.6575 0.6089 Idx 5 22.5316 0.6726 Idx 6 22.6002 0.6909 Idx 7 22.6652 0.6779 Idx 8 22.6273 0.6728 Idx 9 22.7522 0.6866 Idx 10 21.9502 0.6826 Idx 11 22.7247 0.6793 Idx 12 22.7384 0.6809 Idx 13 22.4326 0.6861 Idx 14 22.4880 0.6803 Idx 15 22.5095 0.6802 Idx 16 22.7451 0.6810 Idx 17 22.7658 0.6911 Idx 18 20.8271 0.6286 Idx 19 22.6869 0.6851 Avg. 22.4231 0.6737 Std. 0.4647 0.0226 Table 5.Influence of employing different visual prompts on Gaussian deblur task. 15Unifying Image Processing as Visual Prompting Question Answering C.2. Importance of Prompt Quality in Prompt Learning The success of prompt learning hinges upon the quality and relevance of the provided prompts. The impact of ambiguous or poorly defined prompts on the final results underscores the critical role of prompt selection in this paradigm. In this context, our main paper showcases a meticulous process of prompt selection, where we curate 20 distinct prompts for each task, subsequently reporting the most favorable quantitative outcomes. The detailed outcomes of these prompt variations across several representative tasks are outlined in Tab. 4 and Tab. 5. This exploration of prompt quality aligns with similar investigations conducted in fields like NLP and high-level vision (Liu et al., 2021; Sun et al., 2023; Chen et al., 2023; Zhang et al., 2023). Building upon this, we delve into a comparative analysis of visual prompts’ impact on Gaussian denoising and Gaussian deblurring tasks in Fig. 10. Notably, the analysis underscores that the effectiveness of a visual prompt is closely tied to its richness in texture and color. Those prompts that embody these qualities consistently produce superior outcomes, thereby significantly enhancing overall model performance. The strategic crafting of prompts thus emerges as a pivotal determinant in unlocking the full potential of models such as ours. D. More Visual Results In order to comprehensively assess the performance of PromptGIP, we present an array of qualitative visual results across distinct tasks including image restoration, image enhancement, and image edge detection. In conjunction with this, a comparative evaluation is conducted against the well-established ViT-large model (Dosovitskiy et al., 2020) and the Restormer model (Zamir et al., 2022). The visual outputs and comparisons are thoughtfully illustrated in Fig. 11 and 12. PromptGIP’s proficiency in generating visually appealing outputs is readily evident in the presented results. Notably, the visual quality not only surpasses that of the baseline ViT model but also stands in competitive parity with the Restormer model. However, the significance of PromptGIP’s capability extends beyond visual quality. The distinctive strength of PromptGIP becomes evident in its ability to effectively handle an extensive array of image enhancement tasks and image detection. This becomes a noteworthy distinction from traditional models, which often struggle to concurrently address such a diverse spectrum of tasks. 16Unifying Image Processing as Visual Prompting Question Answering Gaussian BlurGaussian NoiseS&P NoiseJpeg CompressionRinging ArtifactsInpaintingHaze Input  Restormer  Painter  GT Rain ViT-large PromptGIP Figure 11.Visual comparison on all-in-one image restoration task. 17Unifying Image Processing as Visual Prompting Question Answering Low-lightLLFCanny Operator Input  Restormer  Painter  GT Laplacian  Operator ViT-large PromptGIP Figure 12.Visual comparison on image enhancement and image edge detection tasks. 18",
      "references": [
        "Ntire 2017 challenge on single image super-resolution: Dataset and study",
        "Fast local laplacian filters: Theory and applications",
        "Visual prompting via image inpainting",
        "Low-complexity single-image super-resolution based on nonnegative neighbor embedding",
        "Language models are few-shot learners",
        "Learning photographic global tonal adjustment with a database of input/output image pairs",
        "Understanding and improving visual prompting: A label-mapping perspective",
        "Pre-trained image processing transformer",
        "Imagenet: A large-scale hierarchical image database",
        "Image super-resolution using deep convolutional networks",
        "Accelerating the super-resolution convolutional neural network",
        "An image is worth 16x16 words: Transformers for image recognition at scale",
        "Masked autoencoders are scalable vision learners",
        "Single image super-resolution from transformed self-exemplars",
        "Segment anything",
        "Deblurgan: Blind motion deblurring using conditional adversarial networks",
        "Benchmarking single-image dehazing and beyond",
        "All-in-one image restoration for unknown corruption",
        "Swinir: Image restoration using swin transformer",
        "What makes good in-context examples for gpt-3?",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Very lightweight photo retouching network with conditional sequential modulation",
        "Decoupled weight decay regularization",
        "Prores: Exploring degradation-aware visual prompt for universal image restoration",
        "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
        "Sketch-based manga retrieval using manga109 dataset",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Promptir: Prompting for all-in-one blind image restoration",
        "Language models are unsupervised multitask learners",
        "Bayesian-based iterative method of image restoration",
        "Exploring effective factors for improving visual in-context learning",
        "Unsupervised degradation representation learning for blind super-resolution",
        "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
        "Real-esrgan: Training real-world blind super-resolution with pure synthetic data",
        "Images speak in images: A generalist painter for in-context visual learning",
        "Internvideo: General video foundation models via generative and discriminative learning",
        "Uformer: A general u-shaped transformer for image restoration",
        "Deep retinex decomposition for low-light enhancement",
        "Larger language models do in-context learning differently",
        "Image smoothing via l 0 gradient minimization",
        "Track anything: Segment anything meets videos",
        "Inpaint anything: Segment anything meets im- age inpainting",
        "Multi-stage progressive image restoration",
        "Restormer: Efficient transformer for high-resolution image restoration",
        "On single image scale-up using sparse-representations",
        "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising",
        "Designing a practical degradation model for deep blind image super-resolution",
        "What makes good examples for visual in-context learning?"
      ],
      "meta_data": {
        "arxiv_id": "2310.10513v2",
        "authors": [
          "Yihao Liu",
          "Xiangyu Chen",
          "Xianzheng Ma",
          "Xintao Wang",
          "Jiantao Zhou",
          "Yu Qiao",
          "Chao Dong"
        ],
        "published_date": "2023-10-16T15:32:57Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces PromptGIP, a unified framework that reinterprets diverse image processing tasks as a visual prompting question-answering problem. This approach allows a single model to handle tasks like image restoration, enhancement, and feature extraction without task-specific finetuning. PromptGIP unifies up to 15 different image processing tasks, demonstrating a versatile and adaptive solution for general image processing.",
        "methodology": "PromptGIP adopts a visual prompting question answering (QA) paradigm, inspired by NLP. It treats input-output image pairs as structured question-answer sentences. During inference, the model receives input-output image pairs as task prompts (PQ, PA) to guide its operations on a new target input image (XQ) presented as a query, generating a predicted answer image (YA). A masked visual prompting paradigm is employed during training, where answer images in a 'Q-A-Q-A' sequence are randomly masked, and the model is trained to reconstruct these masked patches. This contrasts with prior works by using a more rational and effective 'Q-A-Q-A' sequence and focusing on pixel-wise prediction.",
        "experimental_setup": "PromptGIP was evaluated on 15 diverse image processing tasks including 10 types of image restoration (Gaussian noise, Gaussian blur, Poisson noise, salt & pepper noise, JPEG compression, ringing artifacts, R-L algorithm, inpainting, haze, rain), 2 image enhancement tasks (low-light image enhancement, local Laplacian filtering), and image edge detection (Canny, Laplacian operators). Datasets used include ImageNet, Common528 (composed of Set5, Set14, BSDS100, Manga109, Urban100, General100, DIV2K-Valid), RESIDE (for dehazing), Rain13K (for complex rain removal), LOL dataset (for low-light enhancement), and Adobe-MIT Fivek (for local Laplacian filtering). A vanilla ViT-large model served as the backbone, processing sequences of four 256x256 images. L1 loss was used for optimization with AdamW and a cosine learning rate scheduler. Evaluation metrics included PSNR, SSIM, and MAE. Comparisons were made against ViT-large, Restormer, Painter, and Real-ESRGAN.",
        "limitations": "The model's current capabilities do not extend to generating unexpected or 'emergent' outcomes beyond learned training data, consistent with prior studies in large language models. The reliance on the quality, diversity, and quantity of training data for in-context learning effectiveness. Computational resource constraints and limited training data prevented extensive large-scale experiments. The use of a vanilla ViT backbone, which employs a rough patch-splitting strategy, can lead to loss of high-frequency information, resulting in artifacts and over-smoothed results, thus yielding sub-optimal quantitative results compared to state-of-the-art specialized backbones for low-level vision tasks.",
        "future_research_directions": "Future work could explore more powerful backbone models beyond the vanilla ViT to enhance performance in low-level vision tasks. Further research is needed to fully explore the model's out-of-domain task generalization capabilities. Expanding the training data and leveraging increased computational resources could lead to more impressive effects of in-context learning. Investigating methods to enable the model to generate novel solutions and emergent outcomes beyond what it has learned from training data is another direction. Adapting training data for PromptGIP could facilitate new applications in medical imaging, environmental monitoring, and content creation. Further interdisciplinary work integrating concepts from NLP and computer vision to solve complex problems is also suggested.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Prompt-augmented Temporal Point Process for Streaming Event Sequence",
      "full_text": "Prompt-augmented Temporal Point Process for Streaming Event Sequence Siqiao Xue∗, Yan Wang∗, Zhixuan Chu♠, Xiaoming Shi, Caigao Jiang, Hongyan Hao Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, Jun Zhou Ant Group Hangzhou, China {siqiao.xsq,luli.wy,chuzhixuan.czx}@alibaba-inc.com Abstract Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a stream- ing manner, where the distribution of patterns may shift over time. Additionally, privacy and memory constraints are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP1, by integrating the base TPP with a continuous-time retrieval prompt pool. The prompts, small learnable parameters, are stored in a memory space and jointly optimized with the base TPP, ensuring that the model learns event streams sequentially without buffering past examples or task-specific attributes. We present a novel and realistic experi- mental setup for modeling event streams, where PromptTPP consistently achieves state-of-the-art performance across three real user behavior datasets. 1 Introduction Event sequences are ubiquitous in a wide range of applications, such as healthcare, finance, social media, and so on. Neural TPPs (Mei & Eisner, 2017; Shchur et al., 2020; Zuo et al., 2020; Zhang et al., 2020; Yang et al., 2022) have emerged as the dominant paradigm for modeling such data, thanks to their ability to leverage the rich representation power of neural networks. However, most existing works assume a static setting, where the TPP model is trained on the entire data, and parameters remain fixed after training. In contrast, real-world event data usually arrives in a streaming manner, rendering it impractical to store all data and retrain the model from scratch at each time step due to computational and storage costs. Shown in Figure 1, a common approach is to use sliding windows to frame the data for model training and prediction. Traditional schemes include pretraining a TPP, which is used for all the following test periods, retraining TPP on the data of each slide of windows and online TPPs. However, they either may fail to adapt to new data or suffer from catastrophic forgetting (see Appendix A for an empirical analysis). In our work, we approach the problem by adopting Continual Learning (CL) (Hadsell et al., 2020; Hao et al., 2023; Chu & Li, 2023; Chu et al., 2023b), a relevant area studying how systems learn sequentially from a continuous stream of correlated data. Yet, classical CL models are not fully *These authors contributed equally to this work. ♠Corresponding author. 1Our code is available at https://github.com/yanyanSann/PromptTPP. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.04993v2  [cs.LG]  13 Oct 2023applicable to our problem. A major line of CL methods (Cha et al., 2021; Buzzega et al., 2020) rely on a rehearsal buffer to retrain a portion of past examples. However, they become ineffective when a rehearsal buffer is not allowed – for example, in real-world scenarios where data privacy matters (Shokri & Shmatikov, 2015) or there are resource constraints. Another branch of works (Ke et al., 2020) bypass the forgetting issue by assuming known task identity at test time, but knowing task identity at test time restricts practical usage. Furthermore, the problem of sequential tasks of event sequence in continuous time have barely been studied. Train Model #1 …Streaming Event Sequence Time Pretrained Train Model #1 Test #2 Retrained Train Model #1 Train Model #2 Test #2 Ours Test #1 Train Model #N Test #N … Train #1 Test #1 Train #2 trainable Cont-timeRetroPrompt instruct query query instruct … Test #2 Test #1 Test #1 Online Online Model Test #2 Test #1 … Online Model … update model Figure 1: Overview of the classical schemes and PromptTPP framework for streaming event sequences. To develop a CL algorithm for such data in real- world scenarios with applicability and general- ity, we draw inspiration from recent advances in prompt-augmented learning (Liu et al., 2022a; Varshney et al., 2022; Cho et al., 2022; Li et al., 2023; Chu et al., 2023a; Wang et al., 2023). Prompt-augmented learning is a form of ma- chine learning that involves adding additional information or prompts to the training data in order to further improve the performance of the model. This can include adding labels or anno- tations to the data, providing additional context to help the model better understand the data, or incorporating feedback from human experts to guide the learning process. By incorporating these prompts, the model is able to learn more effectively and make more accurate predictions. Prompt-augmented learning has been used suc- cessfully in a variety of applications, including natural language processing, computer vision, and speech recognition. Intuitively, prompt-augmented learning reformulates learning downstream tasks from directly adapting model weights to designing prompts that “instruct” the model to perform tasks conditionally while maintaining model plasticity. Thus, it is promising to leverage prompts to sequentially learn knowledge and further store learned knowledge of event sequence in the CL context. While prompt learning (Wang et al., 2022b,a) already demonstrates its effectiveness on multiple CL benchmarks in language modeling, we wish to extend their success to the models of neural TPPs. To this end, we propose PromptTPP, a novel CL framework whose basis is a continuous-time retrieval prompt pool for modeling streaming event sequences. Specifically, we develop a module of temporal prompt that learns knowledge and further store the learned knowledge for event sequences in continuous time. To improve the applicability, building upon prior works (Wang et al., 2022b), we structure the prompts in a key-value shared memory space called the retrieval prompt pool, and design a retrieval mechanism to dynamically lookup a subset of task-relevant prompts based on the instance-wise input of event sequences. The retrieval prompt pool, which is optimized jointly with the generative loss, ensures that shared (unselected) prompts encode shared knowledge for knowledge transfer, and unshared (selected) prompts encode task-specific knowledge that helps maintain model plasticity. PromptTPP has two distinctive characteristics: (i) applicability: despite the effectiveness in augmenting TPP with CL, the prompt pool and the event retrieval mechanism removes the necessity of a rehearsal buffer and knowing the task identity, making the method applicable to modeling the event streams in a more realistic CL setting, i.e., memory efficient and task agnostic. (ii) generality: our approach is general-purpose in the sense that it can be integrated with any neural TPPs. In summary, our main contributions are: • We introduce PromptTPP, a novel prompt-augmented CL framework for neural TPPs. It represents a new approach to address the challenges of modeling streaming event sequences by learning a pool of continuous-time retrieval prompts. These prompts serve as parameterized instructions for base TPP models to learn tasks sequentially, thus enhancing the performance of the model. • We formalize an experimental setup for evaluating the streaming event sequence in the context of CL and demonstrate the effectiveness of our proposed method across three real user datasets. • By connecting the fields of TPP, CL, and prompt learning, our method provides a different perspective for solving frontier challenges in neural TPPs. 22 Preliminaries Generative Modeling of Event Sequences. Suppose we observe I events at a fixed time interval [0, T]. Each event is denoted mnemonically as e@t (i.e., “type e at time t”) and the sequence is denoted as s[0,T] = [e1@t1, . . . , eI@tI] where 0 < t1 < . . . < tI ≤ T and ei ∈ {1, . . . , E} is a discrete event type. Note that representations in terms of time ti and the corresponding inter-event time τi = ti − ti−1 are isomorphic, we use them interchangeably. Generative models of event sequences are TPPs. Specifically, TPPs define functionsλe that determine a finite intensity λe(t | s[0,t)) ≥ 0 for each event type e at each time t >0 such that pe(t | s[0,t)) = λe(t | s[0,t))dt. Then the log-likelihood of a TPP given the entire event sequence s[0,T] is Lll = IX i=1 log λei(ti | s[0,ti)) − Z T t=0 EX e=1 λe(t | s[0,t))dt, (1) Instead of posing strong parametric assumptions on the intensity function, neural TPPs (Du et al., 2016; Mei & Eisner, 2017; Zhang et al., 2020; Zuo et al., 2020; Yang et al., 2022) use expressive representations for the intensity function via neural networks and maximize the associated log- likelihood equation 1 via stochastic gradient methods. CL Problem Formulation for Streaming Event Sequences. The typical CL problem is defined as training models on a continuum of data from a sequence of tasks. Given a sequence s[0,T], we split it based on a sliding window approach shown in Figure 1 and form a sequence of tasks over the time {D0, ...,DN }, where the T -th task DT = (sT train, sT test) contains a tuple of train and test set of event sequences and the two sets have no overlap in time. Data from the previous tasks are not available when training for future tasks. We use the widely-adopted assumption that the task boundaries are clear and the task switch is sudden at training time (Pham et al., 2021). Our goal is to continually learn the sequences while avoiding catastrophic forgetting from the previous tasks. Prompt Learning. Prompt learning methods propose to simply condition frozen language models (LMs) to perform down-stream tasks by learning prompt parameters that are prepended to the input tokens to instruct the model prediction. Compared with ordinary fine-tuning, literature shows In our context, a naive application of prompt learning is to prepend learnable parameters Ps ∈ RLp×D, called a prompt, to the event embedding h = [Ps||x], where x ∈ RD denotes the output of a TPP’s embedding layer of an event, and then feed it to the model function g(h), i.e., a decoder, to perform downstream tasks. Instead of the native application, in our proposed method, we design a novel prompt learning mechanism to properly model the event streams (see section 3.3). 3 Prompt-augmented TPP We introduce a simple and general prompt-augmented CL framework for neural TPPs, named PromptTPP. As shown in Figure 2, PromptTPP consists of three components: a base TPP model, a pool of continuous-time retrieval prompts and a prompt-event interaction layer. In this section, we omit the task index T in our notation as our method is general enough to the task-agnostic setting. 3.1 Base TPP A neural TPP model autoregressively generates events one after another via neural networks. For the i-th event ei@ti, it computes the embedding of the event xi ∈ RD via an embedding layer, which takes the concatenation 2 of the type and temporal embedding xi = [xTYPE i ||xTEMP i ] where || denotes concatenation operation and xTYPE i ∈ RD1 , xTEMP i ∈ RD2 , D= D1 + D2. Then one can draw the next event conditioned on the hidden states that encode history information sequentially: ti+1, ei+1 ∼ Pθ(ti+1, ei+1|hi), hi = fr(hi−1, xi), (2) where fr could be either RNN (Du et al., 2016; Mei & Eisner, 2017) or more expressive attention- based recursion layer (Zhang et al., 2020; Zuo et al., 2020; Yang et al., 2022). For the simplicity of notation, we denote the embedding layer and recursion layer together as the encoder fϕenc parame- terized by ϕenc. Our proposed PromptTPP is general-purpose in the sense that it is straightforward to incorporate any version of neural TPP into the framework. 3.2 Continuous-time Retrieval Prompt Pool The motivations for introducing Continuous-time Retrieval Prompt Pool (CtRetroPromptPool) are two-fold. First, existing prompt-learning works focus on classification tasks in NLP or CV domains, 2The sum operation is also used in some literature. In this paper, we apply concatenation for event embedding. 3Prompt- TPP    Input Sequence Base TPP Encoder Prompt-Event Interaction Intensity Layer Score Function Continuous-time Retrieval Prompt Pool Key-value Pair Prompt Key Prompt Token 2 2 2  1 Query  Retrieval  1 Temporal Prompt 2 Prompt-Event Interaction Asynchronous Refresh Top-N  Temporal Prompt      Multi-head Self-attention            Event    Repr.  T emporal  Encoding  Temporal Prompt       Input Sequence             Figure 2: Overview of PromptTPP. Up: At training time, PromptTPP selects a subset of temporal prompts from a key-value paired CtRetroPromptPool based on our proposed retrieval mechanism; then it prepends the selected prompts to the event representations; finally it feeds the extended event representations into the prompt-event interaction and intensity layer, and optimizes the CtRetroPromptPool through the loss defined in equation 11. Down Left: Illustration of how to parameterize a temporal prompt. Down Right: Illustration of prompt tuning in the prompt-event interaction layer. whose methods are not directly applicable for sequential tasks of learning event streams in continuous time (see section 4.2). Second, the practical setup for modeling event streams closes to the task- agnostic CL setting, where we do not know task identity at test time so that training task-dependent prompt is not feasible. Even if we use extra sources to memorize the task identity, naive usage of prompts (Liu et al., 2022b, 2021; Tam et al., 2022) are still found to result in catastrophic forgetting. For the first motivation, we construct temporal prompt that properly encodes the knowledge of temporal dynamics of event sequence. To address the second, we build a store of prompts in a key- value shared space to transfer knowledge sequentially from one task to another without distinguishing between the common features among tasks versus the features that are unique to each task. Temporal Prompt. In contrast to the standard prompt, the temporal prompt is a time-varying learnable matrix that encodes not only the structural but also the temporal knowledge of the event sequence. We define the temporal promptP = [Ps; Pt] ∈ RLp×D, where Lp is the prompt length and Ps ∈ RLp×D1 , Pt ∈ RLp×D2 denotes the structural component and temporal component. While Ps is a learnable submatrix, the temporal componentPt is set to be continuous-time positional encodings of the estimated conditional time so as to consider the timing. More concretely, given i-th event, we estimate the arithmetic mean of inter-event times up to ti−1, denoted by Ei = E[{τj}j<i] and add this estimated inter-event time to ti−1 to get the estimated conditional time tp := bti = ti−1 + Ei. Inline with Yang et al. (2022), we compute the temporal embedding TE(tp) ∈ RD2 by TE(t) = cos \u0012 t nte · (5Nte nte ) d−1 D2 \u0013 if d is odd , TE(t) = sin \u0012 t nte · (5Nte nte ) d D2 \u0013 if d is even (3) where {Nte, nte ∈ N} are hyperparameters selected according to the time scales in different periods. As TE(Ei) is a vector, we concatenate it repeatedly to form Pt, i.e, Pt = [TE(tp)||, ...,||TE(tp)] ∈ RLp×D2 . Note that the structural component Ps is learnable while the temporal component Pt is computed deterministically. An important consideration of employing such a mechanism is that the mean characterizes the most important property (the long-run average) of the inter-event time distribution, and the computation is straightforward. By taking the temporal embedding of the estimated average conditional time, the prompt efficiently encodes the time-varying knowledge up to the current event, which facilitates learning prediction tasks. We verify the effectiveness of temporal prompt in section 4.2. From Prompt to Prompt Pool. Ideally, one would learn a model that is able to share knowledge when tasks are similar while maintaining knowledge independently otherwise. Thus, instead of 4applying a single prompt, we introduce a pool of temporal prompts to store encoded knowledge, which can be flexibly grouped as an input to the model. The pool is defined as P = [P1, ...,PM ], (4) where M denotes the total number of prompts and Pi ∈ RLp×D is a single temporal prompt. Following the notation in section 3.1, recall hi ∈ RD denotes the hidden representation of the i-th event in the sequence 3 which encodes the event history up to ti via the recursion by equation 2 and let {Prj , j= 1, ..., N} be a subset of N selected prompts, we then incorporate them into the event sequences as in-context augmentation as follows: [Pr1 ||, ...,||PrN ||hi], (5) Prompts are free to compose, so they can jointly encode knowledge for the model to process, which provides flexibility and generality in the sense that a more fine-grained knowledge sharing scheme can be achieved via prompt retrieval mechanism. Under this mechanism, a combination of prompts is selected for each task - similar inputs tend to share more common prompts, and vice versa. Retrieval Prompt Pool. The retrieval prompt pool shares some design principles with methods in other fields, such as RETRO (Borgeaud et al., 2022). Specifically, the prompt pool is augmented to be a key-value store (K, V), defined as the set of learnable keys k ∈ RD and values - temporal prompts P in equation 4: (K, V) = {(ki, Pi)}M i=1 (6) The retrieval prompt pool may be flexible to edit and can be asynchronously updated during the training procedure. The input sequence itself can decide which prompts to choose through query-key matching. Let φ : RD × RD be the cosine distance function to score the match between the query and prompt key. Given a query hi, the encoded event vector, we search for the closest keys overK via maximum inner product search (MIPS). The subset of top-N selected keys is denoted as: Ktop−N = argmin {rj}N j=1 NX i=1 φ(hi, krj ) (7) Importantly, the design of this strategy brings two benefits: (i) it decouples the query learning and prompt learning processes, which has been empirically shown to be critical (see section 4.2); (ii) the retrieval is performed in an instance-wise fashion, which makes the framework become task agnostic, meaning the method works without needing to store extra information about the task identity at test time. This corresponds to a realistic setting for modeling event streams in real applications. 3.3 Prompt-Event Interaction The interaction operation controls the way we combine prompts with the encoded event states, which directly affects how the high-level instructions in prompts interact with low-level representations. Thus, we believe a well-designed prompting function is also vital for the overall CL performance. The interaction mechanism is also called prompting function in the NLP community. We apply the multi-head self-attention mechanism (Vaswani et al., 2017) (MHSA) for modeling the interactions and adopt the mainstream realization of prompting function - Prefix Tuning (Pre-T) (Li & Liang, 2021). Denote the input query, key, and values as zQ, zK, zV and the MHSA layer is constructed as: MHSA (zQ, zK, zV ) = [z1||, ...,||zm]WO, (8) where zi = ATTN (zQWQ i , zKWK i , zV WV i ), WO, WQ i , WK i , WV i are projection matrix. In our context, let {Pri}N i=1 be the retrieved prompts from the pool, we set hi to be the query, split each prompt Pri into PK ri , PV ri ∈ RLp/2×D and prepend them to keys and values, respectively, while keeping the query as-is: hPre −T i = MHSA (hi, [PK||hi], [PV ||hi]), (9) where PK = [PK r1 ||, ...,||PK rN ],PV = [PV r1 ||, ...,||PV rN ]. Apparently, the key and value zK, zV ∈ R( Lp∗N 2 +1)×D and the output hPre −T i ∈ RD. Noted that there exist other prompting methods, such as Prompt Tuning (Pro-T), where all the prompts concurrently prepend to the query, key and values: hPro −T i = MHSA ([PQ||hi], [PK||hi], [PV ||hi]), (10) 3As D1 + D2 = D, we use D and (D1 + D2) interchangeable throughout the paper. 5where PQ = PK = PV = [ Pr1 ||, ...,||PrN ]. As a result, the query, key, value and output zQ, zK, zV , hPro −T i ∈ R(Lp∗N+1)×D. Despite being less efficient in computation, we empirically demonstrate that Pre-T brings better performance. See Analysis III in section 4.2. The output of the MHSA is then passed into an intensity layer (an MLP with softplus activation) to generate the intensity λe(ti), e∈ {1, ..., E}. For simplicity, we denote the prompt-event interaction and intensity layer together as the the decoder fϕdec parameterized by ϕdec. 3.4 Model Optimization The full picture of PromptTPP at training and test time is described in Algorithm 1 and Algorithm 2 in Appendix C.1. At every training step, each event ei@ti is recursively fed into the encoder fϕenc, after selecting N prompts following the aforementioned retrieval strategy, the intensity λ(ti) is computed by the decoder fϕdec. Overall, we seek to minimize the end-to-end loss function: min P,ϕenc,ϕdec,K Lnll(P, fϕenc, fϕdec) + α X i X Ktop−N φ(fϕenc(ei@ti), krj ), (11) where the first term is the negative loglikelihood of the event sequence (Lnll equals to −Lll defined in equation 1) and the second term refers to a surrogate loss to pull selected keys closer to corresponding query in the retrieval process. α is a scalar to control the importance of the surrogate loss. Given the learned parameters, we may wish to make a minimum Bayes risk prediction about the next event via the thinning algorithm (Mei & Eisner, 2017; Yang et al., 2022). Asynchronous Refresh of Prompt Pool. The prompts may lead to the variable contextual representa- tion of the event as the parameters of the based model are continually updated. To accelerate training, we propose to asynchronously update all embeddings in the prompt pool every C training epochs. 4 Experiments 4.1 Experimental setup Datasets and Evaluation Setup We conduct our real-world experiments on three sequential user- behavior datasets. In each dataset, a sequence is defined as the records pertaining to a single individual. The Taobao (Alibaba, 2018) dataset contains time-stamped user click behaviors on Taobao shopping pages with the category of the item involved noted as the event type. The Amazon (Ni, 2018) dataset contains time-stamped records of user-generated reviews of clothing, shoes, and jewelry with the category of the reviewed product defined as the event type. The StackOverflow (Leskovec & Krevl, 2014) dataset contains two years of user awards on a question-answering website: each user received a sequence of badges with the category of the badges defined as the event type. See Appendix D.1 for dataset details. We partition Taobao and Amazon datasets into10 consecutively rolling slides (namely 10 tasks) and partition the StackOverflow dataset into 6 rolling slides (namely 6 tasks). For the Taobao dataset, each slide covers approximately 1 day of time; for the Amazon dataset, each slide covers 2 years of time; for the StackOverflow dataset, each slide covers approximately 5 months time. The subset in each task is split into training, validation, and test sets with a 70%, 10%, 20% ratio by chronological order. Each task has no overlap in the test set. For a detailed discussion, a demonstration of the evaluation process is provided in Figure 9 in Appendix D.3. Metrics. Following the common next-event prediction task in TPPs (Du et al., 2016; Mei & Eisner, 2017), each model attempts to predict every held-out event (ti, ki) from its history Hi. We evaluate the prediction ˆki with the error rate and evaluate the prediction ˆti with the RMSE. Base models. While our proposed methods are amenable to neural TPPs of arbitrary structure, we choose two strong neural TPPs as our base models: NHP (Mei & Eisner, 2017) and AttNHP (Yang et al., 2022), an attention-based TPP whose performance is comparable to or better than that of the NHP as well as other attention-based models (Zuo et al., 2020; Zhang et al., 2020). Competitors. With NHP and AttNHP as base models, we trained PromptNHP (Pt-NHP) and PromptAttNHP (Pt-ANHP) in the proposed prompt-augmented setup and compared with 7 baselines. • PretrainedTPP. PretrainedNHP (Pre-NHP) and PretrainedAttNHP (Pre-ANHP) represent NHP and AttNHP learned at the first task (time step) and not trained any longer. 60 2 4 6 8 T ask ID over time 61 65 69 73 77 81T ype Error Rate % 0 2 4 6 8 T ask ID over time 0.60 0.65 0.70 0.75 0.80 0.85Time RMSE Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 71 73 75 77 79 81Average Error Rate % Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 0.60 0.65 0.70 0.75 0.80Average Time RMSE Pre-NHP Pre-ANHP Re-NHP Re-ANHP O-TPP CL-NHP CL-ANHP Pt-NHP Pt-ANHP (a) Taobao dataset 0 2 4 6 8 T ask ID over time 61 65 69 73 77 81T ype Error Rate % 0 2 4 6 8 T ask ID over time 0.50 0.55 0.60 0.65 0.70 0.75Time RMSE Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 65 67 69 71 73Average Error Rate % Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 0.50 0.53 0.56 0.59 0.62Average Time RMSE Pre-NHP Pre-ANHP Re-NHP Re-ANHP O-TPP CL-NHP CL-ANHP Pt-NHP Pt-ANHP (b) Amazon dataset 0 1 2 3 4 5 T ask ID over time 49 51 53 55T ype Error Rate % 0 1 2 3 4 5 T ask ID over time 1.12 1.15 1.18 1.21Time RMSE Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 48.0 50.0 52.0 54.0Average Error Rate % Pre-NHPPre-ANHPRe-NHPRe-ANHP O-TPPCL-NHPCL-ANHPPt-NHPPt-ANHP 1.12 1.14 1.16Average Time RMSE Pre-NHP Pre-ANHP Re-NHP Re-ANHP O-TPP CL-NHP CL-ANHP Pt-NHP Pt-ANHP (c) StackOverflow dataset Figure 3: Performance of all the methods on Taobao (up), Amazon (middle) and StackOverflow (down). In each figure, the subfigures from left to right are the evolution of type error rate and the time RMSE of each task, the average error rate, and the average time RMSE of all the tasks. • RetrainedTPP. RetrainedNHP (Re-NHP) and RetrainedAttNHP (Re-ANHP) refer to TPPs re- trained at every sliding widow. • OnlineTPP. As there is no prior work on online neural TPPs, we use online Hawkes process OnlineMHP (O-TPP) (Yang et al., 2017), trained in an online manner without any consideration for knowledge consolidation. • CLTPP. The concurrent work (Dubey et al., 2022), to the best of our knowledge, is the only neural TPP with CL abilities proposed so far. Based on their work 4, we implement CL-NHP (CL-NHP) and CLAttNHP (CL-ANHP) as two variants of the hypernetwork-based CLTPPs. Implementation and Training Details. For a fair comparison, they (except O-TPP which is a classical TPP model) are of similar model size (see Table 2 in Appendix D.4). For Pt-NHP and Pt-ANHN, we set M = 10, N= 4, Lp = 10 for both datasets. During training, we set C = 2 by default and explore the effect of asynchronous training in Analysis IV of section 4.2. More details of the implementation and training of all the methods are in Appendix D.5. 4.2 Results and Analysis The main results are shown in Figure 3. Pre-NHP and Pre-ANHP work the worst in most cases because of inadequate ability to handle the distribution shift in the event sequence. Besides. O-TPP has a similarly poor performance because of two reasons: first it is a classical (non-neural) TPP with weaker representation power of modeling event sequence compared to its neural counterparts; second as a traditional online learning method, it easily loses memory of previously encountered data and suffers from catastrophic forgetting. Retraining at every task (Re-NHP and Re-ANHP) achieves 4They have not published the code yet. 7Pt-ANHP-std Pt-ANHP 71.0 72.0 73.0Average Error Rate % Pt-ANHP-std Pt-ANHP 0.60 0.62 0.64 0.66 0.68Average Time RMSE T aobao Pt-ANHP-std Pt-ANHP 67.0 68.0 69.0Average Error Rate % Pt-ANHP-std Pt-ANHP 0.55 0.56 0.57 0.58 0.59Average Time RMSE Amazon (a) Effect of the temporal prompt: performances of Pt-ANHP-std and Pt-ANHP on Taobao (the two on the left) and Amazon (the two on the right) datasets. N-PPro-TPre-T 67 69 71 73Average Error Rate % N-PPro-TPre-T 0.60 0.62 0.64 0.64 0.68Average Time RMSE T aobao N-PPro-TPre-T 66 68 70Average Error Rate % N-PPro-TPre-T 0.52 0.54 0.56 0.58 0.60Average Time RMSE Amazon (b) Effect of prompting functions: performances of N-P, Pro-T, and Pre-T on Taobao (the two on the left) and Amazon (the two on the right) datasets. Figure 5: Effect of temporal prompt and prompting function of PromptTPP. 1 2 4 8 Selection size 2 6 10 16 Prompt length 69.88 68.89 68.56 68.52 69.21 68.62 68.49 68.50 69.03 68.47 68.44 68.49 68.48 68.46 68.47 68.54 1 2 4 8 Selection size 2 6 10 16 Prompt length 0.5982 0.5743 0.5725 0.5739 0.5792 0.5732 0.5721 0.5726 0.5787 0.5724 0.5727 0.5771 0.5772 0.5730 0.5742 0.5861 (a) Average performance of Pt-ANHP w.r.t. prompt length Lp and prompt selection size N, given prompt pool size M = 15. Left: Average Type error rate (%); Right: Average time RMSE. 5 10 15 20 25 Prompt Pool Size 68.3 68.6 68.9 69.2Average Error Rate % 0.572 0.573 0.574 0.575 Average RMSE (b) Average performance of Pt-ANHP w.r.t. prompt pool size M, given Lp = 10, N= 4. Figure 6: Effect of hyperparameters of PromptTPP on Amazon dataset. moderate results but it also causes catastrophic forgetting. Not surprisingly, CL-NHP and CL-ANHP perform better than retraining, by applying a regularized hypernetwork to avoid forgetting. However, the hypernetwork relies on task descriptors built upon rich meta data, which limits its applicability and performance in our setup (and in real applications as well!). Lastly, our methods (both Pt-NHP and Pt-ANHP) work significantly better than all these baselines across the three datasets: they substantially beat the non-CL methods; they also consistently outperform CL-NHP and CL-ANHP by a relative 4% − 6% margin on both metrics, thanks to our novel design of the CtRetroPromptPool, which successfully reduces catastrophic forgetting (see Analysis 0). 0 2 4 6 8 T ask ID over time -0.5 0.5 1.5 2.5 Abs Change on Error Rate % Re-ANHP CL-ANHP Pt-ANHP 0 2 4 6 8 T ask ID over time 0.00 0.01 0.02Abs Change on Time RMSE Re-ANHP CL-ANHP Pt-ANHP Figure 4: The performance drop when re-evaluating the0-8-th tasks using model trained on the 9-th task on Amazon dataset. Analysis 0: How models perform on previous tasks after learning new events? We aim to validate that the improvement in performances indeed is due to the alleviation in catastrophic forgetting instead of simply a better fit on the current task. We use ANHP trained on task 9 and Pt-ANHP contin- uously trained on task 9, re-evaluate them on previous tasks and see how the metrics changed. Specifically, on Figure 4, (i) Each number on the curves of Re- ANHP and CL-ANHP corresponds to the performance difference on the test set of task i, i <9 using ANHP trained on task 9 vs ANHP trained on task i. (ii) Each number on the curves of Pt-ANHP corresponds to the performance difference on the test set of task i, i <9 using Pt-ANHP trained until (including) task 9 vs Pt-ANHP trained until task i. See from Figure 4, on both metrics, we see the drop in performance (i.e., error rate / RMSE increases) of Pt-ANHP is much less significant than ANHP, indicating Pt-ANHP stores well the knowledge of previous tasks, which largely alleviates catastrophic forgetting. 8(a) Learning curves of Re-ANHP and Pt-ANHP with varying asynchronous refresh parameter C on a randomly selected task. w/o CtRetroPP w/o k-vPt-ANHP 67.0 67.5 68.0 68.5 69.0 69.5 70.0Average Error Rate % w/o CtRetroPP w/o k-vPt-ANHP 0.560 0.565 0.570 0.575 0.580 0.585 0.590Average Time RMSE (b) Effect of prompt related components of PromptTPP on Amazon dataset. Figure 7: Effect of asynchronous refresh and prompt related components of PromptTPP on dataset. Analysis I: Does stronger base TPP model naively improve CL? Our method builds upon a backbone TPP and understanding this question is important for fair comparisons and future research. From Figure 3, Re-ANHP makes no consistent improvement against Re-NHP on average CL perfor- mance, which indicates a stronger TPP is not a solution for CL without being appropriately leveraged. Besides, for the CL-based methods, CL-ANHP is tied with CL-NHP on Taobao and makes a limited advancement against CL-NHP on Amazon, while Pt-NHP and Pt-ANHP perform closely on both datasets. Therefore, we can conclude that, although AttNHP is a more robust base model than common non attention-based TPP, i.e., NHP, it is not necessarily translated to CL performance. Analysis II: Temporal prompt vs standard prompt. For a fair comparison, we initialize a pool of standard prompts without time-varying parameters by fixing their temporal components Pt to be an all-ones matrix and incorporate it into the base model AttNHP. This method is named Pt-ANHP-std. With other components fixed, we compare Pt-ANHP-std with Pt-ANHP to validate the effectiveness of the temporal prompt introduced in section 3.2. Figure 5a shows that Pt-ANHP achieves better performance on both datasets: the introduction of temporal prompts slightly improves the RMSE metric and reduces the error rate with a larger margin. We did the paired permutation test to verify the statistical significance of the improvements. See Appendix D.6 for details. Overall, on both datasets, we find that the performance improvements by using the temporal prompts are enormously significant on error rate (p-value < 0.05 ) and weakly significant on RMSE (p-value ≈ 0.08). Analysis III: How to better attach prompts? We explore how to attach prompts and enhance their influences on overall performance. We compare three types of prompting: 1 Naive Prompting (N-P), where the retrieval and prompting are performed after the event embedding layer: we replace hi with xi in equation 7, prepend the selected prompts to xi and pass it to the rest structure of TPP. 2 Prompt Tuning (Pro-T): which concurrently prepend the prompts to query, key, and value, introduced at the end of section 3.3. 3 Prefix-Tuning (Pre-T), proposed in the main body of section 3.3, which is the prompting method used in PromptTPP. In Figure 5b, we observe that Pre-T leads to better performance on both datasets compared to those two variants. Despite its empirically better performance, the architecture of Pre-T is actually more scalable and efficient when attached to multiple layers since it results in unchanged output size: hPre −T i ∈ RD remains the same size as the input while hPro −T i ∈ R(Lp∗N+1)×D increases the size along the prompt length dimension. Analysis IV: Efficiency of our method. We examine the efficiency of our method in two steps: • Firstly, seen from Table 2, on both datasets, Pt-NHP / Pt-ANHP leads to a12% / 8% total parameter increase to the base model, which in fact causes a marginal impact on training speed: Figure 7a shows that learning curves of Re-ANHP and Pt-ANHP(C = 1) converge at almost the same speed to achieve competitive log-likelihood, respectively. • Furthermore, to accelerate the training (especially when introducing large size prompts), we intro- duce the asynchronous refresh mechanism (see section 3.4) with prompts updated in a frequency C >1 (refresh the prompt pool less frequently). We observe in Figure 7a that Taobao training with C = 2 has a comparable performance with C = 1 while Amazon training with C = 2 improves the convergence notably. C = 4 leads to no advancement. 9Overall, PromptTPP only adds a small number of parameters so that it generally has the same convergence rate as the base model. The asynchronous prompt optimization scheme with C = 2 improves the convergence more remarkably on the Amazon dataset. In addition, we indeed provide a complexity analysis. See Appendix D.7. Analysis V: Effect of prompt related components of our method. Firstly we completely remove the CtRetroPromptPool design (w/o CtRroPP in Figure 7b) and use a single temporal prompt to train tasks sequentially. The performance declines with a notable drop, indicating that a single prompt suffers severe catastrophic forgetting between tasks, while our design of CtRetroPromptPool encodes task-invariant and task-specific knowledge well. Secondly, we remove the learnable key associated with prompts (w/o k-v in Figure 7b) and directly use the mean of prompts as keys. This strategy causes a moderate drop in performance. To conclude, learnable keys decouple the query and prompt learning processes and markedly contribute to the performance. Analysis VI: Effect of hyperparameters of our method. We evaluate how the performance of PromptTPP changes as we vary three key hyperparameters: (i) prompt length Lp, (ii) selection size N, and (iii) prompt pool size M. Theoretically, Lp determines the capacity of a single prompt (which jointly encodes certain knowledge), Lp × N is the total size used to prepend the event vector, while M sets the up limit of the capacity of learnable prompts. • Prompt length Lp and selection size N. From the results in Figure 6a, a too small Lp negatively affects results as a single prompt has a too limited ability to encode the knowledge. Besides, given an optimal Lp, an overly large N makes the total prompts excessively oversized, leading to underfitting and negatively impacting the results. We conclude that a reasonably largeLp and N enable the model properly encode the shared knowledge between the tasks of event sequences and substantially improve the predictive performance. • Prompt pool size M. Figure 6b illustrates that M positively contributes to the performance. This is because the larger pool size means the larger capacity of the prompts. 5 Conclusion In summary, this paper has proposed a groundbreaking framework, known as PromptTPP, for modeling streaming event sequences. By incorporating a continuous-time retrieval prompt pool, the framework effectively facilitates the learning of event streams without requiring rehearsal or task identification. Our experiments have shown that PromptTPP performs exceptionally well compared to other competitors, even under challenging and realistic conditions. 6 Limitations and Societal Impacts Limitations. Our method uses neural networks, which are typically data-hungry. Although it worked well in our experiments, it might still suffer compared to non-neural models if starved of data. Societal Impacts. By describing the model and releasing code, we hope to facilitate probabilistic modeling of continuous-time sequential data in many domains. However, our model may be applied to unethical ends. For example, it may be used for unwanted tracking of individual behavior. 10References Alibaba. User behavior data from taobao for recommendation, 2018. Bacry, E., Bompaire, M., Gaïffas, S., and Poulsen, S. tick: a Python library for statistical learning, with a particular emphasis on time-dependent modeling. ArXiv e-prints, 2017. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J.-B., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens, 2022. Boyd, A., Bamler, R., Mandt, S., and Smyth, P. User-dependent neural sequence models for continuous-time event data. 2020. Buzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020. Cha, H., Lee, J., and Shin, J. Co2l: Contrastive continual learning. In ICCV, 2021. Cho, H., Kim, H. J., Kim, J., Lee, S.-W., Lee, S.-g., Yoo, K. M., and Kim, T. Prompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners.arXiv preprint arXiv:2212.10873, 2022. Chu, Z. and Li, S. Continual treatment effect estimation: Challenges and opportunities. AAAI Bridge Program on Continual Causality, pp. 11–17, 2023. Chu, Z., Hao, H., Ouyang, X., Wang, S., Wang, Y ., Shen, Y ., Gu, J., Cui, Q., Li, L., Xue, S., et al. Leveraging large language models for pre-trained recommender systems. arXiv preprint arXiv:2308.10837, 2023a. Chu, Z., Li, R., Rathbun, S., and Li, S. Continual causal inference with incremental observational data. arXiv preprint arXiv:2303.01775, 2023b. Du, N., Dai, H., Trivedi, R., Upadhyay, U., Gomez-Rodriguez, M., and Song, L. Recurrent marked temporal point processes: Embedding event history to vector. 2016. Dubey, M., Srijith, P. K., and Desarkar, M. S. Hyperhawkes: Hypernetwork based neural temporal point process. CoRR, abs/2210.00213, 2022. Hadsell, R., Rao, D., Rusu, A. A., and Pascanu, R. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028—1040, December 2020. ISSN 1364-6613. Hall, E. C. and Willett, R. M. Tracking dynamic point processes on networks. IEEE Transactions on Information Theory, 62(7):4327–4346, 2016. Hao, H., Chu, Z., Zhu, S., Jiang, G., Wang, Y ., Jiang, C., Zhang, J., Jiang, W., Xue, S., and Zhou, J. Continual learning in predictive autoscaling. In CIKM, 2023. Hawkes, A. G. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 1971. Hoi, S. C., Sahoo, D., Lu, J., and Zhao, P. Online learning: A comprehensive survey. Neurocomput., 459(C):249–289, oct 2021. ISSN 0925-2312. Ke, Z., Liu, B., and Huang, X. Continual learning of a mixed sequence of similar and dissimilar tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. ISBN 9781713829546. Kingma, D. and Ba, J. Adam: A method for stochastic optimization. 2015. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. 11Leskovec, J. and Krevl, A. SNAP Datasets: Stanford large network dataset collection, 2014. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Lewis, P. A. W. Some results on tests for poisson processes. Biometrika, 52(1/2), 1965. Li, B., Dou, L., Hou, Y ., Feng, Y ., Mu, H., and Che, W. Mixpro: Simple yet effective data augmentation for prompt-based learning. arXiv preprint arXiv:2304.09402, 2023. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., sep 2022a. ISSN 0360-0300. Just Accepted. Liu, X., Zheng, Y ., Du, Z., Ding, M., Qian, Y ., Yang, Z., and Tang, J. Gpt understands, too.arXiv preprint arXiv:2103.10385, 2021. Liu, X., Ji, K., Fu, Y ., Tam, W., Du, Z., Yang, Z., and Tang, J. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61–68, 2022b. Mallya, A. and Lazebnik, S. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018. McCloskey, M. and Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of Psychology of Learning and Motivation, pp. 109–165. Academic Press, 1989. Mei, H. and Eisner, J. The neural Hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems, 2017. Mei, H., Qin, G., Xu, M., and Eisner, J. Neural Datalog through time: Informed temporal modeling via logical specification. 2020. Ni, J. Amazon review data, 2018. Omi, T., Ueda, N., and Aihara, K. Fully neural network based model for general temporal point processes. In Advances in Neural Information Processing Systems, 2019. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. 2017. Pham, Q., Liu, C., and Hoi, S. Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34, 2021. Qu, C., Tan, X., Xue, S., Shi, X., Zhang, J., and Mei, H. Bellman meets hawkes: Model-based reinforcement learning via temporal point processes. In AAAI 2023. AAAI Press, 2023. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Shchur, O., Biloš, M., and Günnemann, S. Intensity-free learning of temporal point processes. 2020. Shchur, O., Turkmen, A. C., Januschowski, T., Gasthaus, J., , and Günemann, S. Detecting anomalous event sequences with temporal point processes. Advances in Neural Information Processing Systems (NeurIPS), 2021. Shi, X., Xue, S., Wang, K., Zhou, F., Zhang, J. Y ., Zhou, J., Tan, C., and Mei, H. Language models can improve event prediction by few-shot abductive reasoning. In Advances in Neural Information Processing Systems, 2023. 12Shokri, R. and Shmatikov, V . Privacy-preserving deep learning. InProceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, pp. 1310–1321, New York, NY , USA, 2015. Association for Computing Machinery. ISBN 9781450338325. Snoek, J., Ovadia, Y ., Fertig, E., Lakshminarayanan, B., Nowozin, S., Sculley, D., Dillon, J. V ., Ren, J., and Nado, Z. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 13969–13980, 2019. Tam, W. L., Liu, X., Ji, K., Xue, L., Zhang, X., Dong, Y ., Liu, J., Hu, M., and Tang, J. Parameter- efficient prompt tuning makes generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087, 2022. Varshney, V ., Patidar, M., Kumar, R., Vig, L., and Shroff, G. Prompt augmented generative replay via supervised contrastive learning for lifelong intent detection. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 1113–1127, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. 2017. Wang, J., Song, G., Wu, Y ., and Wang, L. Streaming graph neural networks via continual learning. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM ’20, pp. 1515–1524, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450368599. Wang, Y ., Chu, Z., Ouyang, X., Wang, S., Hao, H., Shen, Y ., Gu, J., Xue, S., Zhang, J. Y ., Cui, Q., et al. Enhancing recommender systems with large language model reasoning graphs. arXiv preprint arXiv:2308.10835, 2023. Wang, Z., Zhang, Z., Ebrahimi, S., Sun, R., Zhang, H., Lee, C.-Y ., Ren, X., Su, G., Perot, V ., Dy, J., et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. European Conference on Computer Vision, 2022a. Wang, Z., Zhang, Z., Lee, C.-Y ., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V ., Dy, J., and Pfister, T. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139–149, 2022b. Xiao, S., Yan, J., Yang, X., Zha, H., and Chu, S. Modeling the intensity function of point process via recurrent neural networks. In AAAI, 2017. Xue, S., Shi, X., Hao, H., Ma, L., Zhang, J., Wang, S., and Wang, S. A graph regularized point process model for event propagation sequence. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1–7, 2021. Xue, S., Shi, X., Zhang, Y . J., and Mei, H. Hypro: A hybridly normalized probabilistic model for long-horizon prediction of event sequences. In Advances in Neural Information Processing Systems, 2022. Xue, S., Shi, X., Chu, Z., Wang, Y ., Zhou, F., Hao, H., Jiang, C., Pan, C., Xu, Y ., Zhang, J. Y ., Wen, Q., Zhou, J., and Mei, H. Easytpp: Towards open benchmarking the temporal point processes. 2023. Yang, C., Mei, H., and Eisner, J. Transformer embeddings of irregularly spaced events and their participants. 2022. Yang, Y ., Etesami, J., He, N., and Kiyavash, N. Online learning for multivariate hawkes processes. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learning through synaptic intelligence. In ICML, 2017. Zhang, Q., Lipani, A., Kirnap, O., and Yilmaz, E. Self-attentive Hawkes process. 2020. Zuo, S., Jiang, H., Li, Z., Zhao, T., and Zha, H. Transformer Hawkes process. In International Conference on Machine Learning, pp. 11692–11702. PMLR, 2020. 13Appendices A Discussion on Classical Schemes for Modeling Event Sequence As shown in Figure 1, a common approach is to use sliding windows to frame the data for model training and prediction. In this setup, we discuss three classical schemes. • Pretrained TPP. A straightforward solution is to pretrain a TPP in the first train set and use it for all the following test periods. Such an approach faces the problem of distribution shift, i.e., the new data is systematically different from the data the model was trained on Snoek et al. (2019). Take the Taobao dataset (Alibaba, 2018) for example, which contains time-stamped user click behaviors 5. We split the dataset by timestamps into 10 periods sequentially and compute the 3S statistics (Shchur et al., 2021) as a measure of the distribution of event sequences for each period. Figure 8a shows that the 3S statistics differentiate between periods while Figure 8b illustrates an increasing KL divergence of the 3S statistics between the first and later period, implying a pattern shift over time. As a result, this approach may fail to adapt to new data and produce unsatisfactory predictions. • Retrained TPP. Another classical solution is to train a new TPP on the data of sliding windows over again. The TPP can quickly adapt to new data but may suffer from catastrophic forgetting (McCloskey & Cohen, 1989): adaptation usually implies that the model loses memory of previously encountered data that may be relevant to future predictions. For example, Figure 8a shows a large overlap in distributions of different periods on the Taobao dataset, indicating the necessity of maintaining the knowledge of existing patterns to improve generalization (Snoek et al., 2019; Wang et al., 2020). • Online TPP. A better solution is an online approach: discretize the time axis into small intervals and then incrementally update the TPP at the end of each interval using an online algorithm. However, online models are generally more difficult to maintain and may also cause catastrophic forgetting (Hoi et al., 2021). Besides, to the best of our knowledge, apart from online classical TPPs (Yang et al., 2017; Hall & Willett, 2016), the field of online neural TPPs is much less well-studied. B Related Work Details Here we draw connections and discuss differences between our method to related works. Temporal Point Process. A large variety of Neural TPPs have been proposed over recent decades, aimed at modeling event sequences with varying sorts of properties. Many of them are built on recurrent neural networks (Du et al., 2016; Mei & Eisner, 2017; Xiao et al., 2017; Omi et al., 2019; Shchur et al., 2020; Mei et al., 2020; Boyd et al., 2020). Models of this kind enjoy continuous state spaces and flexible transition functions, thus achieving superior performance on many real-world datasets, compared to classical models such as the Hawkes process (Hawkes, 1971). To properly capture the long-range dependency in the sequence, the attention mechanism (Vaswani et al., 2017) has been adapted to TPPs (Zuo et al., 2020; Zhang et al., 2020; Xue et al., 2021; Qu et al., 2023; ?; Wang et al., 2023; Shi et al., 2023) to enhance the predictive performance. However, learning the event sequence under the stream setting is largely unexplored. To the best of our knowledge, there exist two prior works (Yang et al., 2017; Hall & Willett, 2016) that propose online learning algorithms for classical TPPs while that for neural TPP have rarely been studied. We show our method works better than classical online TPPs in practice (see section 4.2). Continual Learning. There is also a rich existing literature on CL: the models can be categorized into regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), which regularize important parameters for learned tasks, architecture-based methods (Rusu et al., 2016; Mallya & Lazebnik, 2018) which assign isolated parameters for each task and rehearsal-based methods (Cha et al., 2021; Buzzega et al., 2020) which save data from learned tasks in a rehearsal buffer to train with 5Please see Appendix D.1 for the details of the Taobao dataset and Appendix D.2 for the explanations on 3S statistics and the procedure of the experiment on distribution shift. 141 3 5 7 9 3S statistic 0.00 0.02 0.04 0.06 0.08Probability Period 0 Period 5 Period 9 (a) Distributions of 3S statistics for event sequence data in three randomly selected periods. 1st 3rd 5th 7th 9th Between 0-th and the following period 0.15 0.20 0.25 0.30KLD in 3s Statistics (b) KL divergence of distributions of 3S statistics between the 0-th and the following period. Figure 8: Analysis of distribution shift on Taobao dataset. the current task. In retrospect, we realize a concurrent work (Dubey et al., 2022) which also augments TPP with CL abilities. Important distinctions of their work from ours include: 1. setup: they use standard TPP train/valid setting while we formalize more realistic streaming setting to train/validate the models; 2. methodology: they model the event streams with a hypernetwork-based regularizer while we use a trainable prompt pool with more flexibility and generality in CL. 3. task agnostic: they rely on a task descriptor built from meta attributes of events while we do not - our method is task agnostic. As their source code is not available yet, we independently implement it and our method still outperforms it (see section 4.2). Prompt Learning. Prompt-based learning (or prompting), as an emerging transfer learning tech- nique in NLP, applies a fixed function to condition the model so that the language model gets additional instructions to perform the downstream task. Continuous prompts have also been proposed (Lester et al., 2021; Li & Liang, 2021) to reduce prompt engineering, which directly appends a series of learnable embeddings as prompts into the input sequence, achieving outstanding performance on transfer learning. Wang et al. (2022a,b) connect prompting and CL, which attaches prompts to the pretrained backbone to learn task-invariant and task-specific instructions. Note that it is non-trivial to apply prompting to neural TPPs (See Analysis II and III in section 4.2), and our proposed novel framework reveals its values to event sequence modeling. C Method Details C.1 PromptTPP at Training and Test Time The training and test time Algorithms for PromptTPP are illustrated in Algorithm 1 and Algorithm 2, respectively. For simplicity of notations, in test time, we show how to sample the next event given one historical event sequence via the thinning algorithm (Mei & Eisner, 2017), which can be easily extended to batch-wise inference (see the implementation in EasyTPP (Xue et al., 2023)). D Experimental Details D.1 Dataset Details We evaluate our methods on three industrial user behavior datasets. We provide details on the preparation and utilization of each below. For both datasets, users are associated with anonymous aliases to remove personally identifiable information (PII). Taobao (Alibaba, 2018). This dataset contains time-stamped user click behaviors on Taobao shopping pages from November 25 to December 03, 2017. Each user has a sequence of item click events where each event contains the timestamp and the category of the item. Following the previous work (Xue et al., 2022), the categories of all items are first ranked by frequencies, and the top 19 are kept while the rest are merged into one category, with each category corresponding to an event type. We work 15Algorithm 1 PromptTPP at training time of the T -th task. Input: Train set {strain}. CtRetroPromptPool (K, V) = {(ki, Pi)}M i=1 (inherited from the previous task) , score function φ, loss weight α and asynchronous refresh frequency C. Output: Trained base model with a encoder fϕenc and a decoder fϕdec; trained CtRetroPromptPool (K, V) = {(ki, Pi)}M i=1. 1: procedure TRAIN ({strain}, (ki, Pi)}M i=1, φ, α, C) 2: for epoch_id in total_epochs : 3: Draw a mini batch B 4: ▷ For illustration purposes, we use batch size=1 here. The computation here can be easily extended to batch size ≥ 1. 5: for s[0,T] in B : 6: Update the loss ← CALC LOSS (s[0,T], fϕenc, fϕdec, α) 7: Update fϕenc, fϕdec by backpropagation. 8: if epoch_id %C == 0 : Update (ki, Pi)}M i=1 by backpropagation. 9: procedure CALC LOSS (s[0,T], fϕenc, fϕdec, α) 10: L ←0. 11: ▷ Recursively compute the loss. 12: for e@t in s[0,T] : 13: ▷ Compute the likelihood loss; the technical details can be found in Mei & Eisner (2017) and Yang et al. (2022). 14: Levent ← Take a sum of log intensity at the event time by calling CALC INTEN - SITY (s[0,t), e@t, fϕenc, fϕdec,(ki, Pi)M i=1). 15: Lnon_event ← Integrate log CALC INTENSITY (s[0,t), e@t) over inter-event time interval. 16: Lnll ← Lnon_event − Levent 17: ▷ Compute the matching loss. 18: Lmatching ← P Ktop−N φ(fϕenc(e@t), krj ) 19: L ← Lnll + αLmatching 20: return L 21: procedure CALC INTENSITY (s[0,t), e@t, fϕenc, fϕdec, (ki, Pi)M i=1) 22: s[0,t] ← Append e@t to history s[0,t). 23: Encode s[0,t] by fϕenc to generate the hidden state ht. 24: Matching the index rjN j=1 based on equation 7. 25: Select Top-N prompts {Pri}N i=1. 26: Prepend {Pri}N i=1 to ht and pass to the decode fϕdec to generate the intensity λe(t), e∈ {1, ..., E}. 27: return λe(t), e∈ {1, ..., E} on a subset of 4800 most active users with an average sequence length of 150 and then end up with K = 20 event types. Amazon (Ni, 2018). This dataset includes time-stamped user product review behavior from January 2008 to October 2018. Each user has a sequence of produce review events where each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type. We work on a subset of 5200 most active users with an average sequence length of 70 event tokens and then end up with K = 16 event types. StackOverflow (Leskovec & Krevl, 2014). This dataset has two years of user awards on a question- answering website: each user received a sequence of badges and there are K = 22 different kinds of badges in total. We work on a subset of xx most active users with an average sequence length of xx event tokens. For the Taobao dataset, each task includes approximately 1 day of time;for the Amazon dataset, each task includes approximately 2 years of time; for the StackOverflow dataset, each task includes approximately 5 months of time. Table 1 shows statistics about each dataset mentioned above. 16Algorithm 2 PromptTPP at test time of the T -th task. Input: An event sequence s[0,T] = {ei@ti}I i=1. Trained base model with a encoder fϕenc and a decoder fϕdec; trained CtRetroPromptPool (K, V) = {(ki, Pi)}M i=1 and the score function φ. Output: Sampled next event beI+1@btI+1. 1: procedure DRAWNEXT EVENT (s[0,T], fϕenc, fϕdec) 2: t0 ← T; H ←s[0,T] 3: ▷ Compute sampling intensity 4: {λe(tj | H)}N j=1 ← SAMPLE INTENSITY (s[0,T], fϕenc, fϕdec, {(ki, Pi)}M i=1) for all tj ∈ (t0, ∞) 5: ▷ Compute the upper bound λ∗. 6: ▷ Technical details can be found in Mei & Eisner (2017) 7: find upper bound λ∗ ≥ PE e=1 λe(tj | H) for all tj ∈ (t0, ∞) 8: repeat 9: draw ∆ ∼ Exp(λ∗); t0 += ∆ ▷ time of next proposed event btI+1 10: u ∼ Unif(0, 1) 11: until uλ∗ ≤ PE e=1 λe(t0 | H) 12: draw beI+1 ∈ {1, . . . , E} where probability of e is ∝ λe(t0 | H) 13: return beI+1@btI+1 14: procedure SAMPLE INTENSITY (s[0,T], fϕenc, fϕdec, {(ki, Pi)}M i=1) 15: Assume the last event in s[0,T] is e@t 16: Generate a list of sample times {tj}N j=1, tj ≥ T. 17: Compute the intensity at sample times λetj ← CALC INTEN - SITY (s[0,t], e@t, fϕenc, fϕdec, {(ki, Pi)}M i=1) 18: return {λe(tj | H)}N j=1 DATASET K # E VT TOKENS AVG # EVT TOKENS AVG # EVT TOKENS AVG # SEQ TOTAL PER SEQ PER TASK PER TASK TAOBAO 20 720,000 150 80,000 32 AMAZON 16 360,000 70 42,000 10 STACKOVERFLOW 22 240,000 60 43,000 12 Table 1: Statistics of each dataset. D.2 3S statistics and Experiment on Distribution Shift We use the 3S (sum-of-squared-spacings) statistics proposed by Shchur et al. (2021) to depict the distribution of an event sequence in continuous time. Compared to the classical KS statistics (Lewis, 1965), it uniformly captures multiple properties of event sequence, such as total event count and distribution of inter-event times. Empirically, replacing the KS score with the 3S statistic consistently leads to a better separation between distributions generated by different TPPs. Please refer to the original paper (Shchur et al., 2021) for a detailed discussion. For exploring the distribution shift in the Taobao dataset, we randomly sampled a thousand sequences of events and split them into 10 subsets by timestamps: each subset has approximately equal time horizon and is notated sequentially from 0-th to the 9-th subset. Then we follow the procedure in (Shchur et al., 2021) to compute the 3S statistics for each subset and illustrate the results in Figure 8a. D.3 Evaluation Setup To set up the training and evaluation process, we partition Taobao and Amazon datasets into 10 consecutively rolling slides (namely 10 tasks) and partition the StackOverflow dataset into 6 rolling slides (namely 6 tasks). For the Taobao dataset, each slide covers approximately 1 day of time; for the Amazon dataset, each slide covers 2 years of time; for the StackOverflow dataset, each slide covers approximately 5 months time. The subset in each task is split into training, validation, and test sets with a 70%, 10%, 20% ratio by chronological order. Each task has no overlap in the test set. In such a setting, the total test set covers approximately 70% of data. 17MODEL # PARAMETERS TAOBAOAMAZON SO PRE-NHP 23.3K 23.4K 23.3K PRE-ANHP 25.4K 25.6K 25.4K RE-NHP 23.3K 23.4K 23.3K RE-ANHP 25.4K 25.6K 25.4K O-TPP <1K <1K <1K CL-NHP 27.6K 27.7K 27.6K CL-ANHP 29.5K 29.6K 29.5K PT-NHP 26.2K 26.3K 26.2K PT-ANHP 27.8K 27.0K 27.8K Table 2: Total number of parameters for models trained on the three datasets. We train and evaluate each task sequentially. Our evaluation setup is close to that used in real applications: train the model using a fixed length of historical data and evaluate the model using the following window of the data. … Time … Train Test  Test  … Test  Union of test set Val  Train Val  Train Val  Test Train Val  Task #1 Task #2 Task #3 Task #4 Figure 9: A demonstration of the sliding window validation method. D.4 Implementation Details All models are implemented using the PyTorch framework (Paszke et al., 2017). For the implementation of NHP, AttNHP, and thinning algorithm, we used the code from the public GitHub repository at https://github.com/yangalan123/anhp-andtt (Yang et al., 2022) with MIT License. For O-TPP, as the authors Yang et al. (2017) have not published the code, we implement it using the tick (Bacry et al., 2017) library. For CL-NHP and CL-ANHP, without the public code, by following the main idea of the authors Dubey et al. (2022), we develop the hypernetwork with an MLP layer and add apply regularizer to the hypernetwork parameters while learning a new event sequence, which prevents adaptation of the hypernetworks parameters completely to the new event sequence. Note that, the base models used are NHP and ANHP, respectively, instead of FullyRNN (Omi et al., 2019) applied in the original paper. We implemented our methods with PyTorch (Paszke et al., 2017) and published the code at at https://github.com/yanyanSann/PromptTPP. D.5 Training and Testing Details D.5.1 Training and Hyperparameters Selection Training base TPP model. To train the parameters for a given neural TPP, we performed early stopping based on log-likelihood on the held-out dev set. • For NHP, the main hyperparameters to tune are the hidden dimensionD of the neural network. In practice, the optimal D for a model was usually 32, 64, 128, and we search for the optimal value among them for different datasets. 18MODEL DESCRIPTION VALUE USED TAOBAO AMAZON SO PRE-NHP RNN H IDDEN SIZE 76 76 76 TEMPORAL EMBEDDING 64 32 64 PRE-ANHP H IDDEN SIZE 64 64 64 LAYER NUMBER 3 3 3 RE-NHP RNN H IDDEN SIZE 76 76 76 TEMPORAL EMBEDDING 64 32 64 RE-ANHP H IDDEN SIZE 64 64 64 LAYER NUMBER 3 3 3 O-TPP K ERNEL SIZE 20 × 20 16 × 16 22 × 22 CL-NHP RNN H IDDEN SIZE 64 64 64 TEMPORAL EMBEDDING 64 32 64 CL-ANHP H IDDEN SIZE 64 64 64 LAYER NUMBER 3 3 3 RNN H IDDEN SIZE 64 64 64 M(RETRIEVAL PROMPT POOL SIZE ) 10 10 10 PT-NHP N(TOP-N SELECTED ) 4 4 4 Lp(PROMPT LENGTH ) 10 10 10 C(ASYNCHRONOUS REFRESH FREQUENCY ) 2 2 2 TEMPORAL EMBEDDING 64 32 64 HIDDEN SIZE 64 64 64 LAYER NUMBER 2 2 2 PT-ANHP M(RETRIEVAL PROMPT POOL SIZE ) 10 10 10 N(TOP-N SELECTED ) 4 4 4 Lp(PROMPT LENGTH ) 10 10 10 C(ASYNCHRONOUS REFRESH FREQUENCY ) 2 2 2 Table 3: Descriptions and values of hyperparameters used for models trained on the three datasets. • For AttNHP, in spite of D, another important hyperparameter to tune is the number of layers L of the attention structure. In practice, the optimal L was usually 1, 2, 3, 4. In the experiment, we choose the hyperparameter based on the held-out dev set while keeping AttNHP to have a similar size to that of NHP. Training PromptTPP. We find α in equation 11 is not sensitive and works well in a large range, so we set α = 0.1 consistently for both datasets. For the prompts, we set M = 10, N= 4, Lp = 10 for both datasets. For the asynchronous training parameter C, we choose C = 2 for Taobao and Amazon datasets by default. 19Error Rate RMSE 0.00 0.02 0.04 0.06 0.08p-value T aobao Error Rate RMSE 0.00 0.02 0.04 0.06 0.08p-value Amazon Figure 10: Statistical significance of the improvements from a temporal prompt on Taobao and Amazon datasets. Chosen Optimizer and Hyperparameters. All models are optimized using Adam (Kingma & Ba, 2015). Table 3 contains descriptions that list all of the hyperparameters set throughout our experiments. Testing. As described in Mei & Eisner (2017), we minimized the Bayes risk via the thinning algorithm to determine decisions for what a predicted next event time bti+1 and type bei+1 would be after conditioning on a portion of a sequence s[0,ti] = [ e1@t1, . . . , ei@ti]. All experimental results are averaged over 5 runs, and the corresponding standard deviation is reported as well. In the experiment, we report the metrics on the test set for each task as well as the average metrics over all the tasks. Integral Approximations. During training and testing, there are a number of integrals (e.g., log- likelihood in equation 11) that need to be computed, which are not feasible in closed form. Thus, we must approximate them. All integrals and expectations are approximated via Monte Carlo (MC) estimates with certain amounts of samples used. Lnon_event in equation 1 uses 100 MC samples during training and testing. When evaluating the integrals used for next event predictions in thinning algorithm, we used 100 samples where the sample points were shared across integrals for a single set of predictions in order to save on computation. The exact approximation procedure for the log-likelihood can be found in Mei & Eisner (2017). Environment. All the experiments were conducted on a server with 256G RAM, a 64 logical cores CPU (Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz) and one NVIDIA Tesla P100 GPU for acceleration. D.6 Analysis II Details: Statistical Significance We performed the paired permutation test to validate the significance of our proposed temporal prompt. Particularly, for each model variant (Pt-NHP and Pt-ANHP), we split the test data into ten folds and collected the paired test results with temporal prompt and with the standard prompt, respectively, for each fold. Then we performed the test and computed the p-value following the recipe at https://axon.cs.byu.edu/Dan/478/assignments/permutation_test.php. The results are in Figure 10. It turns out that, on both datasets, the performance differences are strongly significant for the error rate metric (p-value < 0.05 ) and weakly significant for the RMSE metric (p-value ≈ 0.08 ). D.7 More Result: Computational Complexity of PromptTPP The computational complexity comes from two parts: the TPP model and prompt pool. Take ANHP as the base model. Assume the input sequence length is K , event embedding size de. Recall the prompt length Lp, selection size N and prompt pool size M, key size D. ANHP is attention-based, so its original complexity isO(K2de) . By considering the attached prompt, the prompt-augmented ANHP’s complexity becomes O((K + NLp)2d2) . Retrieval’s complexity is O(MD2) . So the total complexity is O((K + NLp)2d2 + MD2) . 20Pre-NHPPre-ANHPRe-NHPRe-ANHPCL-NHPCL-ANHPPt-NHPPt-ANHP 0.8 0.9 1.0 1.1 1.2 1.3 1.41-event Average OTD Pre-NHPPre-ANHPRe-NHPRe-ANHPCL-NHPCL-ANHPPt-NHPPt-ANHP 3.8 4.0 4.2 4.4 4.6 4.83-event Average OTD (a) Evaluation results on Task 9. Pre-NHPPre-ANHPRe-NHPRe-ANHPCL-NHPCL-ANHPPt-NHPPt-ANHP 1.1 1.2 1.3 1.4 1.51-event Average OTD Pre-NHPPre-ANHPRe-NHPRe-ANHPCL-NHPCL-ANHPPt-NHPPt-ANHP 5.0 5.2 5.4 5.6 5.83-event Average OTD (b) Evaluation results on Task 10. Figure 11: OTD distance comparison of generated sequence of all models on Amazon dataset. D.8 More Result: Generation Ability Comparison We investigated the generative ability of the models empirically on Amazon dataset. Given a trained model, we fixed the prefix event sequence and performed the 1-event sampling and 3-event sampling (autoregressively) on the test set of task 9 and task 10. We followed (Xue et al., 2022) to compute the average optimal transport distance (OTD) to measure the distance between the generated sequence and the ground truth. Seen from Figure 11, our proposed models Pt-NHP and Pt-AttNHP achieves the best results. This is consistent with the findings in Main Results in the paper. 21",
      "references": [
        "User-dependent neural sequence models for continuous-time event data.",
        "Dark experience for general continual learning: a strong, simple baseline.",
        "Co2l: Contrastive continual learning.",
        "Prompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners.",
        "Continual treatment effect estimation: Challenges and opportunities.",
        "Leveraging large language models for pre-trained recommender systems.",
        "Continual causal inference with incremental observational data.",
        "Recurrent marked temporal point processes: Embedding event history to vector.",
        "Hyperhawkes: Hypernetwork based neural temporal point process.",
        "Embracing change: Continual learning in deep neural networks.",
        "Tracking dynamic point processes on networks.",
        "Continual learning in predictive autoscaling.",
        "Online learning: A comprehensive survey.",
        "Continual learning of a mixed sequence of similar and dissimilar tasks.",
        "Adam: A method for stochastic optimization.",
        "Overcoming catastrophic forgetting in neural networks.",
        "SNAP Datasets: Stanford large network dataset collection",
        "The power of scale for parameter-efficient prompt tuning.",
        "Some results on tests for poisson processes.",
        "Mixpro: Simple yet effective data augmentation for prompt-based learning.",
        "Prefix-tuning: Optimizing continuous prompts for generation.",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.",
        "Gpt understands, too.",
        "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.",
        "Packnet: Adding multiple tasks to a single network by iterative pruning.",
        "Catastrophic interference in connectionist networks: The sequential learning problem.",
        "The neural Hawkes process: A neurally self-modulating multivariate point process.",
        "Neural Datalog through time: Informed temporal modeling via logical specification.",
        "Fully neural network based model for general temporal point processes.",
        "Automatic differentiation in PyTorch.",
        "Dualnet: Continual learning, fast and slow.",
        "Bellman meets hawkes: Model-based reinforcement learning via temporal point processes.",
        "Progressive neural networks.",
        "Intensity-free learning of temporal point processes.",
        "Detecting anomalous event sequences with temporal point processes.",
        "Language models can improve event prediction by few-shot abductive reasoning.",
        "Privacy-preserving deep learning.",
        "Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift.",
        "Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers.",
        "Prompt augmented generative replay via supervised contrastive learning for lifelong intent detection.",
        "Attention is all you need.",
        "Streaming graph neural networks via continual learning.",
        "Enhancing recommender systems with large language model reasoning graphs.",
        "Dualprompt: Complementary prompting for rehearsal-free continual learning.",
        "Learning to prompt for continual learning.",
        "Modeling the intensity function of point process via recurrent neural networks.",
        "A graph regularized point process model for event propagation sequence.",
        "Hypro: A hybridly normalized probabilistic model for long-horizon prediction of event sequences.",
        "Easytpp: Towards open benchmarking the temporal point processes.",
        "Transformer embeddings of irregularly spaced events and their participants.",
        "Online learning for multivariate hawkes processes.",
        "Continual learning through synaptic intelligence.",
        "Self-attentive Hawkes process.",
        "Transformer Hawkes process."
      ],
      "meta_data": {
        "arxiv_id": "2310.04993v2",
        "authors": [
          "Siqiao Xue",
          "Yan Wang",
          "Zhixuan Chu",
          "Xiaoming Shi",
          "Caigao Jiang",
          "Hongyan Hao",
          "Gangwei Jiang",
          "Xiaoyun Feng",
          "James Y. Zhang",
          "Jun Zhou"
        ],
        "published_date": "2023-10-08T03:41:16Z",
        "github_url": "https://github.com/yanyanSann/PromptTPP"
      },
      "llm_extracted_info": {
        "main_contributions": "PromptTPP is introduced as a novel prompt-augmented Continual Learning (CL) framework for neural Temporal Point Processes (TPPs), addressing the challenge of modeling streaming event sequences. It utilizes a continuous-time retrieval prompt pool where prompts act as parameterized instructions for base TPP models to learn tasks sequentially, thereby enhancing model performance. The paper also formalizes an experimental setup for evaluating streaming event sequences within a CL context and demonstrates the effectiveness of PromptTPP across three real user datasets. By integrating TPP, CL, and prompt learning, the method offers a new perspective on frontier challenges in neural TPPs.",
        "methodology": "PromptTPP operates on a base TPP model and integrates a continuous-time retrieval prompt pool with a prompt-event interaction layer. Instead of directly adapting model weights, it designs prompts to instruct the model conditionally, maintaining model plasticity. The temporal prompt is a time-varying learnable matrix encoding structural and temporal knowledge, with its temporal component derived from continuous-time positional encodings of estimated conditional time. A key-value shared memory space, the retrieval prompt pool, stores these prompts, and a retrieval mechanism dynamically selects a subset of task-relevant prompts based on instance-wise input. The prompt-event interaction uses a multi-head self-attention mechanism, specifically Prefix Tuning, to combine prompts with encoded event states. The model is optimized by minimizing a combined loss function of negative log-likelihood and a surrogate loss to pull selected keys closer to their corresponding queries, with asynchronous refreshing of the prompt pool to accelerate training.",
        "experimental_setup": "The evaluation was conducted on three real-world user behavior datasets: Taobao (user click behaviors, 20 event types), Amazon (user product review behaviors, 16 event types), and StackOverflow (user awards, 22 event types). Each dataset was partitioned into rolling slides (10 tasks for Taobao and Amazon, 6 for StackOverflow) to simulate streaming data, with tasks having no overlap in their test sets. The data within each task was split chronologically into training (70%), validation (10%), and test (20%) sets. Performance was evaluated using next-event prediction metrics: error rate for predicting event type and RMSE for predicting event time. Two strong neural TPPs, NHP and AttNHP, served as base models, resulting in PromptNHP (Pt-NHP) and PromptAttNHP (Pt-ANHP). These were compared against 7 baselines, including Pretrained TPPs, Retrained TPPs, Online TPPs (OnlineMHP), and CL TPPs (HyperHawkes variants). Parameters such as prompt length (Lp=10), selection size (N=4), prompt pool size (M=10), and asynchronous refresh frequency (C=2) were set for PromptTPP. All models were optimized using Adam, and experimental results were averaged over 5 runs with reported standard deviations.",
        "limitations": "The method relies on neural networks, which are typically data-hungry and may underperform compared to non-neural models in data-scarce scenarios. There is also a potential for unethical applications of the model, such as unwanted tracking of individual behavior, despite the intention to facilitate probabilistic modeling of continuous-time sequential data in various domains.",
        "future_research_directions": "The paper implicitly suggests future work could involve exploring the applicability and performance of PromptTPP in data-scarce environments to address its limitation regarding data hunger. Further research could also focus on developing safeguards or ethical guidelines to prevent the misuse of such models for purposes like unwanted tracking of individual behavior. Additionally, further investigation into prompt-related components and hyperparameters could yield more optimized results or expand the generality of the approach to other domains beyond user behavior datasets, as well as testing PromptTPP with other diverse neural TPP architectures.",
        "experimental_code": "# File: model_run/neural_tpp/model/torch_model/torch_baselayer.py\nimport math\nimport torch\nfrom torch import nn\n\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        seq_len = mask.shape[-1]\n        scores[:, :, :seq_len, :seq_len] = scores[:, :, :seq_len, :seq_len].masked_fill(mask > 0, -1000000000.0)\n    p_attn = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return (torch.matmul(p_attn, value), p_attn)\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, n_head, d_input, d_model, dropout=0.1, output_linear=False):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_k = d_model // n_head\n        self.d_v = self.d_k\n        self.d_model = d_model\n        self.output_linear = output_linear\n        if output_linear:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)] + [nn.Linear(d_model, d_model)])\n        else:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)])\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        query, key, value = [lin_layer(x).view(nbatches, -1, self.n_head, self.d_k).transpose(1, 2) for lin_layer, x in zip(self.linears, (query, key, value))]\n        x, attn_weight = attention(query, key, value, mask=mask, dropout=self.dropout)\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.n_head * self.d_k)\n        if self.output_linear:\n            return self.linears[-1](x)\n        else:\n            return x\n\nclass SublayerConnection(nn.Module):\n\n    def __init__(self, d_model, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, self_attn, feed_forward=None, use_residual=False, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.use_residual = use_residual\n        if use_residual:\n            self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n        self.d_model = d_model\n\n    def forward(self, x, mask):\n        if self.use_residual:\n            x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n            if self.feed_forward is not None:\n                return self.sublayer[1](x, self.feed_forward)\n            else:\n                return x\n        else:\n            return self.self_attn(x, x, x, mask)\n\nclass PromptPool(nn.Module):\n\n    def __init__(self, length=5, embed_dim=32, embedding_key='mean', prompt_init='uniform', pool_size=None, top_k=None, batchwise_prompt=False, prompt_key_init='uniform'):\n        super().__init__()\n        self.length = length\n        self.embed_dim = embed_dim\n        self.embedding_key = embedding_key\n        self.prompt_init = prompt_init\n        self.pool_size = pool_size\n        self.top_k = top_k\n        self.batchwise_prompt = batchwise_prompt\n        prompt_pool_shape = (pool_size, length, embed_dim)\n        if prompt_init == 'zero':\n            self.prompt = nn.Parameter(torch.zeros(prompt_pool_shape))\n        elif prompt_init == 'uniform':\n            self.prompt = nn.Parameter(torch.randn(prompt_pool_shape))\n            nn.init.uniform_(self.prompt, -1, 1)\n        key_shape = (pool_size, embed_dim)\n        if prompt_key_init == 'zero':\n            self.prompt_key = nn.Parameter(torch.zeros(key_shape))\n        elif prompt_key_init == 'uniform':\n            self.prompt_key = nn.Parameter(torch.randn(key_shape))\n            nn.init.uniform_(self.prompt_key, -1, 1)\n\n    def l2_normalize(self, x, dim=None, epsilon=1e-12):\n        \"\"\"Normalizes a given vector or matrix.\"\"\"\n        square_sum = torch.sum(x ** 2, dim=dim, keepdim=True)\n        x_inv_norm = torch.rsqrt(torch.maximum(square_sum, torch.tensor(epsilon, device=x.device)))\n        return x * x_inv_norm\n\n    def forward(self, x_embed, prompt_mask=None, cls_features=None):\n        out = dict()\n        if self.embedding_key == 'mean':\n            x_embed_mean = torch.mean(x_embed, dim=1)\n        elif self.embedding_key == 'max':\n            x_embed_mean = torch.max(x_embed, dim=1)[0]\n        elif self.embedding_key == 'mean_max':\n            x_embed_mean = torch.max(x_embed, dim=1)[0] + 2 * torch.mean(x_embed, dim=1)\n        elif self.embedding_key == 'cls':\n            if cls_features is None:\n                x_embed_mean = torch.max(x_embed, dim=1)[0]\n            else:\n                x_embed_mean = cls_features\n        else:\n            raise NotImplementedError('Not supported way of calculating embedding keys!')\n        prompt_norm = self.l2_normalize(self.prompt_key, dim=1)\n        x_embed_norm = self.l2_normalize(x_embed_mean, dim=1)\n        similarity = torch.matmul(x_embed_norm, prompt_norm.t())\n        if prompt_mask is None:\n            _, idx = torch.topk(similarity, k=self.top_k, dim=1)\n            if self.batchwise_prompt:\n                prompt_id, id_counts = torch.unique(idx, return_counts=True, sorted=True)\n                if prompt_id.shape[0] < self.pool_size:\n                    prompt_id = torch.cat([prompt_id, torch.full((self.pool_size - prompt_id.shape[0],), torch.min(idx.flatten()), device=prompt_id.device)])\n                    id_counts = torch.cat([id_counts, torch.full((self.pool_size - id_counts.shape[0],), 0, device=id_counts.device)])\n                _, major_idx = torch.topk(id_counts, k=self.top_k)\n                major_prompt_id = prompt_id[major_idx]\n                idx = major_prompt_id.expand(x_embed.shape[0], -1)\n        else:\n            idx = prompt_mask\n        batched_prompt_raw = self.prompt[idx]\n        batch_size, top_k, length, c = batched_prompt_raw.shape\n        batched_prompt = batched_prompt_raw.reshape(batch_size, top_k * length, c)\n        out['prompt_idx'] = idx\n        out['prompt_norm'] = prompt_norm\n        out['x_embed_norm'] = x_embed_norm\n        out['similarity'] = similarity\n        batched_key_norm = prompt_norm[idx]\n        out['selected_key'] = batched_key_norm\n        x_embed_norm = x_embed_norm.unsqueeze(1)\n        sim = batched_key_norm * x_embed_norm\n        reduce_sim = torch.sum(sim) / x_embed.shape[0]\n        out['reduce_sim'] = reduce_sim\n        out['total_prompt_len'] = batched_prompt.shape[1]\n        out['prompted_embedding'] = torch.cat([batched_prompt, x_embed], dim=1)\n        return out\n\nclass PromptLayer(nn.Module):\n\n    def __init__(self, n_head, d_input, d_model, dropout=0.1, output_linear=False, prompt_length=5, embed_dim=48, embedding_key='mean', prompt_init='uniform', pool_size=10, top_k=5, batchwise_prompt=True, prompt_key_init='uniform'):\n        super(PromptLayer, self).__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_k = d_model // n_head\n        self.d_v = self.d_k\n        self.d_model = d_model\n        self.output_linear = output_linear\n        if output_linear:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)] + [nn.Linear(d_model, d_model)])\n        else:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)])\n        self.dropout = nn.Dropout(p=dropout)\n        self.prompt_length = prompt_length\n        self.embed_dim = embed_dim\n        self.embedding_key = embedding_key\n        self.prompt_init = prompt_init\n        self.pool_size = pool_size\n        self.top_k = top_k\n        self.batchwise_prompt = batchwise_prompt\n        self.prompt_key_init = prompt_key_init\n        self.prompt_pool_k = PromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init)\n        self.prompt_pool_q = PromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init)\n        self.prompt_pool_v = PromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init)\n\n    def forward(self, query, key, value, mask):\n        prompt_mask = None\n        cls_features = None\n        res_k = self.prompt_pool_k(key, prompt_mask=prompt_mask, cls_features=cls_features)\n        res_q = self.prompt_pool_q(query, prompt_mask=prompt_mask, cls_features=cls_features)\n        res_v = self.prompt_pool_v(value, prompt_mask=prompt_mask, cls_features=cls_features)\n        key = res_k['prompted_embedding']\n        query = res_q['prompted_embedding']\n        value = res_v['prompted_embedding']\n        prompt_len = res_k['total_prompt_len']\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        query, key, value = [lin_layer(x).view(nbatches, -1, self.n_head, self.d_k).transpose(1, 2) for lin_layer, x in zip(self.linears, (query, key, value))]\n        x, attn_weight = attention(query, key, value, mask=mask, dropout=self.dropout)\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.n_head * self.d_k)\n        x = x[:, prompt_len:, :]\n        if self.output_linear:\n            return self.linears[-1](x)\n        else:\n            return x\n\ndef prefix_attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        seq_len = mask.shape[-1]\n        scores[:, :, :seq_len, :seq_len] = scores[:, :, :seq_len, :seq_len].masked_fill(mask > 0, -1000000000.0)\n    p_attn = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return (torch.matmul(p_attn, value), p_attn)\n\ndef prefix_attention_nhp(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        seq_len = mask.shape[-1]\n        scores[:, :, :seq_len, :seq_len] = scores[:, :, :seq_len, :seq_len].masked_fill(mask > 0, -1000000000.0)\n    p_attn = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return (torch.matmul(p_attn, value), p_attn)\n\nclass PrxfixPromptLayer(nn.Module):\n\n    def __init__(self, n_head, d_input, d_model, dropout=0.1, output_linear=False, prompt_length=5, embed_dim=64, embedding_key='mean', prompt_init='uniform', pool_size=10, top_k=5, batchwise_prompt=True, prompt_key_init='uniform'):\n        super(PrxfixPromptLayer, self).__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_k = d_model // n_head\n        self.d_v = self.d_k\n        self.d_model = d_model\n        self.output_linear = output_linear\n        if output_linear:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)] + [nn.Linear(d_model, d_model)])\n        else:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)])\n        self.dropout = nn.Dropout(p=dropout)\n        self.prompt_length = prompt_length\n        self.embed_dim = embed_dim\n        self.embedding_key = embedding_key\n        self.prompt_init = prompt_init\n        self.pool_size = pool_size\n        self.top_k = top_k\n        self.batchwise_prompt = batchwise_prompt\n        self.prompt_key_init = prompt_key_init\n        self.prompt_pool_k = PromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init)\n        self.prompt_pool_v = PromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init)\n\n    def forward(self, query, key, value, mask):\n        prompt_mask = None\n        cls_features = None\n        res_k = self.prompt_pool_k(key, prompt_mask=prompt_mask, cls_features=cls_features)\n        res_v = self.prompt_pool_v(value, prompt_mask=prompt_mask, cls_features=cls_features)\n        key = res_k['prompted_embedding']\n        value = res_v['prompted_embedding']\n        prompt_len = res_k['total_prompt_len']\n        prompt_key_loss = res_k['reduce_sim'] + res_v['reduce_sim']\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        query, key, value = [lin_layer(x).view(nbatches, -1, self.n_head, self.d_k).transpose(1, 2) for lin_layer, x in zip(self.linears, (query, key, value))]\n        x, attn_weight = prefix_attention_nhp(query, key, value, mask=mask, dropout=self.dropout)\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.n_head * self.d_k)\n        if self.output_linear:\n            return self.linears[-1](x)\n        else:\n            return (x, prompt_key_loss)\n\nclass ContTPromptPool(nn.Module):\n\n    def __init__(self, length=5, embed_dim=32, embedding_key='mean', prompt_init='uniform', pool_size=None, top_k=None, batchwise_prompt=False, prompt_key_init='uniform', time_cde=None, type_embed_dim=32, time_embed_dim=16, est_time_type='MEAN'):\n        super().__init__()\n        self.length = length\n        self.embed_dim = embed_dim\n        self.embedding_key = embedding_key\n        self.prompt_init = prompt_init\n        self.pool_size = pool_size\n        self.top_k = top_k\n        self.batchwise_prompt = batchwise_prompt\n        self.est_time_type = est_time_type\n        self.time_cde = time_cde\n        self.type_embed_dim = type_embed_dim\n        self.time_embed_dim = time_embed_dim\n        type_prompt_pool_shape = (pool_size, length, type_embed_dim)\n        if prompt_init == 'zero':\n            self.type_prompt = nn.Parameter(torch.zeros(type_prompt_pool_shape))\n        elif prompt_init == 'uniform':\n            self.type_prompt = nn.Parameter(torch.randn(type_prompt_pool_shape))\n            nn.init.uniform_(self.type_prompt, -1, 1)\n        self.time_prompt = None\n        self.prompt = None\n        key_shape = (pool_size, embed_dim)\n        if prompt_key_init == 'zero':\n            self.prompt_key = nn.Parameter(torch.zeros(key_shape))\n        elif prompt_key_init == 'uniform':\n            self.prompt_key = nn.Parameter(torch.randn(key_shape))\n            nn.init.uniform_(self.prompt_key, -1, 1)\n\n    def l2_normalize(self, x, dim=None, epsilon=1e-12):\n        \"\"\"Normalizes a given vector or matrix.\"\"\"\n        square_sum = torch.sum(x ** 2, dim=dim, keepdim=True)\n        x_inv_norm = torch.rsqrt(torch.maximum(square_sum, torch.tensor(epsilon, device=x.device)))\n        return x * x_inv_norm\n\n    def forward(self, x_embed, est_time_emb, prompt_mask=None, cls_features=None):\n        out = dict()\n        if self.embedding_key == 'mean':\n            x_embed_mean = torch.mean(x_embed, dim=1)\n        elif self.embedding_key == 'max':\n            x_embed_mean = torch.max(x_embed, dim=1)[0]\n        elif self.embedding_key == 'mean_max':\n            x_embed_mean = torch.max(x_embed, dim=1)[0] + 2 * torch.mean(x_embed, dim=1)\n        elif self.embedding_key == 'cls':\n            if cls_features is None:\n                x_embed_mean = torch.max(x_embed, dim=1)[0]\n            else:\n                x_embed_mean = cls_features\n        else:\n            raise NotImplementedError('Not supported way of calculating embedding keys!')\n        prompt_norm = self.l2_normalize(self.prompt_key, dim=1)\n        x_embed_norm = self.l2_normalize(x_embed_mean, dim=1)\n        similarity = torch.matmul(x_embed_norm, prompt_norm.t())\n        if prompt_mask is None:\n            _, idx = torch.topk(similarity, k=self.top_k, dim=1)\n            if self.batchwise_prompt:\n                prompt_id, id_counts = torch.unique(idx, return_counts=True, sorted=True)\n                if prompt_id.shape[0] < self.pool_size:\n                    prompt_id = torch.cat([prompt_id, torch.full((self.pool_size - prompt_id.shape[0],), torch.min(idx.flatten()), device=prompt_id.device)])\n                    id_counts = torch.cat([id_counts, torch.full((self.pool_size - id_counts.shape[0],), 0, device=id_counts.device)])\n                _, major_idx = torch.topk(id_counts, k=self.top_k)\n                major_prompt_id = prompt_id[major_idx]\n                idx = major_prompt_id.expand(x_embed.shape[0], -1)\n        else:\n            idx = prompt_mask\n        if self.est_time_type == 'CDE':\n            self.time_prompt = self.time_cde(self.type_prompt)\n        else:\n            self.time_prompt = est_time_emb\n        self.prompt = self.type_prompt\n        batched_prompt_raw = self.prompt[idx]\n        batch_size, top_k, length, c = batched_prompt_raw.shape\n        batched_prompt = batched_prompt_raw.reshape(batch_size, top_k * length, c)\n        self.time_prompt = est_time_emb.expand(-1, batched_prompt.size()[-2], -1)\n        batched_prompt = torch.cat([batched_prompt, self.time_prompt], dim=-1)\n        out['prompt_idx'] = idx\n        out['prompt_norm'] = prompt_norm\n        out['x_embed_norm'] = x_embed_norm\n        out['similarity'] = similarity\n        batched_key_norm = prompt_norm[idx]\n        out['selected_key'] = batched_key_norm\n        x_embed_norm = x_embed_norm.unsqueeze(1)\n        sim = batched_key_norm * x_embed_norm\n        reduce_sim = torch.sum(sim) / x_embed.shape[0]\n        out['reduce_sim'] = reduce_sim\n        out['total_prompt_len'] = batched_prompt.shape[1]\n        out['prompted_embedding'] = torch.cat([batched_prompt, x_embed], dim=1)\n        return out\n\nclass ConTEncoderLayer(nn.Module):\n\n    def __init__(self, d_model, self_attn, feed_forward=None, use_residual=False, dropout=0.1):\n        super(ConTEncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.use_residual = use_residual\n        if use_residual:\n            self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n        self.d_model = d_model\n\n    def forward(self, x, time_emb, est_time_emb, type_emb, mask):\n        if self.use_residual:\n            x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n            if self.feed_forward is not None:\n                return self.sublayer[1](x, self.feed_forward)\n            else:\n                return x\n        else:\n            return self.self_attn(x, time_emb, est_time_emb, type_emb, mask)\n\nclass ContTPrxfixPromptLayer(nn.Module):\n\n    def __init__(self, n_head, d_input, d_model, dropout=0.1, output_linear=False, type_embed_dim=32, time_embed_dim=16, prompt_length=5, embedding_key='mean', prompt_init='uniform', pool_size=10, top_k=5, batchwise_prompt=True, prompt_key_init='uniform'):\n        super(ContTPrxfixPromptLayer, self).__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head\n        self.d_k = d_model // n_head\n        self.d_v = self.d_k\n        self.d_model = d_model\n        self.output_linear = output_linear\n        if output_linear:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)] + [nn.Linear(d_model, d_model)])\n        else:\n            self.linears = nn.ModuleList([nn.Linear(d_input, d_model) for _ in range(3)])\n        self.dropout = nn.Dropout(p=dropout)\n        self.type_embed_dim = type_embed_dim\n        self.time_embed_dim = time_embed_dim\n        self.time_cde = nn.Linear(self.type_embed_dim, self.time_embed_dim, bias=True)\n        self.cde_loss_fn = torch.nn.MSELoss(reduction='mean')\n        self.prompt_length = prompt_length\n        self.embed_dim = self.type_embed_dim + self.time_embed_dim\n        self.embedding_key = embedding_key\n        self.prompt_init = prompt_init\n        self.pool_size = pool_size\n        self.top_k = top_k\n        self.batchwise_prompt = batchwise_prompt\n        self.prompt_key_init = prompt_key_init\n        self.prompt_pool_k = ContTPromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init, time_cde=self.time_cde, type_embed_dim=self.type_embed_dim, time_embed_dim=self.time_embed_dim)\n        self.prompt_pool_v = ContTPromptPool(length=self.prompt_length, embed_dim=self.embed_dim, embedding_key=self.embedding_key, prompt_init=self.prompt_init, pool_size=self.pool_size, top_k=self.top_k, batchwise_prompt=self.batchwise_prompt, prompt_key_init=self.prompt_key_init, time_cde=self.time_cde, type_embed_dim=self.type_embed_dim, time_embed_dim=self.time_embed_dim)\n\n    def forward(self, x, time_emb, est_time_emb, type_emb, mask):\n        time_emb_est = self.time_cde(type_emb)\n        cde_loss = self.cde_loss_fn(time_emb_est, time_emb)\n        query = x\n        key = x\n        value = x\n        prompt_mask = None\n        cls_features = None\n        res_k = self.prompt_pool_k(key, est_time_emb, prompt_mask=prompt_mask, cls_features=cls_features)\n        res_v = self.prompt_pool_v(value, est_time_emb, prompt_mask=prompt_mask, cls_features=cls_features)\n        key = res_k['prompted_embedding']\n        value = res_v['prompted_embedding']\n        prompt_len = res_k['total_prompt_len']\n        prompt_key_loss = res_k['reduce_sim'] + res_v['reduce_sim']\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        query, key, value = [lin_layer(x).view(nbatches, -1, self.n_head, self.d_k).transpose(1, 2) for lin_layer, x in zip(self.linears, (query, key, value))]\n        x, attn_weight = prefix_attention(query, key, value, mask=mask, dropout=self.dropout)\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.n_head * self.d_k)\n        if self.output_linear:\n            return (self.linears[-1](x), prompt_key_loss)\n        else:\n            return (x, prompt_key_loss, cde_loss)\n\n# File: model_run/neural_tpp/model/torch_model/torch_pro_anhp.py\nimport math\nimport torch\nfrom torch import nn\nfrom neural_tpp.model.torch_model.torch_baselayer import EncoderLayer, ConTEncoderLayer, MultiHeadAttention, PromptLayer, PrxfixPromptLayer, ContTPrxfixPromptLayer\nfrom neural_tpp.model.torch_model.torch_basemodel import TorchBaseModel\n\nclass PromptAttNHP(TorchBaseModel):\n    \"\"\"\n    Reference: Continuous Transformer, ICLR 2022\n    https://github.com/yangalan123/anhp-andtt/blob/master/anhp/model/xfmr_nhp_fast.py\n    \"\"\"\n\n    def __init__(self, model_config):\n        super(PromptAttNHP, self).__init__(model_config)\n        self.d_model = model_config['hidden_size']\n        self.d_time = model_config['time_emb_size']\n        self.use_norm = model_config['use_ln']\n        self.div_term = torch.exp(torch.arange(0, self.d_time, 2) * -(math.log(10000.0) / self.d_time)).reshape(1, 1, -1)\n        self.n_layers = model_config['num_layers']\n        self.n_head = model_config['num_heads']\n        self.dropout = model_config['dropout']\n        self.mc_num_sample_per_step = model_config['mc_num_sample_per_step']\n        self.heads = []\n        for i in range(self.n_head):\n            self.heads.append(nn.ModuleList([ConTEncoderLayer(self.d_model + self.d_time, ContTPrxfixPromptLayer(1, self.d_model + self.d_time, self.d_model, self.dropout, output_linear=False), use_residual=False, dropout=self.dropout) for _ in range(self.n_layers)]))\n        self.heads = nn.ModuleList(self.heads)\n        if self.use_norm:\n            self.norm = nn.LayerNorm(self.d_model)\n        self.inten_linear = nn.Linear(self.d_model * self.n_head, self.num_event_types_no_pad)\n        self.softplus = nn.Softplus()\n        self.layer_event_emb = nn.Linear(self.d_model + self.d_time, self.d_model)\n        self.layer_intensity = nn.Sequential(self.inten_linear, self.softplus)\n        self.eps = torch.finfo(torch.float32).eps\n        self.model_to_device()\n\n    def freeze_prompt_layer(self):\n        for head_i in range(self.n_head):\n            for layer_i in range(self.n_layers):\n                for p in self.heads[head_i][layer_i].self_attn.prompt_pool_k.parameters():\n                    p.requires_grad = False\n                for p in self.heads[head_i][layer_i].self_attn.prompt_pool_v.parameters():\n                    p.requires_grad = False\n\n    def freeze_backbone(self):\n        pass\n\n    def warm_prompt_layer(self):\n        for head_i in range(self.n_head):\n            for layer_i in range(self.n_layers):\n                for p in self.heads[head_i][layer_i].self_attn.prompt_pool_k.parameters():\n                    p.requires_grad = True\n                for p in self.heads[head_i][layer_i].self_attn.prompt_pool_v.parameters():\n                    p.requires_grad = True\n\n    def compute_temporal_embedding(self, time):\n        batch_size = time.size(0)\n        seq_len = time.size(1)\n        pe = torch.zeros(batch_size, seq_len, self.d_time).to(time.device)\n        _time = time.unsqueeze(-1)\n        div_term = self.div_term.to(time.device)\n        pe[..., 0::2] = torch.sin(_time * div_term)\n        pe[..., 1::2] = torch.cos(_time * div_term)\n        return pe\n\n    def forward_pass(self, init_cur_layer, time_emb, est_time_emb, type_emb, sample_time_emb, event_emb, combined_mask):\n        cur_layers = []\n        total_prompt_key_loss = 0\n        total_cde_loss = 0\n        seq_len = event_emb.size(1)\n        for head_i in range(self.n_head):\n            cur_layer_ = init_cur_layer\n            for layer_i in range(self.n_layers):\n                layer_ = torch.cat([cur_layer_, sample_time_emb], dim=-1)\n                _combined_input = torch.cat([event_emb, layer_], dim=1)\n                enc_layer = self.heads[head_i][layer_i]\n                enc_output, prompt_key_loss, cde_loss = enc_layer(_combined_input, time_emb, est_time_emb, type_emb, combined_mask)\n                total_prompt_key_loss = total_prompt_key_loss + prompt_key_loss\n                total_cde_loss = total_cde_loss + cde_loss\n                _cur_layer_ = enc_output[:, seq_len:, :]\n                cur_layer_ = torch.tanh(_cur_layer_) + cur_layer_\n                event_emb = torch.cat([enc_output[:, :seq_len, :], time_emb], dim=-1)\n                if self.use_norm:\n                    cur_layer_ = self.norm(cur_layer_)\n            cur_layers.append(cur_layer_)\n        cur_layer_ = torch.cat(cur_layers, dim=-1)\n        return (cur_layer_, total_prompt_key_loss, total_cde_loss)\n\n    def seq_encoding(self, time_seqs, event_seqs):\n        \"\"\"\n\n        Args:\n            time_seqs: time seqs input, [batch_size, seq_len]\n            event_seqs: event type seqs input, [batch_size, seq_len]\n\n        Returns:\n\n        \"\"\"\n        time_emb = self.compute_temporal_embedding(time_seqs)\n        type_emb = torch.tanh(self.layer_type_emb(event_seqs.long()))\n        event_emb = torch.cat([type_emb, time_emb], dim=-1)\n        return (event_emb, time_emb, type_emb)\n\n    def make_layer_mask(self, attention_mask):\n        \"\"\"\n\n        Args:\n            attention_mask: mask for attention operation, [batch_size, seq_len, seq_len]\n\n        Returns:\n            layer mask: mean to keep the current layer, the same size of attention mask\n            a diagonal matrix, [batch_size, seq_len, seq_len]\n\n        \"\"\"\n        layer_mask = (torch.eye(attention_mask.size(1)).to(self.device) < 1).unsqueeze(0).expand_as(attention_mask)\n        return layer_mask\n\n    def make_combined_att_mask(self, attention_mask, layer_mask):\n        \"\"\"\n\n        Args:\n            attention_mask: mask for attention operation, [batch_size, seq_len, seq_len]\n            layer_mask: mask for other layers, [batch_size, seq_len, seq_len]\n\n        Returns:\n            combined_mask:  [batch_size, seq_len * 2, seq_len * 2]\n        \"\"\"\n        combined_mask = torch.cat([attention_mask, layer_mask], dim=-1)\n        contextual_mask = torch.cat([attention_mask, torch.ones_like(layer_mask).to(self.device)], dim=-1)\n        combined_mask = torch.cat([contextual_mask, combined_mask], dim=1)\n        return combined_mask\n\n    def forward(self, time_seqs, time_delta_seq, event_seqs, attention_mask, sample_times=None):\n        event_emb, time_emb, type_emb = self.seq_encoding(time_seqs, event_seqs)\n        init_cur_layer = torch.zeros_like(type_emb)\n        layer_mask = self.make_layer_mask(attention_mask)\n        if sample_times is None:\n            sample_time_emb = time_emb\n        else:\n            sample_time_emb = self.compute_temporal_embedding(sample_times)\n        combined_mask = self.make_combined_att_mask(attention_mask, layer_mask)\n        estimated_inter_delta_time = torch.mean(time_delta_seq, dim=-1)\n        estimated_inter_time = time_delta_seq[:, 0] + estimated_inter_delta_time\n        estimated_inter_time = estimated_inter_time.unsqueeze(-1)\n        est_time_emb = self.compute_temporal_embedding(estimated_inter_time)\n        '\\n        cur_layer_ = self.forward_pass(init_cur_layer, time_emb, type_emb, sample_time_emb, event_emb, combined_mask)\\n\\n        return cur_layer_\\n        '\n        cur_layer_, total_prompt_key_loss, total_cde_loss = self.forward_pass(init_cur_layer, time_emb, est_time_emb, type_emb, sample_time_emb, event_emb, combined_mask)\n        return (cur_layer_, total_prompt_key_loss, total_cde_loss)\n\n    def loglike_loss(self, batch):\n        time_seq, time_delta_seq, event_seq, batch_non_pad_mask, attention_mask, type_mask = batch\n        enc_out, total_prompt_key_loss, total_cde_loss = self.forward(time_seq[:, :-1], time_delta_seq[:, :-1], event_seq[:, :-1], attention_mask[:, 1:, :-1], time_seq[:, 1:])\n        lambda_at_event = self.layer_intensity(enc_out)\n        temp_time = self.make_dtime_loss_samples(time_delta_seq[:, 1:])\n        sample_times = temp_time + time_seq[:, :-1].unsqueeze(-1)\n        lambda_t_sample = self.compute_intensities_at_sample_times(time_seq[:, :-1], time_delta_seq[:, :-1], event_seq[:, :-1], sample_times, attention_mask=attention_mask[:, 1:, :-1])\n        event_ll, non_event_ll, num_events = self.compute_loglikelihood(lambda_at_event=lambda_at_event, lambdas_loss_samples=lambda_t_sample, time_delta_seq=time_delta_seq[:, 1:], seq_mask=batch_non_pad_mask[:, 1:], lambda_type_mask=type_mask[:, 1:])\n        loss = -(event_ll - non_event_ll).sum()\n        prompt_loss_alpha = 0.1\n        loss = loss - prompt_loss_alpha * total_prompt_key_loss\n        return (loss, num_events)\n\n    def compute_states_at_sample_times(self, time_seqs, time_delta_seqs, type_seqs, attention_mask, sample_times):\n        \"\"\"\n\n        Args:\n            type_seqs: [batch_size, seq_len]\n            time_seqs: [batch_size, seq_len]\n            attention_mask: [batch_size, seq_len, seq_len]\n            sample_times: [batch_size, seq_len, num_samples]\n\n        Returns:\n            hidden states at all sampled times: [batch_size, seq_len, num_samples, hidden_size]\n\n        \"\"\"\n        batch_size = type_seqs.size(0)\n        seq_len = type_seqs.size(1)\n        num_samples = sample_times.size(-1)\n        sample_times = sample_times.permute((2, 0, 1))\n        _sample_time = sample_times.reshape(num_samples * batch_size, -1)\n        _types = type_seqs.expand(num_samples, -1, -1).reshape(num_samples * batch_size, -1)\n        _times = time_seqs.expand(num_samples, -1, -1).reshape(num_samples * batch_size, -1)\n        _times_delta = time_delta_seqs.expand(num_samples, -1, -1).reshape(num_samples * batch_size, -1)\n        _attn_mask = attention_mask.unsqueeze(0).expand(num_samples, -1, -1, -1).reshape(num_samples * batch_size, seq_len, seq_len).to(self.device)\n        encoder_output, total_prompt_key_loss, total_cde_loss = self.forward(_times, _times_delta, _types, _attn_mask, _sample_time)\n        encoder_output = encoder_output.reshape(num_samples, batch_size, seq_len, -1)\n        encoder_output = encoder_output.permute((1, 2, 0, 3))\n        return encoder_output\n\n    def compute_intensities_at_sample_times(self, time_seqs, time_delta_seqs, type_seqs, sample_times, **kwargs):\n        \"\"\"\n        Args:\n            time_seqs: [batch_size, seq_len]\n            time_delta_seqs: [batch_size, seq_len]\n            type_seqs: [batch_size, seq_len]\n            sample_times: [batch_size, seq_len, num_samples]\n\n        Returns:\n            intensities at sample times: [batch_size, seq_len, num_samples, num_event_types]\n        \"\"\"\n        attention_mask = kwargs.get('attention_mask', None)\n        if attention_mask is None:\n            batch_size, seq_len = time_seqs.size()\n            attention_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).unsqueeze(0)\n            attention_mask = attention_mask.expand(batch_size, -1, -1).to(torch.bool)\n        encoder_output = self.compute_states_at_sample_times(time_seqs, time_delta_seqs, type_seqs, attention_mask, sample_times)\n        lambdas = self.layer_intensity(encoder_output)\n        return lambdas\n\n# File: model_run/neural_tpp/model_runner.py\nimport os\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport numpy as np\nfrom neural_tpp.model import TorchBaseModel\nfrom neural_tpp.preprocess import TorchTPPDataset, create_torch_dataloader\nfrom neural_tpp.torch_wrapper import TorchModelWrapper\nfrom neural_tpp.utils import TPPMetrics, has_key, set_seed, array_pad_cols\nfrom neural_tpp.utils import create_folder, load_pickle, py_assert, save_config, RunnerPhase, LogHandler\n\nclass ModelRunner:\n\n    def __init__(self, config):\n        \"\"\"\n        Args:\n            config: dict of config\n        \"\"\"\n        self.config = config\n        self.data_config = self.config.get('data', None)\n        self.base_config = config.get(config['model_id']).get('base')\n        self.model_config = config.get(config['model_id']).get('model')\n        self.ensure_valid_config()\n        self.use_torch = self.base_config['use_torch']\n        self.use_tfb = self.base_config['use_tfb']\n        self.update_model_config = True\n        self.update_config()\n        self.init_log()\n        self.build_runner()\n        self.save_updated_config()\n        if 'metrics' in self.base_config:\n            self.metrics_function = TPPMetrics.get_metrics_callback_from_names(self.base_config['metrics'])\n\n    def ensure_valid_config(self):\n        \"\"\"Do some sanity check about the config, to avoid conflicts in settings\n        \"\"\"\n        is_training = self.base_config['is_training']\n        if is_training:\n            py_assert(has_key(self.base_config, ['batch_size', 'max_epoch', 'optimizer', 'learning_rate', 'valid_freq', 'use_tfb', 'metrics']), ValueError, 'Missing train configs in training mode (is_training=True)')\n        else:\n            if not hasattr(self.base_config, 'shuffle'):\n                self.base_config['shuffle'] = False\n            if not hasattr(self.base_config, 'use_tfb'):\n                self.base_config['use_tfb'] = False\n            if not hasattr(self.base_config, 'target_loader'):\n                self.base_config['target_loader'] = 'test'\n        return\n\n    def init_log(self):\n        \"\"\"Initialize the logger\n        \"\"\"\n        self.log = LogHandler(self.base_config['saved_log_dir'])\n        self.log.init(self.config)\n        return\n\n    def update_config(self):\n        \"\"\"Updated config dict\n        \"\"\"\n        time = datetime.now()\n        timestamp = datetime.strftime(time, '%Y%m%d-%H:%M:%S')\n        model_id = self.config['model_id']\n        dataset_id = self.model_config['dataset_id']\n        model_folder_name = model_id + '_' + dataset_id + '_' + timestamp\n        self.log_folder = create_folder(self.base_config['base_dir'], model_folder_name)\n        self.model_folder = create_folder(self.log_folder, 'models')\n        self.base_config['log_folder'] = self.log_folder\n        self.base_config['saved_model_dir'] = os.path.join(self.model_folder, 'saved_model')\n        self.base_config['saved_log_dir'] = os.path.join(self.log_folder, 'log')\n        self.base_config['output_config_dir'] = os.path.join(self.log_folder, f'{model_id}_output.yaml')\n        if self.use_tfb:\n            self.base_config['tfb_train_dir'] = create_folder(self.log_folder, 'tfb_train')\n            self.base_config['tfb_valid_dir'] = create_folder(self.log_folder, 'tfb_valid')\n        if not self.use_torch:\n            self.model_config['is_training'] = self.base_config['is_training']\n        return\n\n    def save_updated_config(self):\n        \"\"\"Update the config dict that is to be saved\n        \"\"\"\n        if has_key(self.base_config, 'metrics') and (not has_key(self.model_config, 'thinning_params')):\n            self.log.warning('The metrics has no effect as thinning params has not been filled: the evaluation needs thinning params to set up an event sampler')\n        run = 'Train' if self.base_config['is_training'] else 'Evaluate'\n        model_name = self.model_config['name']\n        tf_torch = 'PyTorch' if self.base_config['use_torch'] else 'Tensorflow'\n        device = 'GPU' if self.model_config['gpu'] >= 0 else 'CPU'\n        critical_msg = '{run} model {model_name} with {device} with {tf_torch} backend'.format(run=run, model_name=model_name, device=device, tf_torch=tf_torch)\n        self.log.critical(critical_msg)\n        save_config(self.base_config['output_config_dir'], self.config)\n        return\n\n    def build_runner(self):\n        \"\"\"Build up dataloader, model and model wrapper\n        \"\"\"\n        if self.use_torch:\n            set_seed(self.model_config['seed'])\n            [self.train_loader, self.dev_loader, self.test_loader] = self.get_dataloader(TorchTPPDataset, create_torch_dataloader)\n            self.model = TorchBaseModel.generate_model_from_config(model_config=self.model_config)\n            self.model_wrapper = TorchModelWrapper(self.model, self.base_config, self.model_config)\n        else:\n            print('The model currently only supports Torch. ')\n        return\n\n    def get_dataloader(self, dataset_cls, dataloader_fn=None, num_event_types=None):\n        \"\"\"Assume that we load data from pickle files with GaTech format.\n        One can modify this function to accommodate other customized format.\n\n        Args:\n            dataset_cls: a Tf or Torch dataset class object\n            dataloader_fn: a mapper from dataset object to data loader\n            num_event_types: default None\n\n\n        Returns:\n            train, dev and test dataloader\n\n        \"\"\"\n        loaders = []\n        splits = ['train', 'dev', 'test']\n        for _split in splits:\n            with open(self.data_config.get('{}_data'.format(_split)), 'rb') as f_in:\n                data = load_pickle(f_in)\n                if num_event_types is None:\n                    num_event_types = data['dim_process']\n                else:\n                    py_assert(data['dim_process'] == num_event_types, ValueError, 'inconsistent dim_process in different splits?')\n                dataset = dataset_cls(dict(event_num=num_event_types, source_data=data[_split]), batch_size=self.base_config['batch_size'])\n                loaders.append(dataloader_fn(dataset, batch_size=self.base_config['batch_size'], shuffle=self.base_config['shuffle']))\n                if self.update_model_config:\n                    self.model_config['num_event_types_pad'] = dataset.num_event_types_pad\n                    self.model_config['num_event_types_no_pad'] = dataset.num_event_types_no_pad\n                    self.model_config['event_pad_index'] = dataset.event_pad_index\n                    self.update_model_config = False\n        return loaders\n\n    def train(self):\n        \"\"\"train the model\n        \"\"\"\n        for i in range(self.base_config['max_epoch']):\n            train_metrics = self.run_one_epoch(self.train_loader, RunnerPhase.TRAIN)\n            message = f'[ Epoch {i} (train) ]: train ' + TPPMetrics.metrics_dict_to_str(train_metrics)\n            self.log.info(message)\n            self.model_wrapper.write_summary(i, train_metrics, RunnerPhase.TRAIN)\n            if i % self.base_config['valid_freq'] == 0:\n                valid_metrics = self.run_one_epoch(self.dev_loader, RunnerPhase.VALIDATE)\n                self.model_wrapper.write_summary(i, valid_metrics, RunnerPhase.VALIDATE)\n                message = f'[ Epoch {i} (valid) ]:  valid ' + TPPMetrics.metrics_dict_to_str(valid_metrics)\n                self.log.info(message)\n                updated = self.log.update_best('loglike', valid_metrics['loglike'], i)\n                message = 'current best loglike is {:.4f} (updated at epoch-{})'.format(self.log.current_best['loglike'], self.log.episode_best)\n                self.log.critical(message)\n                if updated:\n                    message += f', best updated at this epoch'\n                    self.model_wrapper.save(self.base_config['saved_model_dir'])\n                test_metrics = self.run_one_epoch(self.test_loader, RunnerPhase.VALIDATE)\n                message = f'[ Epoch {i} (test) ]: test ' + TPPMetrics.metrics_dict_to_str(test_metrics)\n                self.log.info(message)\n        self.model_wrapper.close_summary()\n        return\n\n    def eval(self):\n        \"\"\"Perform the one step prediction given the ground truth sequence\n        \"\"\"\n        data_loader = self.test_loader if self.base_config['target_loader'] == 'test' else self.dev_loader\n        test_metrics = self.run_one_epoch(data_loader, RunnerPhase.VALIDATE)\n        self.model_wrapper.write_summary(0, test_metrics, RunnerPhase.VALIDATE)\n        self.model_wrapper.close_summary()\n        message = f'Evaluation result: ' + TPPMetrics.metrics_dict_to_str(test_metrics)\n        self.log.critical(message)\n        return\n\n    def run_one_epoch(self, data_loader, phase):\n        \"\"\"Run one complete epoch\n\n        Args:\n            data_loader: data loader object defined in model runner\n            phase: enum, [train, dev, test]\n\n        Returns:\n            a dict of metrics\n\n        \"\"\"\n        total_loss = 0\n        total_num_event = 0\n        epoch_label = []\n        epoch_pred = []\n        epoch_mask = []\n        pad_index = self.data_config['event_pad_index']\n        for batch in data_loader:\n            batch_loss, batch_num_event, batch_pred, batch_label, batch_mask = self.model_wrapper.run_batch(batch, phase=phase)\n            total_loss += batch_loss\n            total_num_event += batch_num_event\n            epoch_pred.append(batch_pred)\n            epoch_label.append(batch_label)\n            epoch_mask.append(batch_mask)\n        pred_exists, label_exists = (False, False)\n        if epoch_pred[0][0] is not None:\n            epoch_pred = self.concat_element(epoch_pred, pad_index)\n            pred_exists = True\n        if epoch_label[0][0] is not None:\n            epoch_label = self.concat_element(epoch_label, pad_index)\n            label_exists = True\n            epoch_mask = self.concat_element(epoch_mask, False)[0]\n            epoch_mask = epoch_mask.astype(bool)\n        avg_loss = total_loss / total_num_event\n        metrics_dict = OrderedDict()\n        metrics_dict.update({'loglike': -avg_loss, 'num_events': total_num_event})\n        if pred_exists and label_exists:\n            metrics_dict.update(self.metrics_function(epoch_pred, epoch_label, seq_mask=epoch_mask))\n        return metrics_dict\n\n    @staticmethod\n    def concat_element(arrs, pad_index):\n        \"\"\" Concat element from each batch output  \"\"\"\n        n_lens = len(arrs)\n        n_elements = len(arrs[0])\n        max_len = max([x[0].shape[1] for x in arrs])\n        concated_outputs = []\n        for j in range(n_elements):\n            a_output = []\n            for i in range(n_lens):\n                arrs_ = array_pad_cols(arrs[i][j], max_num_cols=max_len, pad_index=pad_index)\n                a_output.append(arrs_)\n            concated_outputs.append(np.concatenate(a_output, axis=0))\n        return concated_outputs\n\n# File: model_run/neural_tpp/preprocess/torch_dataset.py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom neural_tpp.preprocess.input import EventSeq\n\nclass TorchTPPDataset(Dataset):\n\n    def __init__(self, data_config, **kwargs):\n        self.data_config = data_config\n        self.kwargs = kwargs\n        self.event_seq_cls = EventSeq(event_num=data_config['event_num'], add_bos=data_config.get('add_bos', False), add_eos=data_config.get('add_eos', False), data_dir=data_config.get('data_dir', None), eos_elapse=data_config.get('eos_elapse', False), pad_end=data_config.get('pad_end', True), source_data=data_config.get('source_data', None))\n        self.time_seq, self.time_delta_seq, self.event_seq = self.event_seq_cls.build()\n        self.max_len = max([len(x) for x in self.time_seq])\n\n    def __len__(self):\n        \"\"\"\n\n        Returns: length of the dataset\n\n        \"\"\"\n        assert len(self.time_seq) == len(self.event_seq) and len(self.time_delta_seq) == len(self.event_seq), f'Inconsistent lengths for data! time_seq_len:{len(self.time_seq)}, event_len: {len(self.event_seq)}, time_delta_seq_len: {len(self.time_delta_seq)}'\n        return len(self.event_seq)\n\n    def __getitem__(self, idx):\n        \"\"\"\n\n        Args:\n            idx: iteration index\n\n        Returns:\n            time_seq, time_delta_seq and event_seq element\n\n        \"\"\"\n        return (self.time_seq[idx], self.time_delta_seq[idx], self.event_seq[idx])\n\n    def collate_fn(self, batch):\n        \"\"\"\n\n        Args:\n            batch: batch sequence data\n\n        Returns:\n            batch tensors of time_seq, time_delta_seq, event_seq,\n            batch_non_pad_mask, attention_mask, type_mask\n\n        \"\"\"\n        time_seq, time_delta_seq, event_seq = list(zip(*batch))\n        time_seq = torch.tensor(self.event_seq_cls.batch_pad_sequence(time_seq), dtype=torch.float32)\n        time_delta_seq = torch.tensor(self.event_seq_cls.batch_pad_sequence(time_delta_seq), dtype=torch.float32)\n        event_seq = torch.tensor(self.event_seq_cls.batch_pad_sequence(event_seq), dtype=torch.long)\n        batch_non_pad_mask, attention_mask = self.event_seq_cls.batch_attn_mask_for_pad_sequence(event_seq)\n        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)\n        type_mask = torch.tensor(self.event_seq_cls.batch_type_mask(event_seq), dtype=torch.bool)\n        return (time_seq, time_delta_seq, event_seq, batch_non_pad_mask, attention_mask, type_mask)\n\n    @property\n    def num_event_types_pad(self):\n        \"\"\"\n\n        Returns: num event types with padding\n\n        \"\"\"\n        return self.event_seq_cls.event_num_with_pad\n\n    @property\n    def num_event_types_no_pad(self):\n        \"\"\"\n\n        Returns: num event types without padding\n\n        \"\"\"\n        return self.event_seq_cls.event_num\n\n    @property\n    def event_pad_index(self):\n        \"\"\"\n\n        Returns: pad index for event sequence\n\n        \"\"\"\n        return self.event_seq_cls.pad_index\n\ndef create_torch_dataloader(dataset, batch_size, **kwargs):\n    \"\"\"\n\n    Args:\n        dataset: TorchTPPDataset object\n        batch_size: batch size to load the data\n        **kwargs: optional parameters, e.g., shuffle, num_workers\n\n    Returns:\n        torch.DataLoader object\n\n    \"\"\"\n    return DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=kwargs.get('shuffle', True))\n\n# File: model_run/neural_tpp/train_eval.py\nimport argparse\nfrom neural_tpp.model_runner import ModelRunner\nfrom neural_tpp.utils import load_config\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=False, default='step_1_train_nhp.yaml', help='Configuration dir to train and evaluate the model.')\n    args = parser.parse_args()\n    config = load_config(args.config)\n    model_runner = ModelRunner(config)\n    model_runner.train()\nif __name__ == '__main__':\n    main()\n\n# File: model_run/run_pt_anhp.py\nimport argparse\nfrom neural_tpp.model_runner import ModelRunner\nfrom neural_tpp.utils import load_config\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config_dir', type=str, required=False, default='example_config', help='Configuration folder dir to train and evaluate the model.')\n    parser.add_argument('--experiment_id', type=str, required=False, default='PromptAttNHP_train', help='Experiment id in the config file.')\n    args = parser.parse_args()\n    config = load_config(args.config_dir, args.experiment_id)\n    model_runner = ModelRunner(config)\n    model_runner.train()\nif __name__ == '__main__':\n    main()",
        "experimental_info": "# File: model_run/example_config/dataset_config.yaml\n\namazon:\n  data_root: ../data/amazon/\n  data_format: pkl\n  train_data: ../data/amazon/amazon_task_8_train.pkl\n  dev_data: ../data/amazon/amazon_task_8_dev.pkl\n  test_data: ../data/amazon/amazon_task_8_test.pkl\n  num_event_types_pad: 17\n  num_event_types_no_pad: 16\n  event_pad_index: 16\n\n\n# File: model_run/example_config/model_config.yaml\nPromptAttNHP_train:\n  base:\n    use_torch: True\n    is_training: True\n    base_dir: './checkpoints/'\n    batch_size: 256\n    max_epoch: 100\n    shuffle: False\n    optimizer: adam\n    learning_rate: 1.e-3\n    valid_freq: 1\n    use_tfb: False\n    metrics: [ 'ACC', 'RMSE' ]\n    pretrained_model_dir: None\n    pretrain_type: Prompt_only\n  model:\n    name: PromptAttNHP\n    dataset_id: amazon\n    hidden_size: 32\n    time_emb_size: 16\n    num_layers: 2\n    num_heads: 2\n    mc_num_sample_per_step: 20\n    sharing_param_layer: False\n    loss_integral_num_sample_per_step: 20\n    dropout: 0.0\n    use_ln: False\n    seed: 2019\n    gpu: 0\n    thinning_params:\n      num_seq: 10\n      num_sample: 1\n      num_exp: 500\n      look_ahead_time: 10\n      patience_counter: 5\n      over_sample_rate: 5\n      num_samples_boundary: 5\n      dtime_max: 5"
      }
    },
    {
      "title": "Cognitive Model Discovery via Disentangled RNNs",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Social Motion Prediction with Cognitive Hierarchies",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Enhancing LLM’s Cognition via Structurization",
      "full_text": "Enhancing LLM’s Cognition via Structurization Kai Liu1,2∗, Zhihang Fu 2†, Chao Chen 2, Wei Zhang 1, Rongxin Jiang 1†, Fan Zhou1, Yaowu Chen1, Yue Wu2, Jieping Ye2 1Zhejiang University, 2Alibaba Cloud Abstract When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequen- tial perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM’s cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks ( e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single- round structurization. In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the halluci- nation evaluator. Besides, we show the feasibility of distilling advanced LLMs’ language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code is available at https://github.com/alibaba/struxgpt. 1 Introduction Large language models (LLMs) have emerged with remarkable language capabilities [6, 53, 1], yet remain at a discernible distance from human-level intelligence, especially when handling long-form, sophisticated contexts as inputs [3, 36]. Scaling up the model size has significant benefits for boosting context-comprehension and instruction-following abilities for LLMs [47, 58]. However, it is generally resource-intensive on both model training and inference. This paper presents another perspective on enhancing LLMs’ cognition capability without altering the models: context structurization. The idea of structurization is motivated by neurocognitive science [51, 5, 17]. In human cognition, as indicated in Fig. 1, sophisticated text sequences will be processed and consolidated into a structured knowledge tree, with factual elements well-organized hierarchically [28, 15]. This process is defined as structurization. People can precisely search information from general concepts to specific details and make connections and comparisons along structures. We thus aim to transform plain texts into structurized inputs, helping LLMs recognize and understand contexts in a human manner [71]. As Fig. 1 illustrates, input sequences are reorganized in a simple but generic three-layer structure: scope, aspects, and descriptions. The scope summarizes the topic and contents, unfolding into several main aspects with corresponding detailed descriptions. The structurized results can be freely *Work done during Kai Liu’s research internship at Alibaba Cloud. Email: kail@zju.edu.cn. †Corresponding authors. Email: rongxinj@zju.edu.cn, zhihang.fzh@alibaba-inc.com. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2407.16434v2  [cs.CL]  31 Oct 2024assembled into various natural language forms, depending on the specific downstream tasks. Fig. 2 provides an exemplar of our overall framework: after structurizing and reassembling the vanilla context to highlight its scope and aspects of data imbalance handling techniques, LLMs become able to grasp the target information about the dynamic weighting strategy and generate reliable responses. The idea of language identiﬁcation is to  classify a given audio signal into a particular  class using a classiﬁcation algorithm.  Commonly language identiﬁcation task was  done using i-vector systems. A well known  approach for language identiﬁcation proposed  by N. Dahek et al. uses the GMM-UBM model  to obtain utterance level features called i- vectors. Recent advances in deep learning have  helped to improve the language identiﬁcation  task using many different neural network  architectures which can be trained efﬁciently  using GPUs for large scale datasets … Context Perceiver Scope: Language Identification Aspect: Method Overview Desc: i-vector is a well-known  approach using GMM-UBM … Aspect: Pooling Strategies Aspect: Model Architecture Desc: Deep learning advances  have improved language… Desc: NetVLAD pooling:  replaces hard assignment… Desc: GhostVLAD pooling:  An extension of NetVLAD… Desc: convolution layers with  ReLU activation functions…  Desc: feature cube is converted  into a 2D feature maps… Cognition Figure 1: Structured cognition on sequential contexts. Humans may easily identify a given pas- sage’s topic/scope, break down the text sentences into several aspect points with detailed descrip- tions, and form a tree-like knowledge structure. We first execute the structurization by prompt- ing advanced commercial LLMs (e.g., GPT-3.5- Turbo1 or Qwen-Max2) with a few examples, and then collect the results to train a smaller 7B-parameter LLaMA2 [53] or Qwen [2] model as our StruXGPT. This is motivated by [ 48], where the fundamental syntactic processing abil- ity from giant LLMs is distilled into a responsive and affordable StruXGPT-7B. Comprehensive evaluations indicate that StruXGPT-7B inher- its 97% of the structurization ability from the teacher model, showing our method’s feasibility. Empirical experiments are conducted on a di- verse set of NLP tasks ( e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). The results show that with a single-turn struc- turization by our StruXGPT, the cognition per- formance of vanilla large language models wit- nesses consistent improvements regardless of the model architecture and size variation. In particular, we boost the open-sourced LLaMA2- 70B [53] to achieve comparable performance against GPT-3.5-Turbo as a hallucination evaluator, and demonstrate the compatibility with other advanced prompting techniques, such as CoT [59]. We hope this paper can bring new insights to the community on building a more powerful and critical language model with human cognition. Our contribution can be summarized as follows: • We propose the concept of structurization, in order to enhance LLM’s cognition capability without altering the models themselves. • We present the feasibility of distilling the structurization ability from giant commercial LLMs into a responsive and affordable StruXGPT-7B model, making our approach practical. • With structurization, we empirically demonstrate the consistent cognition enhancement for various LLMs across model architecture and size variation on diverse NLP tasks. 2 Related Work Large language models (LLMs). LLMs’ emergent abilities [58] has recently received extensive attention in the literature [6, 61, 9, 53, 23, 1], which are found closely related to the scaling law [26]. When the scale reaches a certain level, the performance on complex NLP tasks significantly rises due to the superior in-context learning, instruction following, and reasoning abilities [71, 47]. Numerous efforts have been made to boost the model capacity with training and prompting strategies [37, 38, 59, 14], and this paper presents a new perspective, context structurization, to encourage LLMs to perceive, recognize, and communicate like humans [55, 71], without altering the model themselves. Context augmentation. Recent studies have proposed several context augmentation methods to enhance LLM’s cognition ability [59, 41] on when taking the long-form context (with thousands of tokens) as inputs [3, 36, 30]. Specifically, aspect-based summarization (ABS) and query-based summarization (QBS) [64, 69] are designed to extract important information from lengthy text data as well, but they require pre-defined or user-input aspect/query lists to conduct targeted summarization, and the detailed information will be inevitably lost during the summarization process. In contrast, the 1https://openai.com/ 2https://dashscope.aliyun.com/ 2Vanilla Structurized How are weights dynamically adjusted? The article does not provide information. To address the dominating influence  of easy-negative examples, each  training sample is assigned a weight  proportional to $(1-p)$ … This passage talks about the data imbalance handling in NLP  tasks, which involves several aspects: 1. Introduction to data imbalance. A common issue in NLP tasks … 2. Training objectives. CE or MLE objectives are widely adopted … 3. Handling easy-negative examples: A dynamic weight adjusting  strategy is proposed to associates each training example with … 4. Combining strategies. It significantly boosts performance … StruXGPT Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine  reading comprehension. … Data imbalance results in the following two issues: (1) the training- test discrepancy: … The cross-entropy objective (CE for short) or maximum likelihood (MLE)  objective, which is widely adopted as the training objective for data-imbalanced NLP tasks …  Inspired by the idea of focal loss in computer vision, we propose a dynamic weight adjusting  strategy, which associates each training example with a weight in proportion to $(1-p)$  …  Combing both strategies, we observe signiﬁcant performance boosts on a wide range of data  imbalanced NLP tasks…. Context LLM Response Figure 2: Framework overview. When instructed to generate responses based on vanilla long-form and sophisticated contexts, LLMs often lose their focus and give unreliable answers due to their limited cognition capability. In contrast, we structurize the vanilla context by using our StruXGPT to identify its main scope and aspect points, facilitating the original LLMs to comprehend the context and generate accurate responses. paper develops context-wise structurization to highlight the knowledge structure, running a single-turn structurization on the context to enhance LLMs’ cognition abilities on a diverse set of NLP tasks. Structurization. In the conventional NLP literature, the term structured data usually refers to entity-relation-entity triplets or properties extracted from plain texts, which are utilized to construct knowledge graphs or databases with special data formats or schemas [24, 32]. On the contrary, the structurization in this paper does not focus on entity-level information extraction and aggregation. Instead, it suggests reorganizing the input sentences into a three-layer structure based on their inner linguistic relations. Similar to discourse analysis and constituency parsing [10, 27], the main purpose of our structurization is capturing the dependencies and relations of the elements within specific long-form text inputs, so as to enhance LLMs’ cognition of the knowledge structure and relations. Knowledge Distillation. In the era of large language model learning, distilling specific knowledge from giant LLMs’ outputs has commonly been used to derive a more affordable but still powerful language model [63, 19]. Previous attempts, such as Standford Alpaca [ 50] and Vicuna [8], show the feasibility of collecting instruction-response pairs from GPT-3.5 or GPT-4 to train a smaller fundamental [46] or domain-specific (e.g., with reasoning [43] or coding [20] skills) language models. Our work also distills the structurization capability from the giant Qwen-Max model to a smaller yet effective StruXGPT-7B model, with extensive evaluations to show the efficiency and efficacy. 3 Structurization The substantial purpose of structurization is to mimic the human cognition process and transform plain, sequential text sentences into a well-organized, hierarchical knowledge structure. Inspired by the linguistic discourse analysis [ 11, 18], this paper develops a three-layer hierarchy exemplar to present the cognition/knowledge structure, as introduced in Fig. 1: (1) Scope summarizes the topic and boundary of the textual context. It outlines the central issues of knowledge throughout the text and the scope of the discussion that will be covered. (2) Aspect further subdivides the input context into several parts. It presents the aspects or dimen- sions that must be considered to fully understand the topic and scope. (3) Description is the most specific and detailed layer. It provides in-depth descriptions and analyses to support each aspect of the context scope. This generic three-layer structure is derived for efficacy and efficiency in dealing with diverse textual inputs. There might be some elaborated structures (such as a knowledge mindmap [ 60]) to better deconstruct specific text sources, but the difficulty and complexity of defining, extracting, and utilizing those structures to aid in practical problems are dramatically increased. We leave this exploration in our future work. Next, we will present how to implement effective structurization with minimal cost in Sec. 3.1, and demonstrate the utilization of the structurization results to enhance LLMs’ cognition abilities in different downstream tasks in Sec. 3.2. 33.1 Efficient Implementation of Structurization We have explored two approaches to execute the structurization process by leveraging the extraor- dinary capability of large language models: few-shot prompting on commercial LLMs and direct instructing our developed StruXGPT models. Few-shot commercial LLM. In the initial stage, we use two in-context examples to query the commercial Qwen-Max model to structurize the input corpus, since it shows promising instruction- following and textual analyzing capability with over 200B parameters 3. Here is a simplified example of prompting structurization in Fig. 3, and the full template is displayed in Appendix E.1. Given a sequential statement, you are supposed to identify: ## Statement's scope: ```[generally a noun phrase]``` ## Statement's main aspects and corresponding descriptions: ``` 1. [the first aspect of the statement] 1.1 [a descriptive sentence] 1.3 [another descriptive sentence] 2. [the second aspect of the statement] 2.1 [a descriptive sentence] 3. [another aspect of the statement] ``` ### Input: {input_statement} ### Output: Figure 3: Prompt template for structurization. However, commercial LLMs are usually slow and expensive, and sending user data to LLM APIs may cause privacy and security problems due to information leakage. Thus, we train a smaller 7b-parameter model (e.g., LLaMA2 [53] or Qwen [2]) to inherit the structurization ability from giant commercial LLMs, which can be deployed locally for efficiency and privacy. Fine-tuned StruXGPT. Our tailored model is named by StruXGPT, where Stru is the abbre- viation of structurization, and X implies we do not specify the model architecture. We carefully curate 22,547 raw data pieces from Wikipedia4 and CAMEL-AI dataset [31] to ensure diversity, and collect the structurized results from Qwen- Max to train our StruXGPT via supervised fine- tuning (SFT). From the collected samples, 200 are utilized for evaluation (including human verification), and the remaining training samples are adopted to distill the structurization capability from Qwen-Max to our StruXGPT-7B. It is practical since the structurization only relies on fundamental syntactic understanding and processing ability, which has already been learned from the large-scale corpus. We merely teach the 7B-parameter models how to reorganize the input text via SFT, without introducing new memorizing or creative overloads. In addition, the ultimate StruXGPT does not require few-shot examples, and it reduces the input lengths for further efficiency. The training details are described in Appendix A.1. 3.2 Effective Utilization of Structurization The identified knowledge structure (i.e., the scope, aspect, and description hierarchy) from the raw context is initially parsed in JSON format and unsuitable for direct inputs to handle massive lengthy elements. Therefore, we use a unified template to transform the structured data back into natural language sentences as models’ inputs to fit their intrinsic processing patterns, as LLMs are pre-trained and aligned with mostly natural language data. Concurrently, to preserve and highlight the knowledge structure, we harness specific linguistic markers to signal hierarchy and relationships among concept elements, such as numbered lists for order, bullet points for categorization, and indentation to depict nesting levels of information. Fig. 4 showcases some typical examples. The first row of Fig. 4 provides a unified template to transform structurization results into natural languages. The top-level hierarchy scope is presented as a standalone sentence, serving as the introduction to the structured context and highlighted with bold markers. Subsequently, the secondary aspect are organized with numerical markers and bolded, attaching with its corresponding tertiary descriptions through subclauses or separate sentences. This method not only signifies the rank and relation of each piece of information relative to others but also provides clear, navigable paths for the LLMs to follow and process the information efficiently ( e.g., when examining the long-form comprehension capability). Moreover, the second row of Fig. 4 introduces another variation for transformation, where each description elements are further broken down and enumerated for the delineation of fine-grained details, making it easier for language models to discern and retain specific nuances associated with each aspect (e.g., when examining the hallucination detection capability). 3https://rank.opencompass.org.cn/leaderboard-llm-v2 4We use the 20231020-en dump from https://dumps.wikimedia.org/. 4The life and career of director George Cukor can be deconstructed as: 1. Early life and background - Cukor was born on the Lower East Side of Manhattan, the younger child  and only son of… - His parents selected his middle name in honor of Spanish–American War  hero George… 2. Career milestones - Cukor won the Academy Award for Best Director for \"My Fair Lady\"  (1964), which was … - He continued to work into the 1980s…. This passage talks about {Scope}: 1. **{Aspect 1}**: {Desc 1.1}. {Desc 1.2}. 2. **{Aspect 2}**: {Desc 2.1}. {Desc 2.2}. 3. **{Aspect 3}**: … 4. … {Scope} can be deconstructed as: 1. **{Aspect 1}**  - {Desc 1.1}. - {Desc 1.2}. 2. **{Aspect 2}**  - {Desc 2.1}.  3. … This passage talks about the PrivacyQA dataset and its characteristics: 1. Introduction to Privacy Policies. Privacy policies are legal documents  that disclose how companies gather, use, share, and manage user data… 2. Privacy Policy Misuse and Lack of Awareness. Lack of awareness … 3. PrivacyQA Dataset. Motivated by the need for quick identification … 4. Data Collection Methodology. PrivacyQA comprises 35 mobile  application privacy policies collected from the Google Play Store… Templates Examples Figure 4: Left: templates to transform structurization results into natural languages, with special linguistic markers to preserve and highlight the extracted knowledge structure. Right: transformed context examples with clear information structure for long-form reading comprehension (upper) and hallucination detection (lower) tasks. After transformation, the linguistic input retains its structured knowledge through systematic cues but is presented in a comprehensible manner for LLMs. This not only facilitates an enhanced understanding and interaction with complex data but also enables the models to leverage their existing natural language capabilities to generate more accurate and contextually relevant responses. 4 Experiments In this section, we conduct extensive experiments on a series of downstream NLP tasks to compre- hensively demonstrate the efficacy of structurization. We hope the results can bring new insights to enhancing LLM’s cognition via structurization, regardless of model architecture and size variation. Three representative natural language understanding and processing tasks are investigated, includ- ing context-based question-answering Sec. 4.1, exhaustive hallucination evaluation Sec. 4.2, and passage-level dense retrieval Sec. 4.3. When instructing tested LLMs to perform target tasks, we merely structurize the vanilla textual inputs, transform the results back into natural languages, and immediately feed the results to LLMs to make responses. The tested LLMs themselves are not fine-tuned on structurized data corpus. We adopt our StruXGPT-7B model to execute structurization in all experiments in this section. The evaluation and ablation of StruXGPT model are demonstrated in Sec. 4.4, Sec. 4.5, and Sec. 4.6, and more comparison with related augmentation-based methods can be found in Appendix B.3 and Appendix B.5. 4.1 Application on Context-based Question-Answering Question-answering based on a long-form context is an emerging research area within QA, which requires large language models to precisely seek the target information and generate reliable responses to the question [3, 30]. It is an immediate measure of LLM’s cognition ability to handle intricate and sophisticated contexts. In this section, we comprehensively evaluate how structurization boosts QA ability on seven datasets from the LongBench benchmark [3] with a variety of LLMs to examine. Dataset setup. LongBench [3] is a multi-task benchmark tailored for long context understanding evaluation, composed of 6 major task categories and 21 different tasks. To focus on the investigation of context structurization, we choose 7 subsets from LongBench across single-document QA, multi- document QA, and synthetic QA tasks in English, and the remaining Chinese subsets or code- orientated tasks are eliminated. Except for the MultiFieldQA subset with 150 testing samples, each subset contains 200 pieces of context-question-answer triplets to evaluate, resulting in 1,350 samples to test in total. Each subset has a 4K-18K context length on average. If the context length exceeds an LLM’s window size, we truncate from the middle of the text and preserve information at the beginning and end, as suggested by LongBench. Detailed dataset description is displayed in Appendix B.2. 5Table 1: Performance on LongBench datasets. The prefix Struct- indicates the data fed into LLMs is structurized by our StruXGPT-7B, while the evaluated LLMs themselves are unchanged. The results are acquired by LongBench’s official protocol. Higher is better. Method SingleDoc QA MultiDoc QA Synthetic Tasks Average Qasper MFQA HpQA 2Wiki Musique PsgCnt PsgRet LLaMA2-7B-4k 19.5 34.6 30.4 27.3 10.7 2.0 9.0 19.1 +StruXGPT (ours) 23.1 35.9 32.7 29.9 13.4 3.0 12.0 21.4 LLaMA2-13B-4k 26.9 34.5 38.9 34.4 13.9 2.0 10.0 22.9 +StruXGPT (ours) 28.5 35.3 40.0 39.4 18.9 3.5 16.0 26.0 Qwen-7B-8k 19.6 34.1 20.4 12.5 7.5 2.0 15.5 15.9 +StruXGPT (ours) 22.3 37.0 25.4 14.7 8.2 2.5 17.5 18.2 ChatGLM3-6B-32k 43.3 51.7 54.4 44.9 40.4 2.0 99.0 47.9 +StruXGPT (ours) 44.6 52.1 57.2 47.6 40.1 4.0 99.5 49.3 Evaluated models. Following [3], we evaluate three representative large language models on long context comprehension ability: LLaMA2-7B-4k [ 53], Qwen-7B-8k [2], and ChatGLM3-6B- 32k [67], and extend the larger LLaMA2-13B-4k [53] model. The four LLMs have a relatively similar parameter capacity with different model architectures and window sizes. All the models to examine are pre-trained chat models, and we just employ our StruXGPT to structurize the input contexts to enhance those LLMs’ cognition capabilities. To ensure reproducibility and reduce uncertainty, greedy search is adopted during LLM’s decoding process when generating responses. The accuracy between models’ responses and ground-truth answers is measured by ROUGE-L and F1-score. Experimental results. Tab. 1 suggests structurized contexts bring consistent improvements on almost all 3 tasks and 7 subsets across the model architectures and window sizes. Specifically, structurization leads to relatively greater improvement for the MultiDoc-QA subtask (with a 3% performance gain on average), revealing the potential promotion of LLMs’ multi-hop reasoning abilities. Despite the negligible decline on the Musique [54] subset (see Appendix B.2 for analysis), the advanced ChatGLM3-6B-32k is also boosted, showing structurization’s efficacy for powerful models. Comparison with other baselines. To further evaluate our structurization augmentation, we compare our method against the typical summarization-based methods that also employ LLMs for context augmentation. The results are presented in Appendix B.3, which further demonstrates our superiority in highlighting the knowledge structure without loss of key information. A: Domain experts with legal training. Q: Who were the experts used for annotation? A: Unanswerable. Vanilla Context Structurized Context This passage talks about the PrivacyQA dataset and its characteristics: 1. Introduction to Privacy Policies. … 2. … 3. PrivacyQA Dataset. … 4. Data Collection Methodology. PrivacyQA comprises 35 mobile application  privacy policies collected from the Google  Play Store. Seven experts with legal training  provide answers to Turker questions, which  are then meta-annotated for relevance,  subjectivity, and legal soundness… 5. … Privacy policies are the documents which  disclose the ways in which a company…  … … … … To identify legally sound answers, we  recruit seven experts with legal training to  construct answers to Turker questions.  Experts identify relevant evidence within  the privacy policy, as well as provide meta- annotation on the question's relevance,  subjectivity, OPP-115 category BIBREF49,  and how likely any privacy policy is… Figure 5: Attention maps on vanilla and structur- ized contexts for the same LLaMA2-7B. The sam- ple comes from the QAsper subset. Investigating structurization from the atten- tion perspective. Fig. 5 reveals how structuriza- tion can aid LLM’s cognition from the attention perspective. In particular, we compare the at- tention maps for the same tested LLaMA2-7B model with different contexts as input. At the position of the model’s first token prediction, we average the attention maps across the 32 atten- tion heads for each layer of LLaMA’s last 16 layers [72], and visualize the attention scores in Fig. 5. Specifically, when handling vanilla con- texts, LLaMA2-7B loses its focus on the target information of the experts. On the contrary, the structurized context clearly presents the content structure of the introduced PrivacyQA dataset, and LLaMA2-7B immediately grasps the target aspect and its detailed descriptions of experts with legal training. In this way, LLM’s cognition capability is successfully enhanced via context structurization. 64.2 Application on Exhaustive Hallucination Evaluation Hallucination has raised wide attention in the community [70, 56]. In general, evaluating hallucina- tions involves verifying atomic claims against supportive materials (e.g., Wikipedia passages [66, 42]), yet even advanced GPT-3.5-Turbo and GPT-4 cannot always accurately make the judge, as LLMs- evaluators often struggle to extract relevant information due to the complexity of passage contexts. We introduce how to improve LLM evaluators’ assessing ability by context structurization below. Table 2: Hallucination Evaluation on AttrScore. Evaluator Attr. Contra. Extra. Average GPT-4 87.3 45.0 89.6 74.0 GPT-3.5-Turbo 61.2 20.6 53.3 45.0 Alpaca-13B 50.6 6.1 19.3 25.3 Alpaca-7B 50.7 8.6 3.6 21.0 LLaMA2-7B 51.5 9.1 20.1 26.9 +StruXGPT (ours) 54.5 15.0 30.4 33.3 LLaMA2-70B 70.9 31.1 74.1 58.7 +StruXGPT (ours) 75.4 35.6 78.1 63.0 GPT-3.5-1106 72.0 30.4 71.7 58.0 +StruXGPT (ours) 77.1 31.8 77.4 62.1 GPT-3.5-1106 + CoT 76.4 35.3 74.4 62.0 +StruXGPT (ours) 78.9 42.9 74.5 65.4 Dataset setup. AttrScore [ 66] and FactScore [42] datasets are adopted for evaluation. We take the AttrEval- GenSearch test set with 245 exam- ples from AttrScore, where each ex- ample comprises a statement and its reference passage, and is annotated by Attributable (abbreviated as Attr.), Contradictory (abbreviated as Con- tra.), and Extrapolatory (abbreviated as Extra.). FactScore collected 4,726 atomic claims/statements of people bi- ographies generated by InstructGPT (abbreviated as InstGPT), 5,426 by ChatGPT, and 5,888 by PerplexityAI (abbreviated as PPLAI). For each in- put sample, we leverage StruXGPT to structurize the reference to identify its main aspects and detailed descriptions. The numerically ordered structure is preserved, as displayed in Fig. A3, since we explicitly ask the evaluator to check the information along the structure for judgment. Evaluated models. We mainly investigate the open-sourced LLaMA2-7B and LLaMA2-70B models as the LLM-evaluator, and also explore the integration with the close-source GPT-3.5-Turbo-1106 [6] via API access. The main results are presented in Tab. 2. We also report the results with Qwen models on AttrScore and FactScore and the incorporation to more powerful GPT-4 in Appendix B.4. Experimental results. According to Tab. 2, our structurization brings significant enhancements to both LLaMA2-7B and 70B models (for 6.4% and 4.3% on average, respectively). And the powerful GPT-3.5 model also gains 4.1% (from 58.0% to 62.1%). Furthermore, we incorporate the Chain-of- Thought (CoT) technique into the prompt template to clarify the evaluation steps (such as Carefully read the claim and double-check the answer.) (denoted as “GPT-3.5-1106 + CoT”). After that, the GPT-3.5 model immediately obtains an improvement of 4.0% (from 58.0% to 62.0%). Consequently, on top of the advanced CoT prompt, our method further enhances the model to a higher accuracy of 65.4% on average, demonstrating our method’s compatibility with advanced prompting techniques. 4.3 Application on Passage-level Dense Retrieval Retrieval-augmented generation (RAG) has been empirically validated to significantly bolster LLM’s domain knowledge [29, 62], where precise document retrieval plays a vital role. We now investigate how structurization can facilitate dense retrieval for BERT-like masking language models. Dataset setup. BEIR dataset [ 52] is a popular benchmark for evaluating dense retrievers’ zero- shot effectiveness [39, 33], where retrievers are trained on MS MARCO’s passage-retrieval training split [44] while directly tested on the BEIR benchmark without finetuning. We focus on our evaluation of the 5 subsets from BEIR i.e., NFCorpus, FiQA, ArguAna, SciDocs, and SciFact. Evaluated models. BERT [13], SimLM [ 57], and coCondenser [ 16] are chosen for evaluation, since they achieve state-of-the-art performance on MS MARCO’s development split. To convert the structurized passages from our StruXGPT into natural languages, we eliminate the numerical indicators (such as “1.”, “1.1”, etc.) and attach the description statements in the third layer with their aspects. Fig. A3 presents an example. We only structurize passages to enhance retrievers’ cognition, and the queries remain unchanged. Following the literature, nDCG@10 results are reported in Tab. 3. 7Table 3: Performance on BEIR subsets. Retrievers are trained with MS MARCO corpus and directly evaluated on BEIR without fine-tuning. Retriever NFCorpus FiQA ArguAna SciDocs SciFact Average BERT 24.4 23.7 36.2 11.4 50.8 29.3 +StruXGPT (ours) 24.4 24.9 40.0 11.4 50.7 30.3 SimLM 22.2 17.3 34.2 11.7 48.2 26.7 +StruXGPT (ours) 22.9 19.8 34.6 11.7 52.7 28.3 coCondenser 28.2 22.8 40.5 12.8 55.6 32.0 +StruXGPT (ours) 28.8 23.5 43.4 13.1 56.8 33.1 Experimental results. Structurization boosts all three retrievers on most subsets, yielding a maximum performance improvement of 4.5% on SciFact for SimLM. The results suggest that structurization not only augments decoder-only generative LLMs with explosive parameters (at least 7B), but also benefits encoder-decoder masked language models with constrained parameters (around 110M). It implies that the patterning of linguistic and semantic structurization may be a fundamental mechanism for enhancing language models, transcending distinctions in their architectural design and scale. 4.4 Evaluation of the Structurization Approach Itself In this section, we assess various structurization methods through exhaustive experiments on five approaches, including prompting Qwen-max with few-shot exemplars (serves as our teacher model), few-shot Qwen-7B and LLaMA2-7B pre-trained chat models, and our fine-tuned Qwen-7B and LLaMA2-7B (student) models. As introduced in Sec. 3.1, we use 200 validation cases and have the five models generate 1,000 structurized outputs for analysis. A good structurization should effectively deconstruct the vanilla input text to clearly identify its knowledge structure, so as to facilitate LLM’s cognition. The resulting content should be faithful to the original texts, neither dismiss the factual information nor fabricate statements or opinions that do not exist. To this end, we revise four evaluation metrics to investigate the efficacy of different structurization approaches, and the results are displayed in Tab. 4. Table 4: Comprehensive comparison on structurization approaches. Approach Model LexicalEval HumanEval AppEval SemEval recall? precision? completeness ↑ factuality↑ anti-hallu↑ ∆↑ bertscore↑ Few-shot Qwen-max 0.63 0.68 4.58 4.49 4.57 +3.3 0.31 Qwen-7B 0.56 0.67 3.77 3.67 3.96 -0.1 0.22 LLaMA2-7B 0.61 0.72 4.09 3.98 4.12 +0.3 0.24 Fine-tuned StruXGPT-7B-Q 0.63 0.67 4.41 4.36 4.48 +3.6 0.31 StruXGPT-7B-L 0.61 0.66 4.37 4.38 4.36 +2.8 0.30 Lexical evaluation (LexicalEval). We first leverage the widely-used ROUGE-L [ 34, 35, 25] to assess recall and precision between structured content and original text. However, lexical metrics from the methods, ranging 0.6 to 0.7, inadequately reflect structurization quality where LLMs will paraphrase the words but lexical scores miss the semantic consistency. For instance, a statement pair “They adopt ROUGE” and “ROUGE is adopted” only receives a 0.33 f1-score for ROUGE-L. Therefore, a crucial human evaluation is developed to obtain a trustworthy conclusion. Human evaluation (HumanEval). We recruited 17 well-trained natural language annotators from the PAI-iTAG platform5 to evaluate the structurization quality on a 0-5 scale across three dimensions: completeness (ensuring no loss of information from the original text), factuality (accurate three-layer deconstruction), and anti-hallucination (avoiding fabricated content). As annotating structurization 5An open platform at https://www.aliyun.com/product/bigdata/learn/itag 8only involves linguistic and syntactic level judgments, annotators do not need professional expertise to check the information of a given text itself. The detailed evaluation criteria are displayed in Appendix A.3, and we report the labeling results in Tab. 4. The commercial Qwen-max shows a promising instruction-following and in-context learning ability, generally scoring 4.5 at the three dimensions. However, the pre-trained 7B models from Qwen and LLaMA2 immediately decline the scores to below 4.0, as they struggle to understand the instruction to build the three-layer structure and tend to hallucinate responses due to the limited model capacity. More structurization examples can be found in Appendix E.2. Notably, the fine-tuned StruXGPT- 7B-Q(wen) and StruXGPT-7B-L(LaMA2) both obtain a 4.35 - 4.45 score on average. They inherit 97% of structurization capability from the Qwen-max teacher model, evidencing the effectiveness of training a specialized 7B-parameter model for structurization to aid in efficiency and privacy. Evaluation with downstream application (AppEval). Since our main motivation is to utilize structurization to enhance LLM’s cognition, a further evaluation is conducted to investigate those different structurization methods. Specifically, we compare how much improvement ( ∆) those methods can bring to downstream natural language processing applications. On the Qasper subset [12] from LongBench [3], we instruct an independent LLaMA2-7B chat model for reading comprehension with long-context structurized by different approaches. LLaMA2-7B receives a 19.6 F1-score for QA accuracy when taking the vanilla context as input, which serves as the baseline performance. The evaluation results are displayed in Tab. 4, which are consistent with the human evaluation presented above. In particular, the few-shot Qwen-max achieves an over 3% improvement in answer quality, while the pre-trained 7B-parameter chat models fail to generate validated structurizations. Meanwhile, our fine-tuned StruXGPT-7B-LLaMA and StruXGPT-7B-Qwen models bring com- parable enhancements against the few-shot Qwen-max, emphasizing the efficacy of distilling the structurization ability from giant teacher models to a more responsive and affordable student model. Evaluation with semantic embedding (SemEval). At last, we explore the structurization quality evaluation in the semantic embedding perspective, as a supplementary to lexical evaluation. Follow- ing Zhang et al. [68], we calculate the semantic similarity between original and structurized contents with the embedding similarities, and the results in Tab. 4 show consistent measure against HumanEval and AppEval with a much lower cost. Hence, BERTScore [68] can be further leveraged as an effective and efficient quality-assessment tool for training-data filtering and structurization quality evaluation to derive a better StruXGPT model. Sec. 4.6 presents some preliminary investigations. Through the comprehensive evaluation of three protocols, we demonstrate the feasibility and efficacy of training a specialized StruXGPT. It is more resource-friendly to deploy for efficiency and privacy, meanwhile inheriting 97% of the ability from giant teacher models to perform structurization. 4.5 Ablation Studies on StruXGPT’s Establishment This section studies two major factors of training a StruXGPT model: data quality and model capacity. Using two few-shot examples is sufficient to collect high-quality training data. In this work, we choose 2 in-context examples to prompt commercial LLMs (as a teacher) to generate data pairs of raw/structurized texts to train our StruXGPT-7B model (as a student). We think it is enough for teacher models to understand the structurization process and generate valid training samples, as the 2 examples respectively describe the 2 most common types of real-world text ( i.e., with/without existing indicators like “1.”, “2.”, etc), which is displayed in Appendix E.2. To further verify it, we investigate the number of in-context examples with two evaluation protocols (as in Tab. 4): AppEval (an improvement on Qasper subset with context structurization) and BERTScore (semantic similarity with raw and structurized texts in the validation set). We also report the error rate when parsing structurization results from the teacher model’s outputs (denoted as “FormatError”). According to Tab. 5, 1-shot is apparently insufficient to illustrate structurization, while 2- and 3-shot achieve comparable structurization quality under AppEval and BERTScore. Notably, 3-shot receives a 2% lower FormatError than 2-shot, in trade for the increased inference cost (because of increased few-shot samples). We argue that the 2% gap (around 400 samples) does not make a difference for the final StruXGPT training, which can be verified in Appendix A.2. Therefore, we recommend users to apply 3- or even more shots when prompting teacher LLMs if available, otherwise 2-shot is also a good choice to balance the inference cost and structurization quality. 9Table 5: Number of few-shot examples. nShot AppEval BERTScore FormatError 1-shot +1.8 0.282 25.4% 2-shot +3.2 0.308 7.4% 3-shot +3.3 0.302 5.5% Table 6: Parameter capacity of StruXGPT. StruXGPT AppEval BERTScore FormatError Qwen-1.8B +2.7 0.299 5.0% Qwen-7B +3.6 0.313 0.0% Qwen-14B +3.8 0.323 0.0% StruXGPT-7B balances parameter capacity and structurization quality. As Qwen [2] provides a series of models varying sizes, in Tab. 6, we implemented StruXGPT on Qwen-1.8B/7B/14B respectively to investigate the relationship between model capacity and structurization quality. Compared with the 7B model capacity, the smaller 1.8B model, despite its positive enhancement on downstream applications, shows slight inferiority in both AppEval (+2.7 v.s. +3.6) and BERTScore (0.299 v.s. 0.313), and presents 5% error rate when parsing structurization results. On the other hand, the 14B model brings further improvement to BERTScore, (the structurization content is relatively more faithful to original texts), but the boost on AppEval is insignificant. Hence, the 7B model is a good trade-off between model capacity (training/inference efficiency) and structurization quality. 4.6 Utilization of Structurization Quality Assessment As the structurization quality simultaneously influences StruXGPT’s training performance and application improvements, this section investigates the quality assessment tool BERTScore (as discussed in Sec. 4.2) on StruXGPT’s training and inference stages, respectively. On one hand, as the training data quality determines StruXGPT’s upper bound, we statistic the BERTScore of the 22K training entries, and around 94.45% raw/structurized text pairs present positive scores (normalized by the baseline score of 0.83, and a positive BERTScore presents a benign similarity), demonstrating the high quality of our training data. Consequently, we eliminated around 5% of data with negative scores and trained another StruXGPT model, and the results in Tab. 7 indicate this part of data does not affect the final performance. According to Appendix A.2, data quantity may play a vital role in further improving our StruXGPT. Table 7: Training data filtering. Training Data AppEval BERTScore vanilla +3.6 0.313 filtered +3.4 0.316 Table 8: Inference results filtering. Context Declined Ratio Overall Enhance vanilla 3.5% +3.6 filtered 3.0% +3.7 On the other hand, since poor structurization results can lead to suboptimal application performance, we statistic the enhancement variance in the Qasper subset from LongBench [ 3] in Tab. 8 for structurized context inputs. Our method merely causes degradation on 3.5% of samples (with relatively lower structurization quality), but ultimately receives a +3.6 improvement overall test samples. Furthermore, we filter out the structurization results with a low BERTScore (e.g., <0.05) and take back the original context as input. In this way, the degradation can be alleviated (from 3.5% to 3.0%), further improving the final enhancement to +3.7. The lower bound of our StruXGPT can thus be ensured. 5 Conclusion and Discussion This paper presents a novel concept of context structurization to enhance LLM’s cognition capability. Extensive evaluations of various representative NLP tasks reveal the consistent enhancement across language model’s architectural designs and capacity scales. We demonstrate the feasibility of distilling the structurization ability from giant commercial LLMs into a responsive and private StruXGPT- 7B model, addressing the practicality problem. The limitation and future work are discussed in Appendix C. We hope this paper can bring new insights into how to build a more powerful and reliable language model with human cognition and intelligence. 10Acknowledgments and Disclosure of Funding This work was supported in part by the Fundamental Research Funds for the Central Universities, in part by Alibaba Cloud through the Research Intern Program, and in part by Zhejiang Provincial Natural Science Foundation of China under Grant No. LDT23F01013F01. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Asma Ben Abacha and Dina Demner-Fushman. A question-entailment approach to question answering. BMC bioinformatics, 20(1):1–23, 2019. [5] S.M. Breedlove, N.V . Watson, and M.R. Rosenzweig.Biological Psychology: An Introduction to Behav- ioral, Cognitive, and Clinical Neuroscience. Sinauer Associates, Incorporated Publishers, 2010. ISBN 9780878933242. URL https://books.google.com/books?id=VYRFAQAAIAAJ. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [7] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehen- sion. In International Conference on Learning Representations, 2023. [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. [10] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. arXiv preprint arXiv:1804.05685, 2018. [11] Malcolm Coulthard and CN Condlin. An introduction to discourse analysis. Routledge, 2014. [12] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [14] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture- of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022. [15] John Duncan. The structure of cognition: Attentional episodes in mind and brain. Neuron, 80(1):35– 50, 2013. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2013.09.015. URL https://www. sciencedirect.com/science/article/pii/S0896627313008465. [16] Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540, 2021. [17] M.S. Gazzaniga, R.B. Ivry, and G.R. Mangun. Cognitive Neuroscience: The Biology of the Mind (5th edition). W.W. Norton, 2018. ISBN 9780393603170. URL https://books.google.com/books?id= ZbtotwEACAAJ. 11[18] James Paul Gee. An introduction to discourse analysis: Theory and method. routledge, 2014. [19] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2023. [20] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. [22] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [24] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645, 2023. [25] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [27] Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. arXiv preprint arXiv:1805.01052, 2018. [28] David R Krathwohl. A revision of bloom’s taxonomy: An overview. Theory into practice, 41(4):212–218, 2002. [29] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020. [30] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, E. Gonzalez Joseph, Stoica Ion, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. URL https://lmsys.org/blog/2023-06-29-longchat . [31] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [32] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13067–13075, 2023. [33] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, and Xiangnan He. Llara: Aligning large language models with sequential recommenders. arXiv preprint arXiv:2312.02445, 2023. [34] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81, 2004. [35] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [36] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. [37] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint arXiv:2305.13169, 2023. 12[38] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. arXiv e-prints, pages arXiv–2308, 2023. [39] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023. [40] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pages 1941–1942, 2018. [41] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023. [42] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. [43] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. [44] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. choice, 2640:660, 2016. [45] Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding. arXiv preprint arXiv:2307.15337, 2023. [46] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. [47] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [48] Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 2387–2413, Singapore, December 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.findings-emnlp.157. [49] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003–13051, 2023. [50] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. [51] P. Thagard. Mind: Introduction to Cognitive Science . Bradford Bks. MIT Press, 1996. ISBN 9780262201063. URL https://books.google.com/books?id=-SQJngEACAAJ. [52] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heteroge- nous benchmark for zero-shot evaluation of information retrieval models.arXiv preprint arXiv:2104.08663, 2021. [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [54] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022. [55] Alan M Turing. Computing machinery and intelligence. Springer, 2009. [56] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. arXiv preprint arXiv:2310.07521, 2023. 13[57] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. arXiv preprint arXiv:2207.02578, 2022. [58] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. [60] Yilin Wen, Zifeng Wang, and Jimeng Sun. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729, 2023. [61] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. [62] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023. [63] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. [64] Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. Exploring the limits of chatgpt for query or aspect-based text summarization. arXiv preprint arXiv:2302.08081, 2023. [65] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [66] Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311, 2023. [67] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. [68] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert, 2020. [69] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:39–57, 2024. [70] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023. [71] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be- ichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [72] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. 14A Implementation Details of Structurization A.1 Implementation Details The data to train our StruXGPT is carefully curated from two main aspects. First, to ensure diversity, we randomly sample 15,000 passages from Wikipedia across various domains, whose token length ranges from 1K to 3K. Then, we also supply round 15,000 statement passages from the domain- specific model- and human-generated content, including the filtered CAMEL-AI [31], FiQA [40], and MedQuad [4] datasets. With Qwen-max API, we collect the structurized results and eliminate the failed cases (usually due to internet inaccessibility and unexpected output format), resulting in 22,547 pieces of training data in total to derive our StruXGPT, and 200 test samples for evaluation. StruXGPT is built upon LLaMA2- or Qwen-7B-chat models, which had been aligned to humans with promising instruction-following capabilities with their prior knowledge. The 22,547 pieces of input-output pairs are utilized to distill the structurization ability to StruXGPT via supervised fine-tuning (SFT). Specifically, StruXGPT is trained with a constant learning rate of 5 × 10−6 for LLaMA and 1 × 10−5 for Qwen for 1 epoch. The batch size is 128, and other hyper-parameters follow the default settings from Touvron et al.[53] and Bai et al. [2]. The training is resource-friendly, which can be done on 8 NVIDIA V100 (16G) GPUs for 3.5 hours. For all the inference experiments, we leverage 1-2 NVIDIA A100-80G GPUs for model deployment. A.2 Additional Ablation Studies on Training Data To further investigate the training sample size and its impact, we have conducted additional ablation studies on StruXGPT’s training process, and report the question-answering capability enhancement for the same LLaMA2-7B-Chat model on LongBench’s Qasper subset, as introduced in Sec. 4.4. Table A1: Ablation on training samples. Training Samples Enhancement 2K -0.6 5K -1.3 10K +2.6 15K +3.3 22K +3.6 Our results in Tab. A1 reveal an interesting pattern: as the training sample increases, StruXGPT first causes a decline in the LLaMA2-7B model’s QA performance, and then brings stable enhancement with larger training data (over 10K samples). We attribute this to the fact that we train StruXGPT for only one epoch in each experiment to prevent overfitting. Consequently, models trained with too few samples often exhibit subpar structurization quality, commonly resulting in information loss or hallucination, which adversely affects downstream performance. Specifically, for the model trained with 2K samples, the generated outputs frequently contain incorrect formatting and struggle to parse the three-tier structure effectively. In these cases, we resort to using the original text as a fallback input for downstream LLMs, which accounts for the somewhat lower performance drop (-0.6%) compared to the model trained on 5K samples (-1.3%). A.3 Human Evaluation Criteria. A good structurization should effectively deconstruct the vanilla input text to clearly identify its knowledge structure, and the structurized results should be faithful to the original texts, neither dismissing the factual information nor fabricating statements or opinions that do not exist. Therefore, we devise three dimensions to evaluate the structurization quality, and each of the dimensions can be scored from 0 to 5 (higher is better). The detailed criteria are displayed in Fig. A1, and the annotating screenshot is provided in Fig. A2. Instruction. The full annotation instruction is displayed below: Assess the quality of structured outputs produced by different sources, and evaluate whether the structured output is sufficiently “good” by using a score ranging from 0 to 5. To clarify what constitutes a “good” structured result, please consider and score the following aspects, with each aspect also scoring from 0 to 5. Scoring Criteria: Please see Fig. A1. Payment. The payment for annotators is 2,000¥ in total, which is higher than the minimum wage in our country. 15Anti-HallucinationFactualityCompletenessScore The content is consistent to the  original text, with no  fabrications/hallucinations. The structure is clear and standard  with appropriate granularity as  expected. The core content information is  largely retained with the omission of  2-3 descriptive clauses or adverbials  permitted. 5 The overall content is faithful, but it  includes a few expanded sentences  not present in the original text. The hierarchical structure is retained  but with minor formatting issues,  such as extraneous spaces or line  breaks; granularity is generally  suitable, with occasional instances of  a subsection containing only one  description or exceeding seven  descriptions. There is an omission of 20% of  sentences or the absence of  introductory and concluding  overview/summary paragraphs.  4 Considerable content is fabricated,  such as offering additional large  sections of ``suggestion'' text. Poor structure: lack of  aspect/description numbering,  excessively long aspect name, or  overly short description sentences  (lists of words); Inappropriate  granularity: all sentences attached to  one aspect, or dividing each sentence  into a separate aspect. There exist considerable omissions of  sentences or paragraphs are present,  or each paragraph lacks 40% of its  sentences. 3 Severe repetition or meaningless  phrases are present, significantly  reducing the quality of the result. The basic structure is preserved, but  there is a frequent lack of numbering  and non-compliance with  hierarchical organization. Extensive information loss is evident,  with more than half of the original  sentences removed. 2 Essentially nonsensical, with only a  few words or phrases retained from  the original text. Chaotic structure: content is  organized without any adherence to  hierarchical structure. The majority of information is  omitted, with only a few key words  or sentences preserved. 1 If the content is empty or completely hallucination, give it a score of 0.0 Figure A1: Detailed descriptions for human evaluation criteria on structurization quality. Figure A2: Screenshot for human evaluation. 16B Details for Structurization Applications B.1 Structurization Examples Fig. A3 provides several examples of how to structurize the long-form context for downstream applications, including reading comprehension in Sec. 4.1, hallucination detection in Sec. 4.2, and passage retrieval in Sec. 4.3. Structurized Context This passage talks about the PrivacyQA dataset and its characteristics: 1. Introduction to Privacy Policies. … 2. Privacy Policy Misuse and … 3. PrivacyQA Dataset. … 4. Data Collection Methodology. PrivacyQA comprises 35 mobile application privacy  policies collected from the Google Play Store.  Seven experts with legal training provide  answers to Turker questions… Q: Who were the experts used for annotation? Instruction A: Domain experts with legal training. Response The life and career of director George Cukor: 1. Early life and background 1.1 Cukor was born on the Lower East Side of  Manhattan, the younger child and only son of… 1.2 His parents selected his middle name in honor of  Spanish–American War hero George… 2. Career milestones 2.1 Cukor won the Academy Award for Best Director  for \"My Fair Lady\" (1964), which was … 2.2 He continued to work into the 1980s…. Structurized Context Judge: Cukor won the award for Best Director. True or  False? Instruction A: True. George Cukor won the Academy Award for  Best Director for \"My Fair Lady\" (1964). Response DNA methylation in human PBMCs.  **Role of DNA methylation**: DNA  methylation plays a significant role in human  health and disease processes.  **Technological advancements**: Recent  technological advancements have significantly  improved the accuracy and.….  **Whole-genome bisulfite sequencing**:  Using the whole-genome bisulfite sequencing at  24.7-fold coverage (12.3-fold per strand)… Structurized Context Instruction Query: What is the role of DNA methylation  in human health and diseases? A: Matched. Response Reading Comprehension Hallucination Detection Document Retrieval Figure A3: Examples to leverage the structurized results for downstream aookucatuibs B.2 Details for Context-based Question-Answering on LongBench Detailed dataset description. To focus on the investigation of context structurization, we choose 7 subsets from LongBench across single-document QA, multi-document QA, and synthetic QA tasks in English, and the remaining Chinese subsets or code-orientated tasks are eliminated: • Single-Doc QA. For single-document QA, we take two subsets from LongBench: (1) Qasper [12], featured by question-answering over NLP technical papers and annotated by NLP practitioners; (2) MultiFieldQA, manually curated from multiple data sources and anno- tated by Ph.D. students. MultiFieldQA contains 150 Context-Question-Answer triplets to test, and Qasper and other adopted subsets include 200 pieces of test samples respectively. • Multi-Doc QA. Multi-document QA requires LLMs to extract and combine information from multiple documents to derive the answer, which is generally more challenging than single-doc QA. We take three multi-hop QA datasets: (1) HotpotQA [65], containing 2-hop questions written by native speakers given two related paragraphs; (2) 2WikiMultihopQA [22], involving up to 5-hop questions synthesized through manually designed templates on Wikipedia passages; and (3) MuSiQue [54], carefully composed with up to 4-hop reasoning on an increased number of supporting and distracting context evidence. • Synthetic QA. We employ two extra synthetic tasks to test LLM’s long-context handling ability on specific scenarios and patterns: (1) PassageCount, requiring models to count the number of unique passages from a shuffled passages pool; and (2) PassageRetrieval, randomly choosing one of 30 passages to obtain its summarization with GPT-3.5-Turbo, and asking tested LLMs to determine the original passage with which the summarization is crafted. Performance analysis on MuSiQue. On the LongBench dataset [3], ChatGLM-3-32K [67] receives a slight decline (around 0.1%) on the MuSiQue subset [ 54] when taking structurized contexts as input. The main reason is that MuSiQue is too complicated. It requires up to 4-hop reasoning on manually paraphrased questions on a large amount of supporting and distracting context evidence. For ChatGLM-3-32K that has already achieved a considerable cognition ability, context structurization in the inference stage is temporally unable to bring significant advances in such a complex scenario. Next, we will delve into developing methodologies for training LLMs to capture the intrinsic structure of context, unlocking structurization’s potential in enhancing LLMs on more complicated NLP tasks. 17B.3 Comparison with Aspect- and Query-Based Summarization Aspect-Based Summarization (ABS) and Query-Based Summarization (QBS) are classical context augmentation methods in the NLP literature, which aims to gradually extract important information from lengthy text data based on pre-defined aspect list or user-input queries. The summary that contains key information will be integrated into context inputs to augment tested LLMs to generate reliable responses, rather than lost in irrelevant sentences in the original lengthy inputs. We take ABS and QBS as the comparative baseline to further evaluate the efficacy of our proposed structurization augmentation strategy. We investigate ABS and QBS on five document-QA subsets in LongBench [3]. Specifically, as the pre-defined aspect list is unavailable for each passage in LongBench, we turn the traditional ABS to input-agnostic paragraph-based summarization to lengthy paragraphs, where the prompt template is: Summarize the following text with no more than three sentences. Passage: {text}; Summary: . For QBS, the prompt template focuses on specific queries: Summarize the following text to answer the query with no more than three sentences. Query: {query}; Passage: {text}; Summary: . The {text} and {query} are placeholders, and the output summaries are respectively concatenated to form the augmented passages as inputs. We report the performance variance of the same test model (i.e., LLaMA2-7B-Chat [53]) when taking vanilla, summarized, and our structurized passages as inputs to answer the same given questions, as displayed below (due to the space limitation, we merely report the averaged enhancement on the five subsets). Table A2: Comparison with ABS and QBS on subsets of LongBench. DataAug Qasper MultifieldQA HotpotQA 2WikiMQA Misque Average - 19.5 34.6 30.4 27.3 10.7 24.5 ABS 15.6 24.9 29.1 27.7 8.5 21.2 QBS 21.4 30.1 31.2 27.5 13.1 24.7 Ours 23.1 35.9 32.7 29.9 13.4 27.0 Based on Tab. A2, we observed that ABS, without guidance from pre-defined aspects lists or user- input queries, failed to preserve critical information, and led to performance declines in four out of the five subsets. Conversely, QBS, with user query guidance, achieved improvements in the Qasper, HotpotQA, and Musique subsets. However, substantial information loss during the summarization process resulted in decreased performance on the MultiFieldQA subset. In contrast, our structured approach delivered consistent improvements across all subsets, demonstrating its efficacy. In addition, ABS and QBS are applicable for passage-based question-answering tasks but do not extend well to other tasks such as hallucination assessment, where every piece of information counts and should not be summarized. Our approach is task-agnostic, highlighting the generalizability. B.4 Additional Experiments for Hallucination Evaluation Qwen models as the LLM evaluators. In Sec. 4.2 in the manuscript, we mainly study structurization with LLaMA2 and GPT-3.5-Turbo on the AttrScore benchmark. For a thorough comparison, this section provides the investigation of employing another series of models, i.e., Qwen-7B and Qwen- 72B [2], on AttrScore and FactScore datasets to further demonstrate our method’s efficacy. Results are presented in Tab. A3 and Tab. A4, respectively. For the 7B- and 72B-parameter Qwen evaluators, structurization can immediately improve judgment accuracy on both AttrScore and FactScore datasets. The smaller Qwen-7B model receives a more significant enhancement, along with a nearly 10% improvement in all metrics (except theAttr. criteria in Tab. A3) on the two datasets. It indicates smaller LLMs may be more reliant on structurization to enhance their cognitive and information-seeking capabilities than large-scale LLMs. Simultaneously, the larger Qwen-72B evaluator also witnesses a considerable improvement with a 3%-4% increase. In particular, based on the structurized references, Qwen-72B achieves a competitive evaluation performance against GPT-3.5-Turbo on the FactScore benchmark, and even slightly outperforms GPT-4 on the Contra. criteria in AttrScore. Those results substantiate the effectiveness of leveraging structurization to further enhance powerful language models with considerable size capacities. 18Table A3: Results on AttrScore evaluation. Method Attr. Contra. Extra. GPT-4 87.3 45.0 89.6 GPT-3.5-Turbo 61.2 20.6 53.3 Alpaca-13B 50.6 6.1 19.3 Alpaca-7B 50.7 8.6 3.6 Qwen-7B 60.9 11.6 34.2 +Ours 61.9 25.9 44.4 Qwen-72B 75.8 37.7 77.6 +Ours 77.8 47.5 80.2 Table A4: Results on FactScore evaluation. Method InstGPT ChatGPT PPLAI GPT-4 - - - GPT-3.5-Turbo 87.5 80.2 65.8 LLaMA-65B 54.6 42.1 36.1 InstLLaMA-7B 80.1 67.1 55.1 Qwen-7B 68.5 50.8 40.8 +Ours 79.2 64.8 46.0 Qwen-72B 86.9 75.2 60.5 +Ours 89.0 79.4 61.8 Incorporation with GPT-3.5/4 models and CoT techniques. In Sec. 4.2, we the popular chain-of- thought (CoT) technique for prompt augmentation on the advanced GPT-3.5-Turbo model on the AttrScore [66] benchmark, where Tab. A5 is a simply copy of the experimental results. Our method presents comparable performance against CoT, and achieves better enhancement after integrating with CoT, illustrating the compatibility and extensibility to more advanced strategies. Consequently, Tab. A6 further presents the incorporation of our StruXGPT with the more powerful GPT-4 model and the CoT technique, showing consistent benefits for powerful GPT-3.5 and GPT-4 models, either with or without the CoT strategy. The experiments and discussions further validate the effectiveness of our StruXGPT approach. Table A5: Integration with GPT-3.5 and CoT. Method Attr. Contra. Extra. Avg. - 72.0 30.4 71.7 58.0 +Ours 77.1 31.8 77.4 62.1 +CoT 76.4 35.3 74.4 62.0 +CoT+Ours 78.9 42.9 74.5 65.4 Table A6: Integration with GPT-4 and CoT. Method Attr. Contra. Extra. Avg. - 86.2 43.3 88.3 72.6 +Ours 87.6 48.3 89.7 75.2 +CoT 88.8 48.9 89.7 75.8 +CoT+Ours 88.5 52.8 90.3 77.2 B.5 Comparison with Other Augmentation Methods Besides the summarization-based and prompting-based augmentation methods (in Appendix B.5 and Appendix B.4 respectively), here we also compare our structurization approach with AdaptLLM [7], which developed several domain-specific LLMs (in BioMedicine, Finance, and Low) via the proposed reading-comprehension training technique. The experimental results are presented in Tab. A7. Table A7: Comparison with AdaptLLM on various domain benchmarks. Domain Subset Metric Baseline AdaptLLM Ours Medicine PubMedQA Acc 59.6 63.3 63.0 Finance ConvFinQA EM 29.2 41.5 36.5 Law SCOTUS mic-F1/mac-F1 28.3/10.8 30.0/ 17.8 30.6 /15.6 General BoolQ Acc 55.7 53.9 58.2 Accordingly, our method can also boost the Llama-7b baseline for 3%-7% without training, while AdaptLLM requires costly continual training of the baseline model on each domain corpus. In particu- lar, for the general reading comprehension task, AdaptLLM-Fin does not introduce significant boosts, while AdaptLLM-Bio/Law even cause performance drops, which is mainly because AdaptLLM’s domain-adaptation tuning will harm the general capability more or less. In contrast, our method does not alter the baseline model, but only structurizes the input context to enhance LLM’s cognition 19ability on downstream tasks, showing stable and consistent improvements (e.g., a 2.5% increase on BoolQ). Although our final performance is slightly inferior to the domain-specialized AdaptLLM, our generalizability emphasizes the contribution of our work, as we bring consistent enhancement across downstream domains and cause no degradation on general tasks. C Limitation and Future Work Training-time aggregation. This paper mainly investigates structurization during LLM’s inference stage. Despite the universal enhancement across language models and NLP tasks, structurization has not yet fully tapped into its potential. Specifically, developing methodologies for training LLMs to capture the intrinsic structure of context and extrinsic correlations among training corpus presents an unresolved challenge in the field. We will delve into this problem in our future work. Inference efficiency. We acknowledge that the introduced inference cost is a common limitation for the methods adopting LLMs for data augmentation, which depends on computing resources and the optimization techniques employed. Here we conduct extra experiments with the competitive augmentation methods (i.e., ABS and QBS in Appendix B.3) for an in-depth comparison on the Long- Bench [3] dataset for question-answering evaluation. Besides, we also investigate the AttrScore [66] dataset for hallucination evaluation to assess the impact of input length and output length, as well as the model size. The inference time, measured in seconds per sample, is calculated on an NVIDIA A100 GPU with vllm 6 acceleration (except for the LLaMA2-70B model, which demands at least two A100 GPUs for deployment). Table A8: Time cost analysis for reading comprehension in LongBench. DataAug Enhancement Task-Agnostic Extra Cost Total Cost - +0.0 ✓ - 0.7s ABS -3.3 ✓ 2.7s 3.4s QBS +0.2 × 2.5s 3.2s Ours +2.5 ✓ 2.9s 3.6s † Ours + SoT +2.5 ✓ 1.2s 1.9s LongBench contains input passages with an average length of 14K tokens (computed with the LLaMA2 tokenizer), which burdens the cost for all the data augmentation methods. According to Tab. A8, only the task-specific QBS can bring a positive enhancement of 0.2% on average, which is negligible compared with the 2.5% boost by our task-agnostic structurization approach. However, the LLM-based data augmentation methods generally introduce an extra 2.5-2.9 seconds cost. To alleviate this problem, we have recently noticed a new decoding acceleration technique named Selection-of-Thought (SoT) [45], which is also motivated by the aspect-based thinking and writing process of humans and is naturally compatible with our structurization process. With the 2.39× speed up and negligible quality decrease (denoted as “† Ours + SoT”), the extra cost of our method can be further reduced to an acceptable 1.2 seconds, which we believe makes it more practical. Table A9: Time cost analysis for hallucination evaluation in AttrScore. LLM-Evaluator Enhancement Total Cost Extra Cost (%) LLaMA2-7B +0.0 1.4s - +Ours +6.4 3.5s 150% LLaMA2-70B +0.0 17.6s - +Ours +4.3 19.7s 12% AttrScore requires a relatively longer output length (with a detailed explanation for hallucination evaluation), and the vanilla time cost increases compared to LongBench. According to Tab. A9, our 6https://github.com/vllm-project/vllm 20structurization earns a 6.4% gain on the average hallucination evaluation accuracy (detailed values are shown in Tab. 2), in trade of the 1.5× increased time cost for the vanilla LLaMA2-7B evaluator. As the base evaluator is scaled up to a larger LLaMA2-70B model, our method merely introduces 12% extra cost (2.1s) and still gains a considerable enhancement of 4.3%. Our method showcases more advantages by incorporating larger base models. Performance on General Evaluation BenchmarksAs our method focuses on context structurization to enhance LLM’s cognition ability, for the tasks where no context is provided, our method may not bring significant enhancement and even get worse. The commonly used MMLU benchmark [21] is a typical scenario, where LLMs are asked to answer questions without context references but requiring their parametric knowledge (learned during large-scale pre-training), and context structurization does not help. If we insist on structurizing the question alone and feeding it into LLM’s inputs, the model may be disturbed by the introduced information from StruXGPT and generate wrong answers. As shown in Tab. A10, our method causes a slight 0.1% decrease (measured by OpenCompass protocol7) when taking LLaMA2-7B-Chat [53] as the baseline model. Table A10: Evaluation on general benchmarks. Model MMLU BBH LLaMA2-7B-Chat 45.93 30.47 +Ours 45.84 31.30 On the other hand, we have also tested another common benchmark, BBH [49], which is designed to evaluate LLMs’ reasoning capability when dealing with several logical sentences/statements. In this case, our method can adapt well to highlight the logical relations and boost LLM’s reasoning abilities by 0.8%. In conclusion, we suggest users apply structurization to long-form or logically complex contexts, while taking the original question as inputs when there is no context provided. D Broader Impacts We discuss the positive and negative societal impacts as follows: Positive Societal Impacts. Through the innovative approach of context structurization, this work significantly enhances the comprehension abilities of Large Language Models (LLMs), leading to more effective and efficient applications in various sectors such as education, healthcare, and customer service. Avoiding the need for larger models, it not only curtails the environmental impact associated with training sophisticated AI systems but also democratizes access to cutting-edge AI technologies. This fosters a broader base for innovation and empowers smaller entities with the tools to contribute meaningfully to technological advancement. Moreover, our method, rooted in human cognitive processes, promises to enrich human-AI collaboration, paving the way for solving complex societal issues by leveraging AI’s improved problem-solving capabilities. Negative Societal Impacts. On the flip side, the advances made through structurization bear potential risks, including the augmentation of disinformation campaigns and the creation of more sophisticated deepfakes, which could undermine trust in digital content. The enhanced capabilities of LLMs might also be co-opted for intrusive surveillance, raising substantial privacy and ethical dilemmas. Furthermore, the inherent biases in training data could be deepened, potentially automating and perpetuating social inequalities. The concentration of advanced AI technologies in the hands of a few could limit competition and place immense power over information dissemination with those entities, while increasing reliance on AI for critical decision-making could inadvertently erode human analytical skills over time. To mitigate these concerns, it is essential to establish rigorous ethical standards and robust oversight mechanisms that govern the deployment of LLMs, ensuring transparency and accountability in their use. Furthermore, active research and the implementation of bias detection, correction frameworks, and the development of media literacy programs are crucial 7https://github.com/open-compass/opencompass 21steps to preemptively address the potential misuse, privacy violations, and societal impact of these advanced AI systems. E Prompt Examples for Structurization E.1 Prompt Templates Fig. A4 displays the few-shot prompt template to query giant commercial LLMs to execute structur- ization. We first introduce the instruction for structurization (including the task definition and output formats), then provide two representative examples to further illustrate the process. The first example in Fig. A5 tells the model to focus on the existing numerical or enumeration indicators to assist in constructing the aspect-description structure. The second example in Fig. A6 teaches the model to automatically summarize the aspects and attach their corresponding descriptions from the raw text sequences. Finally, in Fig. A4 we ask commercial LLMs to structurize the input_statement from user input as expected. To ensure completed and faithful structurization on long-form input statements (usually with thou- sands of tokens), we provide two detailed exemplars in Fig. A5 and Fig. A6 for commercial LLMs’ few-shot learning. However, these two exemplars take a considerable context length, which may even confuse the distilled StruXGPT-7B with a smaller size capacity. We thus remove the examples in StruXGPT-7B’s prompt template, as StruXGPT-7B has learned how to perform structurization as desired and the functionality of those examples is negligible. Specifically, the example_1 and example_2 (with their introductions) are removed for both efficiency and efficacy. E.2 Structurization Examples In this section, we present several structurization examples from different models as discussed in Sec. 3 in the manuscript. Given an input statement about Facebook’s stock, Fig. A7 shows that Qwen-max produces a factual and faithful structurization result with the help of few-shot exemplars. It clearly identifies the main aspects of the original statement, and attaches the corresponding descriptions into each aspect. On the contrary, Fig. A8 indicates the smaller pre-trained 7B-size Qwen and LLaMA2 models failed to follow the instructions to produce qualified structurizations. There exists a fabricated “Note” content in Qwen-7B’s response. Meanwhile, LLaMA2-7B is even unable to follow the desired format, and information like “Facebook doesn’t sell anything tangible” is missing. Fig. A9 demonstrate the effectiveness of our fine-tuned 7B-parameter StruXGPT model, which is able to structurize the input statement as expected, with comparable performance to the giant Qwen-max model with over 200B-size. 22You are a helpful NLP assistant.  Help me rephrase a given statement to identify: (1) a summary of the statement's scope, which should generally be a noun phrase with a few words. (2) a list of main aspects on which the statement is discussing. Each main aspect should be a noun or noun phrase.  The aspect list should be precise and concise to conclude the statement, and the number of aspects should be limited. (3) an enumeration of descriptive sentences regarding each aspect above, which display the details of those aspects.  Each description sentence must be completed and faithful to the original statement. You should NOT remove any  descriptive segment in this layer. Given an original statement, the rephrased structure should strictly follow this format: ## Statement's scope: ```[generally a noun phrase]``` ## Statement's main aspects and corresponding descriptions: ``` 1. [the first aspect of the statement] 1.1 [a descriptive sentence corresponding to this aspect] 1.2 [a descriptive sentence corresponding to this aspect] 1.3 [another descriptive sentence, if necessary] 2. [the second aspect of the statement] 2.1 [a descriptive sentence corresponding to this aspect] 2.2 [another descriptive sentence, if necessary] 3. [another aspect of the statement, if necessary] ``` Here is an example to illustrate how to rephrase an input statement as the desired structure. Pay attention to  numerical or enumeration indicators, like '1. ', '2. ', '(1) ', '(2) ', '- ', etc. {example_1} Here is another example to illustrate how to rephrase an input statement as the desired structure. When there are no  explicit words indicating the statement's scope and main aspects, please use a few words to precisely summarize the  scope as well as the main aspects. Then you may carefully attach the descriptive sentences to each main aspect. {example_2} Now summarize the \"scope\" of the following statement with a few words, and then rephrase the input statement to its  \"main aspects\" and \"corresponding descriptions\" in the numerically ordered format strictly.  Note that the aspect list should be precise and concise to conclude the statement, and the number of aspects should  be limited. Each description sentence must be completed and faithful to the original statement, and you should NOT remove any  descriptive segment in this layer. # Input:  {input_statement} # Output:  Figure A4: Prompt template for few-shot structurization. 23# Input: ``` Comprehensive prevention measures for malignant diseases in the rice seedling stage are as follows: 1. Choose disease-free seeds. Do not leave seeds in diseased fields and nearby rice fields. Choose healthy rice and  eliminate diseased, dead, and injured rice. 2. Seed disinfection. Before sowing, soak the seeds with 25% 100g (Xibok) EC 3000 times liquid for 1 to 2 days, or  take 20 grams of 17% Dexinqing wettable powder for every 6 kilograms of rice seeds. Soak the seeds in 8 kg of water  for 60 hours. 3. Deal with diseased rice straw. Do not cover germinated or dry seedlings with diseased straw. ``` # Output: ## Statement's scope: ```The comprehensive prevention measures for malignant diseases in rice seedling``` ## Statement's main aspects and corresponding descriptions: ``` 1. Choose disease-free seeds 1.1 Do not leave seeds in diseased fields and nearby rice fields. 1.2 Choose healthy rice and eliminate diseased, dead, and injured rice. 2. Seed disinfection 2.1 Before sowing, soak the seeds with 25% 100g (Xibok) EC 3000 times liquid for 1 to 2 days. 2.2 An alternative is taking 20 grams of 17% Dexinqing wettable powder for every 6 kilograms of rice seeds. 2.3 Soak the seeds in 8 kg of water for 60 hours. 3. Deal with diseased rice straw 3.1 Do not cover germinated or dry seedlings with diseased straw. ``` Figure A5: The first example for few-shot structurization. 24# Input:  ``` The water absorption curve of rice after soaking is an unimodal curve. The inflection point of the curve is the peak  period of water absorption. The relationship between rice water absorption and time is non-linear and can be  expressed by the following formula: a*t+b=c, where a, b, and c are constants. Under different humidity conditions,  the change in the water absorption rate of rice with time is basically similar, that is, the water absorption rate is  between 0 and the point d inflection, and the rate of change accelerates over time. After the inflection point, the  increase in water absorption gradually stabilizes. The changing rules of water absorption and moisture content of  rice are similar, but under different humidity conditions, the relationship between water absorption of rice and  moisture content is different. When the moisture content is low, the water absorption of rice increases as the  moisture content increases. When the moisture content is high, the increase in water absorption of rice gradually  stabilizes. There are three obvious steps for rice seeds to absorb water: First, at the beginning of water absorption,  the water content of the seeds gradually increases, and the water absorption rate slowly increases. Second, during  the peak water absorption period, the water absorption rate increases rapidly. Third, in the later stage of water  absorption, the water content of seeds slowly increases, and the water absorption rate increases. The water  absorption rate of rice seeds is closely related to temperature. In general, water absorption increases as  temperature increases. The relationship between water absorption and temperature can be expressed as: unsaturated  water absorption (%) = 14.289T-10.719 (where T is temperature) ``` # Output: ## Statement's scope: ```characteristics of soaking rice seeds to absorb sufficient water``` ## Statement's main aspects and corresponding descriptions: ``` 1. The water absorption curve 1.1 The water absorption curve of rice after soaking is an unimodal curve. 1.2 The inflection point is the peak period of water absorption. 1.3 The relationship between rice water absorption and time is non-linear, which can be expressed by a*t+b=c,  where a, b, and c are constants. 2. Rice water absorption as time changes 2.1 Under different humidity conditions, the change in the water absorption rate of rice with time is basically  similar. 2.2 The water absorption rate is between 0 and the point d inflection. 3.3 The rate of change accelerates over time 2.3 After the inflection point, the increase in water absorption gradually stabilizes. 3. Relationship between rice water absorption and moisture content 3.1 The changing rules of water absorption and moisture content of rice are similar 3.2 Under different humidity conditions, the relationship between water absorption of rice and moisture content  is different. 3.3 When the moisture content is low, the water absorption of rice increases as the moisture content increases. 3.4 When the moisture content is high, the increase in water absorption of rice gradually stabilizes. 4. Three stages for rice seeds to absorb water 4.1 At the beginning of water absorption, the water content of the seeds gradually increases, and the water  absorption rate slowly increases. 4.2 During the peak water absorption period, the water absorption rate increases rapidly. 4.3 In the later stage of water absorption, the water content of seeds slowly increases, and the water absorption  rate increases. 5. Relationship between water absorption rate and temperature 5.1 The water absorption rate of rice seeds is closely related to temperature. 5.2 Water absorption generally increases as temperature increases. 5.3 The relationship can be expressed as: unsaturated water absorption (%) = 14.289T-10.719 (where T is  temperature) ``` Figure A6: The second example for few-shot structurization. 25Source Statement { ``` The amount of hype and uneducated investors/speculators driving its prices up. Just by that, I would say its prices  are inflated. Bear in mind that Facebook doesn't sell anything tangible. They can go down as fast as they went up.  Most of their income is ad-based and single-product oriented, and as such highly dependent on usage and trends  (remember MySpace?). Having said that, all the other \"classic\" valuation techniques are still valid and you should  utilize them. ``` } Qwen_max's Output { ## Statement's scope: ```Valuation concerns for Facebook due to hype and lack of tangible products``` ## Statement's main aspects and corresponding descriptions: ``` 1. Hype and uneducated investors/speculators 1.1. The amount of hype and uneducated investors/speculators is significant, which drives up the prices. 1.2. This suggests that the current prices are inflated based on this factor alone. 2. Lack of tangible products 2.1. Facebook doesn't sell any tangible goods, making its value potentially unstable. 2.2. Prices can drop rapidly, similar to their rate of increase. 3. Ad-based income and dependency 3.1. Most of Facebook's income is ad-based and focused on a single product. 3.2. Their revenue is highly dependent on usage trends and popularity (citing the decline of MySpace). 4. Classic valuation techniques 4.1. Despite these concerns, traditional valuation methods remain valid for assessing Facebook's value. 4.2. It's recommended to utilize classic valuation techniques when evaluating the company. ``` } Figure A7: An example for source inputs and structurized results from few-shot Qwen-max. Qwen_7B's Output { ## Statement's scope: ```market analysis of Facebook stock``` ## Statement's main aspects and corresponding descriptions: ``` 1. Influence of hype and uneducated investors on stock prices 1.1 The amount of hype and uneducated investors driving its prices up leads to an inflated valuation of Facebook  stock. 1.2 The price increase can be unstable, as the company's value is dependent on user growth and trends. 2. Evaluation of Facebook's business model 2.1 Facebook's revenue is primarily generated through advertising and is highly dependent on user behavior and  trends. 2.2 Their income is single-product oriented, so they are vulnerable to changes in market conditions. ``` Note: The original statement was quite long and complex. It discusses the influence of hype and uneducated investors  on stock prices, before moving on to an evaluation of Facebook's business model. The scope of the statement includes  the analysis of Facebook stock, but it could be more precise by narrowing it down to just \"stock prices.\" This is why  the first part of the rephrased structure includes the phrase \"market analysis of Facebook stock.\" In the second part,  I identified two main aspects: the influence of hype and uneducated investors on stock prices, and the evaluation of  Facebook's business model. Each aspect is further divided into several descriptive sentences that are faithful to the  original statement. } LLaMA2_7B's Output { Scope:  ```The reasons behind the high valuation of Facebook``` Main Aspects and Corresponding Descriptions: 1.  The influence of hype and uneducated investors/speculators 1.1 Just by that I would say its prices are inflated. 1.2 Facebook's reliance on ad-based and single-product-oriented income exposes it to market trends and usage. 2.  Dependence on usage and trends 2.1 Remember MySpace? 3.  Validity of alternative valuation techniques 3.1 Having said that, all the other \"classic\" valuation techniques are still valid and you should utilize them. } Figure A8: Corresponding structurization results from few-shot Qwen-7B and LLaMA2-7B. 26StruXGPT_7B_Qwen 's Output { ## Statement's scope: ```The valuation of Facebook's stock considering unpractical factors``` ## Statement's main aspects and corresponding descriptions: ``` 1. Price inflation due to hype and inexperienced investors/speculators. 1.1. The vast amounts of hype surrounding Facebook's stock drive its prices up. 1.2. This suggests that the materiality of its assets is not justified. 2. Facebook's financial stability and potential for volatility. 2.1. Facebook doesn't sell any tangible goods. 2.2. They can decline as rapidly as they have grown. 2.3. The majority of its income comes from advertising and a single product model, making it highly dependent on  usage trends and fashion (parallels with MySpace's decline). 3. Validity and application of valuation techniques. 3.1. All traditional valuation methods are still applicable to Facebook's stock. 3.2. Investors are advised to utilize these techniques despite the platform's unique situation. ``` } StruXGPT_7B_LLaMA 's Output { ## Statement's scope: ```Misconceptions about the market value of Facebook based on uneducated sentiment and issues related to its business  model``` ## Statement's main aspects and corresponding descriptions: ``` 1. Hype and uneducated investors 1.1. The amount of hype around Facebook is driving up its price. 1.2. Uneducated investors/speculators are also contributing to the inflated prices. 2. Income dependence 2.1. Over 50% of Facebook's income comes from advertisements. 2.2. A significant portion of Facebook's revenue is reliant upon usage and trends. 3. Business model volatility 3.1. The company does not sell any tangible products, hence its value is inherently volatile. 3.2. As such, gains can be just as quick as they were achieved. 4. Classic valuation techniques 4.1. Despite hype and business unpredictability, other \"classic\" valuation techniques remain applicable to assess  its market value. 4.2. They should continue to be utilized for a comprehensive assessment of Facebook's worth. ``` } Figure A9: Corresponding structurization results from our fine-tuned StruXGPT-7B-Qwen and StruXGPT-7B-LLaMA. 27",
      "references": [
        "Gpt-4 technical report",
        "Qwen technical report",
        "Longbench: A bilingual, multitask benchmark for long context understanding",
        "A question-entailment approach to question answering",
        "Biological Psychology: An Introduction to Behavioral, Cognitive, and Clinical Neuroscience",
        "Language models are few-shot learners",
        "Adapting large language models via reading comprehension",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Palm: Scaling language modeling with pathways",
        "A discourse-aware attention model for abstractive summarization of long documents",
        "An introduction to discourse analysis",
        "A dataset of information-seeking questions and answers anchored in research papers",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Glam: Efficient scaling of language models with mixture-of-experts",
        "The structure of cognition: Attentional episodes in mind and brain",
        "Cognitive Neuroscience: The Biology of the Mind (5th edition)",
        "Unsupervised corpus aware language model pre-training for dense passage retrieval",
        "An introduction to discourse analysis: Theory and method",
        "Minillm: Knowledge distillation of large language models",
        "Textbooks are all you need",
        "Measuring massive multitask language understanding",
        "Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps",
        "Mistral 7b",
        "Structgpt: A general framework for large language model to reason over structured data",
        "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
        "Scaling laws for neural language models",
        "Constituency parsing with a self-attentive encoder",
        "A revision of bloom’s taxonomy: An overview",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "How long can open-source llms truly promise on context length?",
        "Camel: Communicative agents for \"mind\" exploration of large language model society",
        "Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql",
        "Llara: Aligning large language models with sequential recommenders",
        "Rouge: A package for automatic evaluation of summaries",
        "Truthfulqa: Measuring how models mimic human falsehoods",
        "Lost in the middle: How language models use long contexts",
        "A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity",
        "# instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
        "Fine-tuning llama for multi-stage text retrieval",
        "Www’18 open challenge: financial opinion mining and question answering",
        "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
        "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
        "Orca: Progressive learning from complex explanation traces of gpt-4",
        "Ms marco: A human generated machine reading comprehension dataset",
        "Skeleton-of-thought: Large language models can do parallel decoding",
        "Instruction tuning with gpt-4",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia",
        "Challenging big-bench tasks and whether chain-of-thought can solve them",
        "Alpaca: A strong, replicable instruction-following model",
        "Mind: Introduction to Cognitive Science",
        "Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.arXiv preprint arXiv:2104.08663",
        "Llama: Open and efficient foundation language models",
        "Musique: Multihop questions via single-hop question composition",
        "Computing machinery and intelligence",
        "Siren’s song in the ai ocean: A survey on hallucination in large language models",
        "Simlm: Pre-training with representation bottleneck for dense passage retrieval",
        "Emergent abilities of large language models",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models",
        "Bloom: A 176b-parameter open-access multilingual language model",
        "Retrieval meets long context large language models",
        "A survey on knowledge distillation of large language models",
        "Exploring the limits of chatgpt for query or aspect-based text summarization",
        "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
        "Automatic evaluation of attribution by large language models",
        "Glm-130b: An open bilingual pre-trained model",
        "Bertscore: Evaluating text generation with bert",
        "Benchmarking large language models for news summarization",
        "Survey on factuality in large language models: Knowledge, retrieval and domain-specificity",
        "A survey of large language models",
        "Representation engineering: A top-down approach to ai transparency"
      ],
      "meta_data": {
        "arxiv_id": "2407.16434v2",
        "authors": [
          "Kai Liu",
          "Zhihang Fu",
          "Chao Chen",
          "Wei Zhang",
          "Rongxin Jiang",
          "Fan Zhou",
          "Yaowu Chen",
          "Yue Wu",
          "Jieping Ye"
        ],
        "published_date": "2024-07-23T12:33:58Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces the concept of context structurization to enhance the cognitive capabilities of Large Language Models (LLMs) without modifying their internal architecture. The main contributions include proposing the concept of structurization, demonstrating the feasibility of distilling this capability from large commercial LLMs into a smaller, more efficient StruXGPT-7B model, and empirically showing consistent cognition enhancement across various LLMs and NLP tasks through structurization.",
        "methodology": "The core methodology involves transforming plain, sequential text into well-ordered and hierarchically structured elements. This structurization is achieved in two ways: few-shot prompting of large commercial LLMs (like GPT-3.5-Turbo or Qwen-Max) and fine-tuning a smaller model (StruXGPT-7B, based on LLaMA2 or Qwen-7B) to inherit this structurization ability. The structurized information is then transformed back into natural language using specific linguistic markers to highlight hierarchy (e.g., numbered lists, bullet points, indentation) before being fed to the LLMs. Knowledge distillation is employed to train StruXGPT-7B from the outputs of larger teacher models.",
        "experimental_setup": "Extensive evaluations were conducted across various model architectures and sizes (auto-regressive LLMs and BERT-like masking models) on diverse NLP tasks. These tasks included: 1. Context-based Question-Answering: Evaluated on 7 subsets of the LongBench benchmark (single-document QA, multi-document QA, synthetic QA) with LLaMA2-7B/13B-4k, Qwen-7B-8k, and ChatGLM3-6B-32k models. 2. Exhaustive Hallucination Evaluation: Assessed using AttrScore and FactScore datasets with LLaMA2-7B/70B and GPT-3.5-Turbo models. 3. Passage-level Dense Retrieval: Applied to the BEIR dataset (NFCorpus, FiQA, ArguAna, SciDocs, SciFact) with BERT, SimLM, and coCondenser models. The structurization approach itself was evaluated using LexicalEval (ROUGE-L), HumanEval (completeness, factuality, anti-hallucination on a 0-5 scale), AppEval (downstream application improvement on Qasper subset), and SemEval (BERTScore for semantic consistency). Ablation studies were conducted on the number of few-shot examples for teacher models and the parameter capacity of StruXGPT.",
        "limitations": "The paper acknowledges several limitations: 1. **Training-time Aggregation**: The current structurization is mainly investigated during the LLM's inference stage, meaning the full potential of training LLMs to intrinsically capture context structures and extrinsic correlations remains an unresolved challenge. 2. **Inference Efficiency**: Methods adopting LLMs for data augmentation, including structurization, generally introduce additional inference costs. While optimization techniques like Selection-of-Thought (SoT) can mitigate this, it remains a consideration. 3. **General Evaluation Benchmarks**: For tasks where no context is provided (e.g., MMLU benchmark), structurization may not bring significant enhancement and can even cause a slight performance decrease, as the model relies on parametric knowledge rather than contextual understanding.",
        "future_research_directions": "Future research directions include: 1. **Developing methodologies for training LLMs to capture the intrinsic structure of context**: This would involve moving beyond inference-time structurization to enable LLMs to inherently understand and utilize knowledge structures from the training corpus. 2. **Exploring more elaborated structures**: While the paper used a generic three-layer structure, investigating more complex structures (like knowledge mindmaps) could lead to better deconstruction of specific text sources, though this increases the complexity of definition, extraction, and utilization. 3. **Mitigating inference costs**: Further research into optimizing the inference efficiency of structurization, potentially through more advanced decoding acceleration techniques or integrating structurization more deeply into the model's architecture, is warranted. 4. **Addressing performance on general benchmarks**: Investigating how structurization might be adapted or selectively applied to tasks that primarily rely on parametric knowledge would be beneficial",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation",
      "full_text": "Published as a conference paper at ICLR 2024 SKELETON -OF-THOUGHT : P ROMPTING LLM S FOR EFFICIENT PARALLEL GENERATION Xuefei Ning1∗ foxdoraame@gmail.com Zinan Lin2∗ linzinan1995@gmail.com Zixuan Zhou14∗ zhouzx21@mails.tsinghua.edu.cn Zifu Wang3 zifu.wang@kuleuven.be Huazhong Yang1 yanghz@tsinghua.edu.cn Yu Wang1 yu-wang@tsinghua.edu.cn 1 Department of Electronic Engineering, Tsinghua University, Beijing, China 2 Microsoft Research, Redmond, Washington, USA 3 ESAT-PSI, KU Leuven, Leuven, Belgium 4 Infinigence-AI Website: https://sites.google.com/view/sot-llm Code: https://github.com/imagination-research/sot ABSTRACT This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton pointin parallel. Not only does SoT provide consid- erable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric op- timization for inference efficiency, and showcases the potential of eliciting high- quality answers by explicitly planning the answer structure in language. 1 I NTRODUCTION Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI, 2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and chatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their interactive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through Slack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on one NVIDIA A100 GPU) to answer the question in Fig. 1. We conclude three major causes of LLMs’ slow inference: (1) A large model size requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT- 3 take 350GB memory, which means at least 5 ×80GB A100 GPUs are needed to keep the model in GPU memory. Even with enough GPUs, the heavy memory access and computation slow down the inference. (2) The attention operation in the prevailing transformer architecture is I/O bounded and has a quadratic memory and computation complexity in sequence length. (3) The sequential decoding approach in inference generates tokens one by one. This approach introduces a significant inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes:large model size(Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) andattention operation (Kitaev et al., 2020; Wang et al., 2020; ∗Equal contribution. 1 arXiv:2307.15337v3  [cs.CL]  2 Mar 2024Published as a conference paper at ICLR 2024 Answer 1. Active listening involves fully  concentrating on … 2. Identify issues. Look into the root  causes of … 3. Compromise. Look for a middle  ground … What are the most effective  strategies for conflict  resolution in the workplace? Question Skeleton-of-Thought  Decoding Generates answers sequentially ➔Slower Normal  Decoding 1. Active listening 2. Identify issues 3. Compromise Generates answers in parallel ➔Faster (1) Skeleton stage (2) Point- expanding stage 1.0 1.2 1.4 1.6 1.8 Speed-up −0.2 0.0 0.2 0.4 Net win rates Vicuna-13B V1.3 StableVicuna-13B UltraLM-13B Vicuna-33B V1.3 LLaMA2-Chat-7B LLaMA2-Chat-13B Vicuna-7B V1.3 ChatGPT-3.5 Claude Vicuna-7B V1.1 OpenChat-13B GPT-4 Baseline Figure 1: Left: An illustration of Skeleton-of-Thought (SoT). Instead of producing answers se- quentially, SoT produces different parts of answers in parallel. In more detail, given the question, SoT first prompts the LLM to give out the skeleton, then conducts batched decoding or parallel API calls to expand multiple points in parallel, and finally aggregates the outputs to get the final answer. Right: The net win rates and speed-ups of SoT with router (SoT-R) compared to normal generation on Vicuna-80. The net win rate is the difference between the fraction of questions that SoT-R has better and worse answers than normal generation. The speed-up is the ratio between the latency of normal and SoT-R generation. (1.0, 0.0) represents normal generation. Higher is better on both axes. For most models, SoT-R not only accelerates the generation but also improves the quality of the answers (evaluated with FastChat metric (Zheng et al., 2023)). See § 3.2 and 4 for more details. Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021). In contrast to prior work, we tackle the third axis and question the common assumption that LLMs have to do fully sequential decoding. We show the feasibility of parallel decoding of off-the-shelf LLMs without any changes to their model, system, or hardware . For instance, for the question in Fig. 1, we can reduce the latency from 22 seconds to 12 seconds (1.83 × speed-up) with Claude, and from 43 seconds to 16 seconds (2.69× speed-up) with Vicuna-33B V1.3 on an NVIDIA A100. The idea stems from reflecting on how humans ourselves answer questions. Humans do not always think about questions and write answers in a sequential fashion. In contrast, for many question types, we first derive the skeleton according to some protocols and strategies, and then add evidence and details to explain each point. This is especially the case on occasions like offering consultancy, taking tests, writing papers, and so on. This intuition has our back to question the necessity of fully sequential decoding. In this paper, we propose Skeleton-of-Thought (SoT). Specifically, as shown in Fig. 1, we guide the LLM to derive a skeleton first by itself. Based on the skeleton, the LLMs can complete each point in parallel so that we get a speed-up. SoT can be utilized to accelerate both open-source models with batched decoding and API-based models with parallel API calls. The current SoT is suitable for questions that require a long answer whose structure can be planned ahead, while not suitable for questions that require step-by-step reasoning or only need a short answer. Therefore, to make the overall solution more practical, we design an extension, SoT with router (SoT-R), which employs a router to only trigger SoT for suitable questions. We test SoT on 12 recently released LLMs. Not only does SoT provide considerable speed-ups (up to 2.39×), but it can also improve the answer quality in many cases (Fig. 1). Note that in contrast to existing model- and system-level efforts for inference efficiency, SoT takes a novel “data-level” pathway by letting the LLM organize its output content. This novel perspective is becoming feasible and is expected to grow in importance, owing to the evolving capabilities of state-of-the-art LLMs. We hope this work can stimulate more research in the realm of data-centric optimization (Zha et al., 2023; HazyResearch, 2023) for efficiency. 2Published as a conference paper at ICLR 2024 Prompt 1. Skeleton Prompt Template Ts [User:] You’re an organizer responsible for only giving the skeleton (not the full content) for answering the question. Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence, each skeleton point should be very short with only 3∼5 words. Generally, the skeleton should have 3∼10 points. Now, please provide the skeleton for the following question. {question} Skeleton: [Assistant:] 1. Prompt 2. Point-Expanding Prompt Template Tpe [User:] You’re responsible for continuing the writing of one and only one point in the overall answer to the following question. {question} The skeleton of the answer is {skeleton} Continue and only continue the writing of point {point index }. Write it **very shortly** in 1 ∼2 sentence and do not continue with other points! [Assistant:] {point index}. {point skeleton} The rest of the paper is organized as follows. We first introduce SoT in § 2 and show its results in § 3. Then, we expand on the SoT-R extension in § 4. § 5 positions SoT in the research ecosystem (expanded in App. D). Finally, we analyze the limitations and share outlooks of SoT in § 6. 2 S KELETON -OF-THOUGHT (SOT) 2.1 M ETHOD Overview. Based on the intuition that humans usually think about and answer a question in an organized way, the core idea of this work is to guide the LLM itself to give a skeleton first and then write the overall answer parallelly instead of sequentially. Fig. 1 illustrates how SoT produces the final answer to a user question q. (1) Skeleton stage. SoT first assembles a skeleton request, Ts(question = q), using the skeleton prompt template Ts (Prompt 1, and Prompt 3 in App. B.1) with the questionq as the parameter. The skeleton prompt template is written to guide the LLM to output a concise skeleton of the answer. Then, we extract the B points from the skeleton response Rs of the LLM. (2) Point-expanding stage. Based on the skeleton, we let the LLM expand on each point in parallel. Specifically, for the point with index b and skeleton Rs b, SoT uses Tpe(question = q, skeleton = Rs, point index = b, point skeleton = Rs b) as the point-expanding request for the LLM, where Tpe is the point-expanding prompt template (Prompt 2). Finally, after completing all points, we concatenate the point-expanding responses {Rpe b }b=1,···,B to get the final answer. Parallel point expanding. We conduct parallel point-expanding so that SoT is able to achieve a speed-up than normal decoding. (1) For proprietary models with only API access, we can issue multiple parallel API calls to get an end-to-end latency gain at the cost of an increased number of API requests and tokens. (2) For open-source models that we can run locally , we let them process the point-expanding re- quests as a batch (paddings are added to the left of the point-expanding requests). We explain below why this could achieve speed-ups. A typical LLM generative process consists of two phases: (a) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (b) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. Note that the decoding phase is bottlenecked by weight loading instead of activation 3Published as a conference paper at ICLR 2024 loading or computation. 1 Consequently, running LLM inference with increased batch sizes does not increase the per-token latency much. Therefore, SoT allows us to decode roughly B× more to- kens within the same amount of time if we parallelly decodeB points. See App. E for the expanded discussions and the supporting experiments. Please refer to App. B for more implementation details. 3 S OT EVALUATION Datasets. We evaluate SoT on two recent assistant-style datasets: (1) Vicuna-80 (Chiang et al., 2023), which contains 80 questions spanning nine categories, such as coding, math, writing, role- play, and so on, and (2) WizardLM (Xu et al., 2023), which contains 218 questions spanning more categories and diverse difficulties. Due to space constraints, we only report Vicuna-80 results in the main paper, and defer WizardLM results to the Apps. G and I. Models. We test SoT on 12 models, including 9 open-source models and 3 API-based models. We obtain the weights of all the open-source models from Hugging Face. See App. A for more details. 3.1 E VALUATION OF EFFICIENCY API-based models. We record the latency of every API call with start = time.time(); ...; elapsed_time = time.time() - start, and add the latency of the skeleton API call and the slowest point-expanding API call as the SoT latency. Open-source models. All open-source models we currently evaluate are based on the LLaMA 7B, 13B, or 33B architectures. Thus, to enable fast analysis, we first make a latency profiling table for each LLaMA architecture on NVIDIA A100. The table contains the architecture’s (1) latency for prefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding one token with a context of length 1 to 1024 with different batch sizes (from 1 to 16). With these three latency profiling tables, given the number of points B, the token lengths of the requests and responses in the skeleton and point-expanding stages, we can quickly estimate the SoT latency by simply looking up entries in the tables and adding them up. See App. F for a more detailed description of how we conduct the profiling and estimate the latency. In addition to the above approach, we also compare the actual latency of SoT and normal sequential generation (abbreviated as “normal” in the following discussion) in App. G.1.4. The rest of this section shows the speed-ups of SoT on different models (§ 3.1.1) and question categories (§ 3.1.2). In addition, we also report the latency breakdown of SoT stages in App. G.1.2 and the SoT speed-ups on an RTX 3090 GPU in App. G.1.3. 3.1.1 S PEED -UP BREAKDOWN : M ODELS We investigate how SoT reduces the end-to-end latency on different models. Fig. 2a shows the average speed-up for each model across all question categories. We can see that SoT obtains a>2× speed-up (up to 2.39×) on 8 out of 12 models. We report the detailed statistics about token lengths and numbers of points in Fig. 11. (1) In terms of the point number B (Fig. 11a), LLaMA2, Vicuna-7B V1.1, Vicuna-7B V1.3, and ChatGPT-3.5 yield relatively fewer points (<6), while GPT-4 and StableVicuna-13B generates the largest number of points on average ( ≈9). (2) Regarding the point-expanding response length , Figs. 11b to 11d show that the API-based models, ChatGPT-3.5, Claude, and GPT-4, follow the point-expanding request better and generate shorter point-expanding responses than the open-source models. One can also notice that StableVicuna-13B’s longest point-expanding responses for many question cat- egories can be as lengthy as the overall normal answer, since it fails to adhere to the “Write it **very shortly**” instruction in the point-expanding request. Consequently, SoT cannot accelerate StableVicuna-13B well. (3) Regarding the length balance degree between point responses, Fig. 11e shows that LLaMA2 and the API-based models generate more balanced point-expanding responses. (4) As for the overall length of the final aggregated answer (Fig. 11f), employing SoT on most models results in answers that are, on average, 1∼2× longer than the normal answer. 1This is true when the number of concurrent queries is small; see § 6 for discussion on other scenarios. 4Published as a conference paper at ICLR 2024 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 StableVicuna-13B Claude Vicuna-13B V1.3 ChatGPT-3.5 GPT-4 Vicuna-7B V1.3 UltraLM-13B Vicuna-33B V1.3 OpenChat-13B Vicuna-7B V1.1 LLaMA2-Chat-13B LLaMA2-Chat-7B 1.13× 1.31× 1.91× 1.97× 2.00× 2.01× 2.18× 2.24× 2.28× 2.30× 2.38× 2.39× (a) Different models. 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 math fermi counterfactual roleplay coding common-sense writing generic knowledge 1.34× 1.69× 1.89× 1.95× 2.06× 2.24× 2.26× 2.31× 2.33× (b) Different categories. Figure 2: Average speed-ups of SoT on different models and question categories. 3.1.2 S PEED -UP BREAKDOWN : Q UESTION CATEGORIES Here we investigate how SoT reduces the end-to-end latency for different question categories. Fig. 2b shows the average speed-up for each question category across all models. The question categories for which SoT can provide high-quality answers are marked in green, and other cate- gories are marked in red (see § 3.2.3 for the answer quality evaluation). We can see that SoT can obtain speed-ups for all question categories. For the five question categories that SoT can provide high-quality answers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT can speed up the overall answer generation process by 1.89× to 2.33× in the meantime. 3.2 E VALUATION OF ANSWER QUALITY In order to compare the answer quality of the normal sequential generation (abbreviated as “normal” in the following discussion) and SoT generation, we adopt two LLM-based evaluation frameworks: FastChat (Zheng et al., 2023) and LLMZoo (Chen et al., 2023c). The evaluation process is to present a question and a pair of answers (from normal or SoT generation) to an LLM judge (GPT-4 in the main paper; see App. I.4 for the results evaluated using ChatGPT-3.5) and ask for its preference. Here are more details about the evaluation of the answer quality: (1) Detailed metrics. FastChat provides one metric for the general answer quality. In addition to a general metric, LLMZoo provides five detailed metrics on the answers’ coherence, diversity, immersion, integrity, and relevance. (2) Question categories. FastChat provides two special evaluation prompts for coding and math questions for more accurate evaluation, whereas LLMZoo does not. Following the implementation in LLMZoo, we exclude math and coding questions in all LLMZoo evaluation results. (3) Extentions to avoid evaluation bias.To avoid the potential bias from the order of the two answers presented to the LLM judge, we extend FastChat and LLMZoo evaluation frameworks by running the evaluation twice with either ordering of the two answers. In either evaluation, a score of 1, 0, and -1 is assigned when SoT wins, ties, or loses, respectively. The final evaluation is that SoT wins/ties/loses when the sum of the two scores is positive/zero/negative. For example, if SoT wins in one evaluation and loses in the other evaluation, the result is “tie”. If SoT wins (loses) in one evaluation and ties in the other, the result is “win” (“lose”). (4) Net win rates. We further define net win rates to give a summarized view of the answer quality. Given the number of questions that SoT wins (#win) and loses (#lose), we define net win rates as #win−#lose/total number of questions. 0% means that SoT performs competitively to the normal baseline (wins and loses in the same number of questions). Higher values mean that SoT performs better. In the following sections, we first present the overall quality of SoT answers (§ 3.2.1), and then go into the details across different question categories (§ 3.2.3), models (§ 3.2.2), and metrics (§ 3.2.4). 3.2.1 O VERALL QUALITY In Fig. 3, we show the win/tie/lose rates (the percentage of the cases when SoT wins/ties/loses compared to normal generation) across all models and questions using the two metrics from FastChat and LLMZoo that capture the general quality of the answers. We notice a discrepancy between the two metrics on when SoT is strictly better than the baseline (45.8% v.s. 29.5%). Despite that, the two metrics agree that SoT is not worse than the baseline in around 60% of the cases, and the win 5Published as a conference paper at ICLR 2024 rates are close to the lose rates. This result suggests that the answers of SoT maintain good quality of that of the normal generation. 0% 20% 40% 60% 80% 100% General quality (LLMZoo) General quality (FastChat) 45.8% 29.5% 19.6% 29.3% 34.5% 41.2% Win Tie Lose Figure 3: Win/tie/lose rates of SoT v.s. normal generation using “general” metrics from FastChat and LLMZoo. SoT performs better than or equal to normal generation in around 60% cases. 3.2.2 Q UALITY BREAKDOWN : M ODELS We compute net win rates on all models in Fig. 4. Again, we see that the two general metrics from FastChat and LLMZoo have different absolute values but similar rankings. In particular, both metrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B have low net win rates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B havehigh net win rates. -60% -40% -20% 0% 20% StableVicuna-13B UltraLM-13B Vicuna-13B V1.3 GPT-4 LLaMA2-Chat-7B Vicuna-33B V1.3 Vicuna-7B V1.3 ChatGPT-3.5 LLaMA2-Chat-13B OpenChat-13B Vicuna-7B V1.1 Claude (a) Metric: general quality (FastChat). -40% -20% 0% 20% 40% 60% StableVicuna-13B UltraLM-13B Vicuna-13B V1.3 GPT-4 LLaMA2-Chat-7B Vicuna-33B V1.3 Vicuna-7B V1.3 ChatGPT-3.5 LLaMA2-Chat-13B OpenChat-13B Vicuna-7B V1.1 Claude (b) Metric: general quality (LLMZoo). Figure 4: Net win rates of SoT on different models. We investigate the answers in App. I.1.1, and summarize the key takeaways as follows. Some models have low SoT net win rates as they cannot understand the skeleton and point-expanding prompts well. Some other models have low SoT net win rates as their normal answers already have good quality, making it hard for SoT to beat them (e.g., Claude). For models that are able to understand the SoT prompts and the normal answers are not good enough, SoT can improve the answer quality. We expect that further improving SoT prompts or fine-tuning the models can make it easier for LLMs to understand the skeleton and point-expanding prompts and ultimately result in better answer quality. 3.2.3 Q UALITY BREAKDOWN : Q UESTION CATEGORIES We compute net win rates on all question categories in Fig. 5. Similar to Fig. 3, we see that LLMZoo tends to be more optimistic about the quality of SoT than FastChat. Nevertheless, the conclusions are consistent: SoT performs relatively well on generic, common-sense, knowledge, roleplay, and counterfactual, and relatively poorly on writing, fermi, math, and coding. -80% -60% -40% -20% 0% 20% 40% counterfactual generic common-sense knowledge roleplay fermi writing math coding (a) Metric: general quality (FastChat). -20% 0% 20% 40% 60% counterfactual generic common-sense knowledge roleplay fermi writing (b) Metric: general quality (LLMZoo). Figure 5: Net win rates of SoT on different question categories. We investigate the answers in App. I.1.2, and summarize the key takeaways as follows. SoT per- forms well when the question can be answered in several points whose details can be expanded independently. This includes a wide range of real-world questions. On the other hand, it is fun- damentally challenging to apply SoT on questions that require step-by-step thinking, in which the latter steps require the details from the earlier steps, such as math questions. To make SoT general 6Published as a conference paper at ICLR 2024 across broader question categories, one promising pathway is to enable SoT to adaptively fall back to normal generation, which we explore in § 4. Interestingly, our results suggest that some LLMs are already able to do that occasionally without special prompting or tuning (see App. I.1.2). 3.2.4 Q UALITY BREAKDOWN : M ETRICS In Fig. 6, we show more detailed metrics from LLMZoo to reveal in which aspects SoT can improve or hurt the answer quality. On average, we can see that SoT improves the diversity and relevance while hurting the immersion and coherence. 0% 20% 40% 60% 80% 100% Integrity Coherence Immersion Relevance Diversity 23.2% 29.8% 40.5% 61.4% 99.9% 34.6% 30.6% 23.7% 11.3% 0.1% 42.1% 39.6% 35.8% 27.3% Win Tie Lose Figure 6: Win/tie/lose rates of SoT v.s. normal generations using metrics from LLMZoo. SoT performs well on diversity and relevance, and relatively worse on coherence and immersion. Through answer investigation (App. I.1.3), we summarize the key takeaways as follows. The skele- ton stage of SoT explicitly require LLMs to discuss the answers from multiple aspects without filler words. This improves the diversity and relevance of the answers. As for coherence and immersion, SoT is not worse than the normal generation around 60% of the time. One future direction is to improve the SoT prompts or pipeline so that the answers can be better in more metrics. 4 S OT WITH ROUTER (SOT-R): A DAPATIVELY TRIGGERING SOT In § 3, we see that SoT provides considerable speed-ups while maintaining (or even improving) answer quality for many question types. However, the biggest limitation is that SoT is not suitable for questions that require step-by-step reasoning (§ 3.2.3). Towards pushing the practical adoption of SoT, we explore the possibility of adaptively triggering SoT only when it is suitable. To achieve that, we propose a router module that decides if SoT should be applied for the user request, and then call either SoT or normal decoding accordingly. This paradigm aligns with the recent trends of composing multiple models to solve complicated tasks (Chase, 2022; Shen et al., 2023). To implement the router, we explore two options: LLM prompting as the router (no model training is needed) (§ 4.1), and trained RoBERTa as the router (§ 4.2). The evaluation is provided in § 4.3. 4.1 P ROMPTING ROUTER We directly ask an LLM if the question is suitable for SoT. More specifically, we ask the LLM if the desired answer is in a list of independent points (see App. C.1 for the prompt). If the answer is yes, we will use SoT; otherwise, we will use normal generation (i.e., directly feeding the question to the LLM). We employ GPT-4 as the LLM router given its strong capability. 4.2 T RAINED ROUTER While leveraging GPT-4 as the router obviates the need for model training, its performance remains sensitive to prompt design. Therefore, we approach the problem as a sequence classification task by fine-tuning a small language model as the router. Specifically, we annotate the LIMA dataset (Zhou et al., 2023) as the training set to train a RoBERTa model (Liu et al., 2019), which has only 120M parameters. Details about the annotation and training can be found in Apps. C.2.1 and C.2.2. 4.3 S OT-R EVALUATION We compare SoT and SoT-R under the same evaluation setup in § 3. Besides the prompting and trained routers, we also consider a “human router” where we manually judge whether SoT should be applied for each question. This serves as a benchmark for comparison. 7Published as a conference paper at ICLR 2024 4.3.1 E VALUATION OF EFFICIENCY Fig. 7 shows the speed-ups of SoT and SoT-R for different models on Vicuna-80 (see App. G.2 for results on the WizardLM dataset). We can see that: (1) As expected, SoT-R obtains lower speed- ups than SoT, since SoT is not triggered for some questions and the router induces a small latency overhead. Nevertheless, SoT-R can still benefit most models with >1× speed-ups. (2) SoT-R with the trained router obtains slightly higher speed-ups for 7 out of 12 models on Vicuna-80, while SoT-R with the prompting router obtains higher speed-ups for all models on WizardLM (Fig. 17). 1.0 1.5 2.0 2.5 3.0 3.5 4.0 StableVicuna-13B Claude Vicuna-13B V1.3 ChatGPT-3.5 GPT-4 Vicuna-7B V1.3 UltraLM-13B Vicuna-33B V1.3 OpenChat-13B Vicuna-7B V1.1 LLaMA2-Chat-13B LLaMA2-Chat-7B SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router Figure 7: Speed-ups of SoT and SoT-R on dif- ferent models across all question categories of the Vicuna-80 dataset. -80% -60% -40% -20% 0% 20% 40% counterfactual generic common-sense knowledge roleplay fermi writing math coding SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router SoT-R w/ human router Figure 8: Net win rates of SoT and SoT-R on different question categories of the Vicuna-80 dataset (evaluated with the FastChat metrics). 4.3.2 E VALUATION OF ANSWER QUALITY Fig. 8 shows the net win rates (averaged across all models) of SoT and SoT-R on Vicuna-80 with the FastChat metrics (see App. I.2 for results of the WizardLM dataset and LLMZoo metrics). We can see that: (1) SoT-R significantly improves the answer quality on questions where SoT is not suitable (e.g., coding, math, writing, fermi) by falling back to normal decoding. At the same time, SoT-R maintains answer quality improvements on questions where SoT is good at. (2) The trained router performs similar to (on Vicuna-80) or better than (on WizardLM; see App. I.2) the prompting router. This accords with our intuition in § 4.2. (3) The prompting and trained routers could even surpass human router (e.g., on roleplay questions; see more examples on WizardLM in App. I.2). We discuss the consistency across three routers in App. C.3. The primary takeaways include: (1) on Vicuna-80, there is a notable consistency among all three routers, and (2) on WizardLM, greater discrepancies emerge, with the trained router showing higher alignment with human annotations. 5 S OT IN THE CONTEXT OF LITERATURE This section positions SoT in related work to reveal how SoT (1) is connected to, (2) is different from, and (3) can harness the power of other methods. See App. D for the expanded discussion. Efficient LLM methods at model and system levels. At the model level, prior work proposes ef- ficient architectures, including dynamic mixture-of-experts (Lepikhin et al., 2021), low-complexity attention (Kitaev et al., 2020), and multi-query attention (Shazeer, 2019). However, they usually require a significant re-training cost. In contrast, compression methods require a smaller amount of fine-tuning cost by reducing the complexity of pre-trained LLMs, such as quantization (Frantar et al., 2022) and weight or activation sparsification (Mishra et al., 2021; Zaheer et al., 2020). At the system level, prior work (1) optimizes the computational graph (Dao et al., 2022), (2) op- timizes the assignment and scheduling of computational graph on devices (Sheng et al., 2023), or (3) designs batching or caching mechanisms for serving multiple users (Fang et al., 2021). These techniques address the large memory access and footprint posed by the vast model scale and atten- tion mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency. As SoT trades off throughput for end-to-end latency, SoT can make these throughput-oriented tech- niques help with end-to-end latency . This interesting synergy offers opportunities for achieving better trade-offs between latency and throughput in future serving systems. In contrast to model- and system-level techniques, SoT is a data-level technique in a new “content co-organization for efficiency” paradigm. See § 6 for more discussions. Efficient LLM methods through parallel generation.Some prior work also addresses the sequen- tial decoding issues. Speculative decoding (SD) methods (Stern et al., 2018) employ smaller models to generate some consecutive tokens sequentially and apply the target LLMs to verify them paral- lelly. Non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023) sample and refine consecutive tokens parallelly, often with the support of a modified and tuned model. 8Published as a conference paper at ICLR 2024 Relying on either assisting models or special models and sampling schemes, SD and NAG methods conduct parallel verification or sampling and refinement of consecutive tokens . In contrast, SoT prompts the LLM itself to plan the contents in a way that permitsthe parallel generation of tokens in different segments, by exploiting the emerging instruction-following and planning ability of LLMs. Prompting methods for LLMs. Recent years have witnessed the emergence of the “pre-train, prompt, and predict” paradigm, which has shown promise in enhancing LLMs’ quality in math and commonsense reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Chen et al., 2022) and planning for multi-modality tasks (Shen et al., 2023; Zhu et al., 2023). Instead of focusing on answer quality, SoT is a first attempt at exploiting the power of prompting to improve efficiency. 6 L IMITATIONS , FUTURE WORK , AND OPEN QUESTIONS Answer quality evaluation. Our answer quality evaluation is far from perfect due to the limited prompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM gener- ations. Currently, we did not conduct human evaluation since it is easy for a human to tell whether an answer is generated with SoT due to its distinctive pattern, which might cause evaluation bias. Eliciting or improving LLMs’ ability. § 3.2.4 demonstrates SoT’s potential of enhancing answer quality. It is part of a broader trend in recent research, exemplified by work including CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022), which collectively affirm the notion that explicitly articulating the thought process in language can elicit high-quality answers from LLMs . These findings resemble human thinking: rather than relying solely on the first intuition or purely sequential thinking, we often document step-by-step reasoning or thought organization to attain high-quality answers. This intriguing parallel prompts us to explore further how we can draw from the human thinking process to facilitate more effective and efficient AI. For instance, SoT currently ignores the dependencies between points. A conceptually better way is to organize the points as Graph-of-Thoughts, where the edges represent the dependencies, and each point is decoded conditioned on the contents of its ancestor points. In addition, instead of complying with a static graph, we expect the need of having dynamic Graph-of-Thoughts, where the high-level thought structure is adjusted dynamically by LLMs themselves. This could potentially combine the efficiency and global thinking advantages of SoT with the logical reasoning and impromptu think- ing strengths of methods like CoT (Kojima et al., 2022; Wei et al., 2022). Notably, a contemporary work (Besta et al., 2023) has attempted to design Graph-of-Thoughts to elicit reasoning. Further- more, it is interesting to explore how the SoT answers can be used to fine-tune LLMs to generate more structured answers in a self-improving way (Zelikman et al., 2022; Huang et al., 2022). Efficiency and overhead of SoT in different scenarios. Serving systems commonly adopt batch processing to handle concurrent queries. This raises a concern of whether SoT may hurt serving throughput due to parallel requests. (1) When there is an unsaturated number of concurrent queries, SoT can effectively reduce latency and enhance GPU utilization. Example scenarios include (a) Edge-side applications with a single user; (b) Centralized services during periods with unsaturated user requests and underutilized computing capacity. It is interesting to study the appropriate SoT triggering conditions based on system workloads. (2) When there is a saturated number of concur- rent queries, SoT is still useful for improving answer quality. However, in this case, it is important to consider the computation overhead from SoT. We delve into this concern in App. H. For API-based models, a notable concern arises regarding the increased number of prefilling tokens (App. H). Given that many APIs charge token usage, SoT may lead to higher costs. To address this, one can use prompt tuning to design shorter SoT prompts (Jiang et al., 2023). Data-centric efficiency optimization. While data-centric engineering for improving answer qual- ity (Zha et al., 2023; HazyResearch, 2023) is gaining popularity, its potential forinference efficiency is not explored yet. SoT is the first attempt. As LLM capabilities and the amount of LLM-generated data are growing rapidly, data-centric techniques could become more useful in the future. To pave the way towards that, there are a lot to explore. For example, the acceleration ratio of SoT depends on the SoT prompt, the model, and the question, and thus not as predictable and controllable as model- or system-level techniques, which might hinder the practical adoption. We look forward to future work to unlock the full potential of data-centric efficiency optimization. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGEMENTS We thank Sergey Yekhanin (Microsoft Research), and Tianji Wu (Infinigence AI) for their support and suggestions on the work. We thank Tianyu Fu for many initial discussions on the idea. We thank Ke Hong and Genghan Zhang for their discussions about profiling. We thank Yue Wu for the help on the Claude scripts. We thank Da Yu, Chulin Xie, and Saiqian Zhang for their suggestions on revising the first version of the paper. We thank Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze Sun, Jun Yan, Zhi Zhang, Yuxuan Tong, Nianhui Guo, and Andrea Santilli for their suggestions on revising the second version of the paper. We thank Chris Stetkiewicz, Amanda Melfi, and Amber Tingle from Microsoft for their suggestions and help on writing. We thank the anonymous reviewers for their insightful questions and suggestions. REFERENCES Anthropic. Introducing claude, May 2023. URL https://www.anthropic.com/index/ introducing-claude. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019. Harrison Chase. LangChain, October 2022. URL https://github.com/hwchase17/ langchain. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023a. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt- ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie. Dynamic n: M fine-grained structured sparse attention mechanism. In Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pp. 369–379, 2023b. Zhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming Chen, Fei Yu, Tiannan Wang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Haizhou Li, and Benyou Wang. Llm zoo: democratizing chatgpt. https://github.com/FreedomIntelligence/ LLMZoo, 2023c. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/ . Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod- els. arXiv preprint arXiv:2210.11416, 2022. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory- efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022. 10Published as a conference paper at ICLR 2024 Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. Advances in neural information processing systems, 27, 2014. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320–335, 2022. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: an efficient gpu serv- ing system for transformer models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pp. 389–402, 2021. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research , 23(1): 5232–5270, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing large-scale transformer-based mod- els: A case study on bert. Transactions of the Association for Computational Linguistics , 9: 1061–1080, 2021. Joao Gante. Assisted generation: a new direction toward low-latency text generation. https: //huggingface.co/blog/assisted-generation, 2023. Accessed: 2023-06-23. Google. Tensorflow serving, 2021. URL https://github.com/tensorflow/serving. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive neural machine translation. InInternational Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1l8BtlCb. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. HazyResearch. Data-centric ai. https://github.com/HazyResearch/ data-centric-ai, 2023. Accessed: 2023-07-04. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems , 32, 2019. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711–732, 2021. 11Published as a conference paper at ICLR 2024 Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, December 2023. URL https://arxiv.org/abs/2310.05736. Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with condi- tional computation and automatic sharding. In International Conference on Learning Represen- tations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. arXiv preprint arXiv:2211.17192, 2022. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for ”mind” exploration of large scale language model society, 2023a. Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1106–1115, 2015. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.arXiv preprint arXiv:2101.00190, 2021. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5315– 5333, 2023c. Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. Terapipe: Token-level pipeline parallelism for training large-scale language models. In Interna- tional Conference on Machine Learning, pp. 6543–6552. PMLR, 2021. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. 12Published as a conference paper at ICLR 2024 Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language pro- cessing. ACM Computing Surveys, 55(9):1–35, 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer- ence on Learning Representations, 2019. Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li. Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks. In 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 553–564. IEEE, 2017. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gre- gory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline par- allelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pp. 1–15, 2019. Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning, pp. 7937–7947. PMLR, 2021. NVIDIA. Fastertransformer, 2019. URL https://github.com/NVIDIA/ FasterTransformer. NVIDIA. Triton inference server, 2021. URL https://developer.nvidia.com/ triton-inference-server. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. Duy Phung. Stablevicuna-13b, May 2023. URL https://huggingface.co/CarperAI/ stable-vicuna-13b-delta . Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pp. 6908–6915, 2019. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Perfor- mance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Min- jia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: Democratizing {Billion-Scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551–564, 2021. 13Published as a conference paper at ICLR 2024 Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Ric- cardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via paral- lel decoding. In acl, 2023. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. SenseTime. Lightllm. https://github.com/ModelTC/lightllm, 2023a. Accessed: 2023-09-26. SenseTime. Openppl. https://github.com/openppl-public/ppl.nn, 2023b. Ac- cessed: 2023-09-26. Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse text generation with planning-based hierarchical variational model. arXiv preprint arXiv:1908.06605, 2019. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023. Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4222–4235, 2020. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore- gressive models. Advances in Neural Information Processing Systems, 31, 2018. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu, Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative decoding via optimal transport. In Workshop on Efficient Systems for Foundation Models @ ICML2023 , 2023. URL https: //openreview.net/forum?id=d0mGsaheuT. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink- ing the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: A strong, replicable instruction-following model. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023. Accessed: 2023- 06-23. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, 14Published as a conference paper at ICLR 2024 Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. Openllms: Less is more for open-source models, July 2023a. URL https://github.com/imoneoi/openchat. Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 97–110. IEEE, 2021. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, and Matthew B Blaschko. Dice semimetric losses: Optimizing the dice score with soft labels. In Medical Image Computing and Computer Assisted Intervention, 2023b. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners.arXiv preprint arXiv:2109.01652, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {Transformer-Based} generative models. In 16th USENIX Sympo- sium on Operating Systems Design and Implementation (OSDI 22), pp. 521–538, 2022. 15Published as a conference paper at ICLR 2024 Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158, 2023. Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu, and Yibo Zhu. Bytetransformer: A high-performance transformer boosted for variable-length inputs. arXiv preprint arXiv:2210.03052, 2022. Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023. Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and {Intra- Operator} parallelism for distributed deep learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 559–578, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment, 2023. Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. {PetS}: A unified framework for {Parameter-Efficient} transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pp. 489–504, 2022. Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023. Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In Interna- tional Conference on Learning Representations (ICLR), 2017. 16Published as a conference paper at ICLR 2024 Appendix Table of Contents A Model Details 18 B Implementation Details of Skeleton-of-Thought 18 B.1 Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2 Supporting Multi-Round Conversation . . . . . . . . . . . . . . . . . . . . . . 20 C Implementation Details of Skeleton-of-Thought with Router 20 C.1 Prompting Router . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2 Trained Router . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.3 Router Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.4 Concurrent execution for SoT-R . . . . . . . . . . . . . . . . . . . . . . . . . . 21 D SoT In the Context of Literature (Expanded) 22 D.1 Efficient LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 D.2 Prompting Methods for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 D.3 Hierarchical Text Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E Efficiency Analysis 24 F Efficiency Profiling 26 G Efficiency Evaluation 27 G.1 Skeleton-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 G.2 Skeleton-of-Thought with Router . . . . . . . . . . . . . . . . . . . . . . . . . 29 H Overhead of SoT in Different Scenarios 31 I Answer Quality Evaluation 32 I.1 Skeleton-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 I.2 Skeleton-of-Thought with Router . . . . . . . . . . . . . . . . . . . . . . . . . 44 I.3 Quality Comparison with Longer Normal Answer . . . . . . . . . . . . . . . . 46 I.4 ChatGPT-3.5 as the Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 J Combining SoT-R with Model Quantization 48 J.1 Speed-ups of SoT + Quantization on Quantized Models . . . . . . . . . . . . . 49 J.2 Speed-ups of SoT + Quantization on Unquantized Models . . . . . . . . . . . . 49 K Additional SoT-R statistics 50 K.1 Number of Suitable Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 K.2 Peak Memory Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 K.3 Speed-ups with Different Number of Points . . . . . . . . . . . . . . . . . . . . 51 L Notes on Application Scenarios 51 17Published as a conference paper at ICLR 2024 A M ODEL DETAILS Table 1 summarizes the models on which we evaluate SoT. We use GPT-4 in the main paper and ChatGPT-3.5 in App. I.4 as the judge in FastChat and LLMZoo evaluation. Table 1: Models evaluated with SoT. All the open-source models are fine-tuned from LLaMA mod- els. Access Model Name Institution Released Date Open-Source LLaMA2-Chat-7B (Touvron et al., 2023b) Meta & Microsoft 2023/07 LLaMA2-Chat-13B (Touvron et al., 2023b) Meta & Microsoft 2023/07 OpenChat-13B (Wang et al., 2023a) Tsinghua 2023/07 Vicuna-7B V1.3 (Chiang et al., 2023) LMSYS 2023/06 Vicuna-13B V1.3 (Chiang et al., 2023) LMSYS 2023/06 Vicuna-33B V1.3 (Chiang et al., 2023) LMSYS 2023/06 StableVicuna-13B (Phung, 2023) CarperAI 2023/05 UltraLM-13B (Ding et al., 2023) OpenBMB & Tsinghua 2023/05 Vicuna-7B V1.1 (Chiang et al., 2023) LMSYS 2023/03 API-Based Claude (Anthropic, 2023) Anthropic 2023/05 ChatGPT-3.5 OpenAI 2022/11 GPT-4 OpenAI 2023/03 Table 2 shows sources of the models we use in the paper. Table 2: The Hugging Face or API endpoints of the models. Access Model Name Hugging Face or API Endpoints Open-Source LLaMA2-Chat-7B (Touvron et al., 2023b) meta-llama/Llama-2-7b-chat-hf LLaMA2-Chat-13B (Touvron et al., 2023b) meta-llama/Llama-2-13b-chat-hf OpenChat-13B (Wang et al., 2023a) openchat/openchat Vicuna-7B V1.3 (Chiang et al., 2023) lmsys/vicuna-7b-v1.3 Vicuna-13B V1.3 (Chiang et al., 2023) lmsys/vicuna-13b-v1.3 Vicuna-33B V1.3 (Chiang et al., 2023) lmsys/vicuna-33b-v1.3 StableVicuna-13B (Phung, 2023) CarperAI/stable-vicuna-13b-delta2 UltraLM-13B (Ding et al., 2023) openbmb/UltraLM-13b2 Vicuna-7B V1.1 (Chiang et al., 2023) lmsys/vicuna-7b-delta-v1.1 API-Based Claude (Anthropic, 2023) Claude extension on Slack3 ChatGPT-3.5 Azure OpenAI, gpt-35-turbo 0301 version4 GPT-4 OpenAI, gpt-4-0613 version B I MPLEMENTATION DETAILS OF SKELETON -OF-THOUGHT B.1 P ROMPT The skeleton prompt is shown in Prompts 1 and 3 and the point-expanding prompt is shown in Prompt 2. Skeleton prompt template. In order to make the output skeleton short and in a consistent format for the good of efficiency and ease of point extraction, the skeleton prompt template (1) describes the 2For convenience, we use the non-official endpoint TheBloke/stable-vicuna-13B-HF and TheBloke/UltraLM-13B-fp16 to get merged weights. 3https://www.anthropic.com/claude-in-slack 4https://azure.microsoft.com/en-us/products/ai-services/openai-service 18Published as a conference paper at ICLR 2024 Prompt 3. Skeleton Prompt Template Ts (with Two-Shot Demonstrations) [User:] You’re an organizer responsible for only giving the skeleton (not the full content) for answering the question. Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence, each skeleton point should be very short with only 3 ∼5 words. Generally, the skeleton should have 3 ∼10 points. Question: What are the typical types of Chinese dishes? Skeleton: 1. Dumplings. 2. Noodles. 3. Dim Sum. 4. Hot Pot. 5. Wonton. 6. Ma Po Tofu. 7. Char Siu. 8. Fried Rice. Question: What are some practical tips for individuals to reduce their carbon emissions? Skeleton: 1. Energy conservation. 2. Efficient transportation. 3. Home energy efficiency. 4. Reduce water consumption. 5. Sustainable diet. 6. Sustainable travel. Now, please provide the skeleton for the following question. {question} Skeleton: [Assistant:] 1. task precisely, and (2) provides a partial answer “1.” for the LLM to continue writing. The skeleton responses are in the desired format in most cases. Therefore, we can use a simple regular expression (\\d+)\\.\\s?([\\s\\S]+?)(?=\\n|\\n*$) to extract point indexes and point skeletons from the skeleton response. We find that GPT-4 can work well without the two demonstrations in the skeleton prompt. Therefore, we do not include the two demonstrations for GPT-4 (Prompt 1). For all other models, the two demonstrations are included, as shown in Prompt 3. Point-expanding prompt template. It describes the point-expanding task and provides a partial answer. We also provide instructions “Write it **very shortly** in 1∼2 sentence” so that the LLMs keep the answers concise. Unlike the skeleton prompt template, we find that demonstrations are not necessary to get reasonable results. We find that Claude and GPT-4 follows the instruction “Write it **very shortly** in 1 ∼2 sentence and do not continue with other points!” in Prompt 2 very well, so that the answers are very short. Therefore, we delete “**very shortly**” from the prompt template in Claude and GPT-4. Partial answer. In the Prompts 1 and 2, we provide partial answers so that LLMs can follow the desired response format better. We can put the partial answer at the end of the prompt for the open-source models to continue writing. An implementation detail is that different open-source models have different conversa- tion templates (i.e., different ways to combine user and assistant messages into one string). For example, Vicuna (Chiang et al., 2023) uses the string “USER:” and “ ASSISTANT:” for the place- holder “[User:]” and “ [Role]” in the Prompts 1 and 2, respectively, while UltraLM (Ding et al., 2023) uses “User:” and “ 〈/s〉Assistant:”. We build our open-source model experiments with the help of the FastChat codebase (Zheng et al., 2023), in which the conversation templates of many models are already handled correctly. We implement the conversation templates of OpenChat-13B, StableVicuna-13B, and UltraLM-13B according to their official guides and codes. For ChatGPT-3.5, we provide partial answers as a last message in the chat history from the assistant. Note that it is not a documented approach. We find it works well in most cases, in that ChatGPT-3.5 19Published as a conference paper at ICLR 2024 Prompt 4. LLM Prompting as the Router [User:] Question: {question} How would you like to answer the question? A. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the points or perspectives can be answered independently without referring to the contents of the previous points. B. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the contents of later points or perspectives cannot be answered independently without referring to the contents of the previous ones. C. Do not organize the answer as a list of points or perspectives. Just say A, B, or C. Do not explain. Do not provide an answer to the question. [Assistant:] continues the texts from the provided partial answer. However, in some rare cases, ChatGPT-3.5 repeats the provided partial answers. For Claude over Slack, there is no obvious way to give the API a partial answer. We resort to modifying the prompt template slightly by adding Please start your answer from “{partial answer}” and do not output other things before that at the end. We find that Claude understands and obeys it well. For GPT-4, we also take this approach. System Message. We do not include the system message in the prompts for open-source models except LLaMA2. The partial answer, “**very shortly**”, and the 2-shot demonstrations discussed above are the only differences between the prompts we used across all models and all evaluations. B.2 S UPPORTING MULTI -ROUND CONVERSATION To use SoT in a multi-round conversation, we can just put the question and the final aggregated answer in the history, removing all the SoT prompts. In this way, using SoT in one conversation round will not introduce additional prefill cost in future rounds. C I MPLEMENTATION DETAILS OF SKELETON -OF-THOUGHT WITH ROUTER C.1 P ROMPTING ROUTER We use Prompt 4 for querying GPT-4 as the router. If the answer is “A” (i.e., the question can be answered in a list of independent points), we will use SoT. Otherwise, if the answer is “B” (i.e., the answer is in a list of points but they depend on each other) or “C” (i.e., the answer should not be in a list of points), SoT is not suitable and we will fall back to normal decoding. C.2 T RAINED ROUTER We tackle the routing problem as a sequence classification task. We first annotate the LIMA training set (Zhou et al., 2023), and then fine-tune a RoBERTa model (Liu et al., 2019) using the labeled data. Finally, we apply the tuned RoBERTa as the router on Vicuna-80 and WizardLM. We detail the steps in the following. C.2.1 A NNOTATION PROCESS In the classification task, a label of 1 (positive) indicates that this question can be answered with SoT, while a label of 0 (negative) suggests that using the normal generation mode is more suitable. We annotate the LIMA training set, which consists of 1,030 Q&As sourced from three community webpages: Stack Exchange, wikiHow, and the Pushshift Reddit. We also annotate the Vicuna-80 and WizardLM datasets for evaluation. 20Published as a conference paper at ICLR 2024 Table 3: Router confusion matrices on the Vicuna-80 dataset. Left: Rows are human annotations (H) and columns are the GPT-4 router (G). Middle: Rows are human annotations (H) and columns are the RoBERTa router (R). Right: Rows are the GPT-4 router (G) and columns are the RoBERTa router (R). G0 G1 H0 38 5 H1 0 37 R0 R1 H0 37 6 H1 5 32 R0 R1 G0 34 4 G1 8 34 Table 4: Router confusion matrices on the WizardLM dataset. Left: Rows are human annotations (H) and columns are the GPT-4 router (G). Middle: Rows are human annotations (H) and columns are the RoBERTa router (R). Right: Rows are the GPT-4 router (G) and columns are the RoBERTa router (R). G0 G1 H0 94 66 H1 3 55 R0 R1 H0 135 25 H1 31 27 R0 R1 G0 93 4 G1 73 48 We use GPT-4 to assist the annotation process. Specifically, we present each question to GPT-4 and analyze its answer to determine whether SoT can be triggered for this question. We assign a positive label to a question if GPT-4’s response meets two criteria: (1) it contains a list of points that can be expanded in parallel, (2) each point provides sufficient details (i.e., the point-expanding response is not too short), which will enable SoT to achieve a speed-up. Two of the paper’s authors conduct the annotation process independently, and discuss the inconsistent annotations to decide the final label. C.2.2 T RAINING DETAILS We use roberta-base with 120M parameters as the router model. The finetuning is conducted using the AdamW optimizer (Loshchilov & Hutter, 2019) with a weight decay of 0.01. The learning rate undergoes a warm-up phase during the first 1% of iterations to 5e-5 and then decays linearly. We train the model for 2 epochs using a batch size of 32. Input sequences are either padded or truncated to achieve a consistent length of 512 tokens. In the application of SoT, false positives (SoT is incorrectly triggered when it should not be, resulting in degraded answer quality) are of more significant concern than false negatives (the router misses a potential SoT trigger, resulting in a reduced speed-up). Thus, to mitigate false positives, we employ the Tversky loss (Wang et al., 2023b) with parameters α = 0.7 and β = 0.3, which penalizes false positives more heavily than false negatives. We also incorporate label smoothing (Szegedy et al., 2016) with a factor of ϵ = 0.2. Overall, the entire fine-tuning process is efficient, completing in 2 minutes on an NVIDIA A100 GPU. C.3 R OUTER CONSISTENCY We present the confusion matrices for the three routers to illustrate their consistency. The results on Vicuna-80 and WizardLM are shown in Tables 3 and 4, respectively. On Vicuna-80, we can observe a notable level of agreement among the three routers. Compared with the GPT-4-prompting router, the trained router exhibits a slightly higher number of false negatives w.r.t. the human annotations. Conversely, on WizardLM, given the intricate answer structure and the presence of many ambiguous cases, the routers show significant discrepancies. Specifically, the GPT-4 router produces many false positives, which pose adverse affects on the answer quality (see App. I.2). The RoBERTa router aligns more closely with the human annotations. C.4 C ONCURRENT EXECUTION FOR SOT-R In SoT-R, the router serves as an additional stage that extends the two-stage SoT pipeline, as illus- trated in Fig. 9. To push the limit of latency optimization, we can run the router, normal generation, and SoT generation concurrently. Once the router makes a decision, one of the normal and SoT generation processes can be aborted. However, this approach will increase the token overhead. Therefore, we did not employ this approach in this work and leave it to future work. 21Published as a conference paper at ICLR 2024 Router  Skeleton Expand positive  negative Decode  Question  Answer  Answer  Router  Skeleton Expand  Decode  Question  Answer  Answer  positive  negative  Figure 9: Left: The SoT-R pipeline. Right: A possible approach to further reduce latency at the cost of token overhead. D S OT IN THE CONTEXT OF LITERATURE (EXPANDED ) D.1 E FFICIENT LLM S Extensive research has been dedicated to enhancing the throughput and latency of LLM infer- ence. We first discuss model-level architecture design or compression techniques. These techniques change the model and can benefit both the latency and throughput but require finetuning to retain the model quality. Then, we discuss system-level efforts that optimize the computational graph or the assignment and scheduling of the computational graph on computation and storage devices. Most system-level efforts accelerate the prefilling phase or focus on improving the throughput. Finally, we discuss some research efforts that share a similar motivation to ours, namely, addressing the efficiency issue of sequential decoding. Model-level optimization. Considerable architectural design efforts have emerged to (1) improve the scalability w.r.t. model size by introducing mixture-of-expert inference (Lepikhin et al., 2021; Fedus et al., 2022), (2) address the quadratic complexity w.r.t. input size of attention by designing new attention mechanisms (Kitaev et al., 2020; Wang et al., 2020), (3) reduce the memory access and footprint of attention by using multi-query attention (Shazeer, 2019), and so on. However, these methods usually require a substantial re-training cost. The model compression techniques require a smaller amount of fine-tuning by reducing the model complexity of a pre-trained LLM from certain aspects (Ganesh et al., 2021). Representative techniques include quantization (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023), the static or dynamic pruning of weights, activation, and attention (Mishra et al., 2021; Zaheer et al., 2020; Wang et al., 2021; Chen et al., 2023b), and so on. Zooming out from LLM compression to the whole field of model compression, we can see that model co-design or compression for efficiency has received tremendous attention in the past few years and has grown into large research fields, such as pruning (Han et al., 2015; Wen et al., 2016), quantization (Krishnamoorthi, 2018), factorization (Denton et al., 2014), and neural architecture search (Zoph & Le, 2017; Elsken et al., 2019; Cai et al., 2019). Different from the model co-design paradigm, SoT is in a “ content co-organization for efficiency ” paradigm for improving the LLM efficiency. Along with the growth in the LLM capabilities and amount of LLM-generated data, data-level techniques could become important tools in the efficient LLM toolbox. System-level optimization. In the realm of lossless acceleration, considerable efforts have been devoted to addressing the I/O-bound nature of LLMs on modern hardware platforms (Dao et al., 2022). Numerous studies (Dao et al., 2022; Zhai et al., 2022; Ivanov et al., 2021; NVIDIA, 2019) have focused on adjusting the computational graph by fusing and implementing operations in an I/O-friendly way. As a representative method, FlashAttention (Dao et al., 2022) fuses all operations of one attention into one GPU kernel with spatially tiled computation to reduce the off-chip I/O of the attention map. While FlashAttention can effectively accelerate training and the prefilling phase of inference, it cannot accelerate the decoding phase much (when the batch size is small), as it is the I/O of weights rather than activation or attention map that bottlenecks the decoding phase. For example, when the context length is 64, decoding one token using LLaMA-7B needs to load each of the 7B parameters from the off-chip HBM onto the GPU chip at least once, but only transferring about 20M (0.02B) activation values between the off-chip HBM and GPU chip. In order to satisfy Service Level Objectives, serving systems focus on improving the serving throughput under latency constraints. To this end, serving systems (Fang et al., 2021; NVIDIA, 22Published as a conference paper at ICLR 2024 2021; Google, 2021) pack multiple queries together into a batch to improve the hardware utiliza- tion. The batching technique has proven highly effective in enhancing throughput, leading to the development of various variants. For example, some work designs methods to decide which queries to batch together (Fang et al., 2021; Zhou et al., 2022), while others selectively batch parts of the model to enable fine-grained iteration-level batching (Yu et al., 2022) or multi-task batching (Zhou et al., 2022). Various model parallelism (Lu et al., 2017; Huang et al., 2019; Narayanan et al., 2019; Rajbhandari et al., 2020; Narayanan et al., 2021; Li et al., 2021; Zheng et al., 2022) and offloading (Ren et al., 2021; Sheng et al., 2023) techniques have been proposed to maximize the throughput of LLM training or inference. In a nutshell, given the computational graph and device configurations, these techniques optimize the split, assignment, and scheduling of computations, storage, and communications on devices. In addition to the model parallelism and batching tech- niques, an efficient memory management mechanism for LLM workloads is also an essential feature in the serving systems (Kwon et al., 2023; SenseTime, 2023a;b). To sum up, these system-level techniques mainly help with the throughput in training and batched inference. They can be used by SoT to improve the throughput of the batched decoding of multiple segments. This means that SoT can harness the power of these throughput-oriented techniques and make them help with the end-to-end latency, offering a new dimension for better trading off latency and throughput in future serving systems. Another parallelism perspective to position SoT is that SoT guides the LLM to adjust the sequen- tial workload to become “inter-content” parallelizable , which differs from the parallelism levels in existing serving systems, including inter-instance (Krizhevsky, 2014; Rajbhandari et al., 2020), inter-operation (Huang et al., 2019; Narayanan et al., 2019; 2021), intra-operation (Xu et al., 2021), and inter-token (Li et al., 2021). It may be worthwhile to explore the integration of SoT into serving systems to maximize the hardware utilization. Decoding optimization. One bottleneck for the end-to-end latency lies in the autoregressive de- coding phase, where tokens must be generated one by one. Due to the dependency between tokens, the computation of different tokens cannot be parallelized, causing severe under-utilization of GPU. In order to improve the end-to-end decoding latency of a given LLM, speculative decoding meth- ods (Stern et al., 2018; Leviathan et al., 2022; Chen et al., 2023a; Gante, 2023; Sun et al., 2023; Miao et al., 2023) propose to use cheaper approaches to generate short candidate token sequences, for example, by sequentially decoding with an assisting model much smaller than the given LLM. Then, they use the LLM to parallelly verify the candidates and keep the prefix sequence that matches the LLM’s verification results. Another line of work that shares the motivation of addressing the autoregressive efficiency issue is non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023). NAG methods sample consecutive tokens parallelly, often with the aid of a modified and tuned model. To maintain the answer quality, instead of sampling for one iteration, many NAG methods refine the output parallelly for multiple iterations (Xiao et al., 2023; Santilli et al., 2023). To summarize, the speculative decoding methods use assisting models for letting the LLM conduct parallel verification of consecutive tokens, and the NAG methods rely on specially designed models, training schemes, or sampling schemes for the parallel sampling and refinement of consecutive to- kens. In contrast, SoT prompts the LLM itself to plan the contents in a way that permits the parallel generation of multiple tokens in different segments. SoT exploits the emerging instruction-following and planning ability of SoTA LLMs rather than relying on specially designed modeling, sampling, and training schemes. This is different from all existing work that targets the autoregressive effi- ciency issue. D.2 P ROMPTING METHODS FOR LLM S In recent years, the “pre-train, prompt, and predict” paradigm has emerged (Liu et al., 2023), which designs prompts comprising task descriptions and (optionally) a few demonstrations to guide pre- trained LLMs in generating answers for a wide range of downstream tasks. Researchers found that instruction-tuned LLMs (Brown et al., 2020; Wei et al., 2021; Ouyang et al., 2022; Chung et al., 2022; Taori et al., 2023) possess a strong ability to (1) generalize to new tasks thanks to the diverse 23Published as a conference paper at ICLR 2024 natural language descriptions encountered during instruction tuning, and (2) learn in-context using a few demonstrations without weight tuning. In virtue of these abilities, the field has been manually engineering (Brown et al., 2020; Kojima et al., 2022; Shen et al., 2023; Li et al., 2023a), automatic searching (Shin et al., 2020), or continu- ously tuning (Li & Liang, 2021; Lester et al., 2021) the prompts for uncovering the capabilities of LLMs on downstream tasks. There are a bunch of prompting methods that improves the reasoning performance of LLMs by designing thinking flows mimicking human reasoning: (1) mimicking the step-by-step or compositional thinking structure (Wei et al., 2022; Kojima et al., 2022; Press et al., 2022; Yao et al., 2023; Besta et al., 2023; Zhang et al., 2023), (2) designing multiple reasoning paths and their aggregation (Wang et al., 2022; Yao et al., 2023; Li et al., 2023c), and (3) using tools for calculation and information retrieval (Chen et al., 2022; Yao et al., 2022; Schick et al., 2023). As a representative example, the Chain-of-Thought prompts largely improve the performance on tasks that require logical reasoning by simply providing a “Let’s think step by step” (Kojima et al., 2022) instruction or a few demonstrations (Wei et al., 2022). Another topic that arises quite a surge of in- terests is to prompt LLMs to help finish complex multi-modality task (Shen et al., 2023; Zhu et al., 2023). For example, HuggingGPT (Shen et al., 2023) design prompts to guide the LLM to generate structural JSON for the orchestration of multi-model execution to finish complex tasks. To summarize, the large literature on prompting methods has been aiming at uncovering different capabilities of LLM and improving the answer quality on different downstream tasks. In contrast, SoT is a first attempt at exploiting the power of prompting to improve efficiency. D.3 H IERARCHICAL TEXT GENERATION SoT can be regarded as being “hierarchical” since it has high-level answer structure planning. Prior studies in hierarchical text generation (Li et al., 2015; Shao et al., 2019; Puduppully et al., 2019; Fan et al., 2018) all focus on enhancing the answer quality, including improving the long-range coherence, relevance to the topic, or reducing redundancy. These methods craft hierarchical neural architectures that contain different modules to model high-level (sentence-level or document-level) and low-level (word-level) dependencies (Li et al., 2015; Shao et al., 2019; Fan et al., 2018). They still employ sequential word-by-word generation without parallelization between sentences. Note that the sentence-level representations in previous work (Li et al., 2015; Shao et al., 2019) are “implicit” latent variables instead of “explicit” language descriptions. Some previous studies (Shao et al., 2019; Puduppully et al., 2019) train a dedicated planning module to execute explicit content planning in advance. Nevertheless, these methods all conduct “closed-form” planning that only reorders and groups the input keywords, rather than producing “free-form” plans on “what to say” and “how to say”. All the hierarchical architectures and planning modules require training or even special data processing (Puduppully et al., 2019). To summarize, in terms of the objective, the primary focus of SoT – efficient generation – is dif- ferent from previous hierarchical text generation literature. In terms of the methodology, instead of designing new hierarchical architectures or planning modules, SoT exploits the emerging planning and instruction-following abilities of LLMs to do explicit (which means the plan is described by in- terpretable language) and free-form planning. This allows SoT to be applied to off-the-shelf LLMs for producing structured answers. As the hierarchical text generation literature focuses on enhancing answer quality, they could pro- vide inspiration for future expansions of SoT to generate high-quality answers for broader types of questions. E E FFICIENCY ANALYSIS This section gives a detailed explanation on why SoT can reduce the overall decoding latency with the same computational resource for local models. The vanilla approach processes only one question and decodes the answers sequentially, whereas SoT processes multiple point-expanding requests and the answers in a batch. We focus on the following question: “Compared to processing only one sequence, how much peak memory overhead and latency increase will be brought by processing a batch of sequences?” 24Published as a conference paper at ICLR 2024 Table 5: The latency and average GPU performance of the prefilling and decoding phases when inferencing LLMs. The prefilling token length is 128, the decoding token length is 64, and the batch size is 1. The test is run on one NVIDIA A100 GPU. Model Prefill/Decode Latency (ms) Prefill/Decode GPU Perf. (TFLOPS) LLaMA-7B 40 / 2735 43 / 0.31 LLaMA-13B 54 / 3725 62 / 0.44 LLaMA-33B 100 / 5506 85 / 0.75 1 2 3 4 5 6 7 8 9 B 3000 3500 4000 4500 5000 5500 LLaMA-7B LLaMA-13B LLaMA-33B (a) Latency (ms) 1 2 3 4 5 6 7 8 9 B 0 1 2 3 4 5 6 LLaMA-7B LLaMA-13B LLaMA-33B (b) Actual GPU Perf. (TFLOPS) 1 2 3 4 5 6 7 8 9 B 20 30 40 50 60 70 LLaMA-7B LLaMA-13B LLaMA-33B (c) Peak Memory (GB) Figure 10: The trends of latency, average GPU performance of decoding one token, and peak mem- ory with respect to the batch size B of sequences. The prefilling token length is 128, and the decoding token length is 64. The test is run on one NVIDIA A100 GPU. A typical LLM generative process consists of two phases: (1) the prefilling phase in which the prompt is parsed to generate the key-value cache for further use, and (2) the decoding phase in which tokens are generated one by one in a sequential manner. The decoding phase accounts for the majority of the end-to-end latency, especially when generating a long response. As shown in Table 5, when running Vicuna-7B on NVIDIA A100-80G, the actual computing performance is only 0.31 TFLOPS (0.1% utilization) in the decoding phase, compared to 43 TFLOPS (13.8% uti- lization) during prefilling. The utilization is calculated with respect to the FP16 5 tensor core peak performance – 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token is comparable to that of prefilling 128 tokens (40ms). This huge gap in actual computing perfor- mance and thereby the latency arises from the fact that all LLM weights need to be loaded onto the GPU chip at least once only for decoding one token, so the decoding is heavily bottlenecked by the I/O of weights and the GPU computation units cannot be well utilized. When conducting batched decoding, as the sequence batch sizeB increases, the latency of decoding one token for each sequence stays roughly the same (Fig. 10a), as the amount of LLM weights that needs to be loaded onto the chip does not change. As a result, the GPU computation utilization (Actual GPU Performance Peak GPU Performance ) increases almost linearly as B increases (Fig. 10b). In other words, for gener- ating a final answer of length N, if we cut the answer into B segments of length N/B and decode them as a batch, we can get a B× decoding speed-up compared to sequential decoding. Never- theless, in practice, as prefilling longer requests brings some overhead, and the lengths of the B segments could be imbalanced, the actual speed-up of the batched point-expanding stage compared with the original prefilling and sequential decoding process is smaller than B. As for the peak memory overhead, the amount of LLM weights can be one to two orders of mag- nitude larger than that of all the intermediate activations as long as the prefilling token length is not too large, not to mention that most activations do not need to be saved for back-propagation during inference. Therefore, the LLM weights account for the majority of the memory footprint in our test cases. Consequently, as shown in Fig. 10c, the peak memory overhead due to the increasing size of the KV cache and activation grows at a slow pace as the batch size B increases. Thanks to the small peak memory overhead, in all of our experiments, we managed to use one GPU to run SoT without seeking help from other peak memory optimization techniques (e.g., quantization (Frantar et al., 2022; Lin et al., 2023), offloading (Sheng et al., 2023)). 5All of our experiments are run with FP16 inference. 25Published as a conference paper at ICLR 2024 F E FFICIENCY PROFILING We run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090) with CUDA 11.7, using the Hugging Face transformer library 4.28.1 and PyTorch 2.0.1. The host of A100-80G has an Intel Xeon Platinum 8358P CPU and 1T memory. The host of RTX 3090 has an Intel Xeon Gold 6246R CPU and 512G memory. Latency profiling and estimation. For the decoding phase, we denote tD B(k) as the latency of batched decoding the k + 1-th token with batch size B, where the superscript D stands for “decode”. For each batch size B = 1, ··· , 16 and each context length k = 1, ··· , 1024, we use torch.cuda.Event to record the latency of decoding one token. We run each decod- ing three times continuously and take their geometric mean as {tD B(k)}k=1,···,1024;B=1,···,16. For the prefilling phase, we profile the latency of batched prefilling the inputs with token length k in range(1, 700, 10) and batch size B = 1, ··· , 16, and denote it as tP B(k), where the superscript P stands for “prefill”. We run each test seven times continuously, regard the first two times as the warmup tests, and take the geometric mean of the last five times as {tP B(k)}k=1,11,···,691;B=1,···,16. Once we get the latency profiling table, given a request with li tokens and the decoding batch size B, the latency of generating lo tokens can be estimated as: T(li, lo, B) =˜tP B(li) + li+lo−1X k=li tD B(k), (1) where the subscripts i and o stand for “input” and “output”. Note that we only test the prefill- ing latency every ten token lengths (i.e., 1, 11, 21, ··· ) for fast profiling and estimate ˜tP B(li) by tP B(⌊ li 10 ⌋ ×10 + 1). The SoT decoding process consists of two stages: the skeleton stage and the point-expanding stage. Denoting the token length of the skeleton request and skeleton response asls i and ls o, the token length of the longest point-expanding request and the longest point-expanding response as lpe i and lpe o , the number of the points as B, we can compute the latency of the skeleton and point-expanding stages as: Ls(ls i , ls o) =T(ls i , ls o, 1), (2) Lpe(lpe i , lpe o , B) =T(lpe i , lpe o , B). (3) Using the latency profiling table, we can further estimate the average GPU computing performance in FLOPS (i.e., FLOPs per second) of decoding lo tokens with prefilling length li as PD(li, lo, B) = Pli+lo−1 k=li fD B (k) Pli+lo−1 k=li tD B(k) , (4) where fD B (k) denotes the FLOPs of decoding one token with context length k, which is calculated by DeepSpeed’s FLOPs profiler6. Fig. 10b reports the average GPU computing performance during the process of decoding 64 tokens (prefilling length=128), i.e., PD(128, 64, B). Memory profiling and evaluation. To evaluate the peak memory, we use torch.cuda.max_memory_allocated to record the memory consumption of prefill- ing sequences of different lengths and decoding with different context lengths and a batch size ranging from 1 to 16. Then, we calculate the peak memory of each stage as the maximum value of the prefilling and decoding phases, and calculate the overall peak memory of SoT as the maximum value of the skeleton and point-expanding stages. 6https://deepspeed.readthedocs.io/en/latest/flops-profiler.html 26Published as a conference paper at ICLR 2024 G E FFICIENCY EVALUATION G.1 S KELETON -OF-THOUGHT G.1.1 D ETAILED STATISTICS OF TOKEN LENGTHS AND POINT NUMBERS Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 6.4 5.3 4.4 7.3 4.9 7.9 6.6 9.7 7.1 5.0 4.7 5.3 8.6 4.4 3.7 3.3 5.7 5.0 4.0 3.7 5.7 5.0 3.3 3.7 4.7 5.3 6.3 4.9 5.6 5.4 5.3 6.5 6.9 10.0 5.8 5.5 6.4 4.8 8.5 7.4 6.7 6.3 6.7 6.0 7.0 7.3 9.9 9.1 5.3 8.6 6.3 9.9 7.5 5.9 6.6 8.2 6.3 8.0 7.8 8.8 8.1 5.8 8.3 5.9 9.8 7.4 7.5 5.9 5.9 6.3 7.5 8.6 9.4 8.1 6.4 7.9 6.1 9.4 7.8 6.3 6.2 7.4 6.7 8.4 8.6 9.7 9.2 6.4 7.9 6.7 9.5 6.8 5.0 6.1 6.1 4.9 9.1 7.7 8.4 8.3 4.4 7.3 4.9 9.5 6.8 6.0 5.5 5.5 4.8 8.6 7.8 9.2 8.8 4.1 7.3 5.1 9.3 6.8 5.7 5.6 6.5 5.6 7.4 7.2 9.0 7.7 5.1 6.9 5.5 8.9 4.0 5.0 6.0 7.0 8.0 9.0 10.0 (a) The number of points B. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 372.4 374.0 462.7 386.9 459.7 394.9 384.9 300.3 338.4 381.4 338.6 343.0 304.3 173.5 177.3 208.0 95.0 254.7 159.7 255.0 83.0 273.7 156.7 137.0 139.7 142.7 391.8 396.6 350.3 453.0 382.8 429.6 465.3 398.1 272.1 402.1 417.6 333.8 400.2 311.4 368.5 356.4 273.4 338.2 285.4 431.7 155.7 361.0 254.3 304.7 235.3 372.5 409.8 436.6 478.1 373.9 397.9 404.1 440.4 260.4 325.2 386.0 464.4 366.6 583.6 401.0 470.6 488.6 468.9 377.1 369.8 497.8 266.7 376.8 341.1 352.9 320.2 481.8 372.7 468.4 469.8 417.2 328.1 341.1 476.3 194.2 417.8 321.8 361.3 231.7 444.3 319.0 285.4 419.5 303.3 245.5 332.1 501.9 198.2 399.7 252.3 404.9 173.4 311.3 335.6 424.5 487.9 326.0 324.9 307.6 479.6 169.6 337.9 285.9 303.7 206.1 373.5 343.0 378.0 413.5 344.2 345.4 336.0 437.0 225.1 344.7 309.1 342.8 261.1 379.4 100.0 200.0 300.0 400.0 500.0 (b) The normal answer length. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 114.9 92.6 126.1 151.7 143.0 124.7 104.9 216.0 120.6 128.4 48.0 56.1 67.0 95.0 104.3 94.3 27.3 162.0 189.0 101.7 79.3 98.7 108.0 64.7 45.3 65.7 116.0 117.2 117.1 170.7 188.9 106.0 126.0 163.3 105.8 80.4 65.0 78.0 74.0 89.0 93.1 108.2 63.4 102.0 100.3 118.7 123.4 76.9 71.8 59.4 66.1 84.6 97.2 94.4 114.5 161.9 125.1 92.6 118.1 89.5 90.2 85.8 53.8 61.3 79.4 94.1 99.9 101.0 98.1 110.5 108.9 114.0 117.8 90.9 81.1 61.2 62.9 83.0 86.0 86.3 108.5 106.6 108.6 89.6 105.4 87.3 81.3 76.8 51.3 55.5 75.1 93.5 106.2 103.2 101.9 88.9 118.3 113.0 129.2 79.6 75.3 66.6 56.7 83.1 86.9 97.4 100.1 75.4 121.3 100.6 98.3 104.2 88.5 75.6 55.0 57.0 69.7 97.0 99.0 108.1 106.3 127.8 114.4 111.1 123.3 92.5 87.0 58.3 59.9 75.7 50.0 75.0 100.0 125.0 150.0 175.0 200.0 (c) The maximum point-expanding response length. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 0.4 0.3 0.4 0.4 0.3 0.3 0.3 0.9 0.5 0.3 0.1 0.2 0.2 0.8 0.6 0.5 0.7 0.6 1.3 0.5 3.0 0.6 0.7 0.6 0.3 0.5 0.3 0.3 0.4 0.4 0.5 0.3 0.3 0.4 0.5 0.2 0.2 0.2 0.2 0.4 0.3 0.4 0.4 0.3 0.4 0.3 1.2 0.3 0.3 0.2 0.3 0.3 0.3 0.2 0.2 0.7 0.3 0.2 0.6 0.4 0.6 0.2 0.1 0.2 0.1 0.3 0.2 0.2 0.2 0.3 0.3 0.2 0.5 0.3 0.2 0.3 0.2 0.2 0.3 0.2 0.2 0.3 0.4 0.3 0.2 0.5 0.3 0.2 0.1 0.2 0.2 0.3 0.4 0.2 0.4 0.4 0.4 0.2 0.7 0.3 0.4 0.2 0.3 0.3 0.3 0.2 0.2 0.4 0.4 0.3 0.2 0.7 0.4 0.3 0.4 0.3 0.2 0.4 0.3 0.3 0.4 0.4 0.4 0.3 0.9 0.4 0.3 0.2 0.3 0.2 0.5 1.0 1.5 2.0 2.5 (d) The ratio of the maximum point-expanding re- sponse length to the normal answer length. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 30.8 13.5 27.6 50.0 42.1 36.2 24.1 64.7 36.6 43.2 10.3 11.8 9.1 25.2 23.6 18.4 6.3 48.7 59.1 17.5 22.6 31.8 41.6 13.0 8.9 10.4 25.0 21.6 18.9 47.4 49.2 21.8 22.2 35.8 29.8 23.0 12.9 8.7 8.4 16.4 9.9 14.8 17.1 21.0 18.0 18.8 26.6 21.5 15.9 9.1 10.8 12.9 17.9 10.9 14.3 39.1 27.1 17.1 19.8 17.1 22.1 18.0 9.4 8.4 12.2 14.4 9.7 10.6 20.3 17.9 17.2 15.0 19.8 20.4 15.3 8.3 7.4 10.8 15.4 9.0 15.9 27.0 24.1 18.4 16.8 16.2 18.0 15.7 7.6 7.6 8.3 15.9 12.4 11.4 22.3 13.1 23.8 14.0 34.0 19.6 15.1 8.8 5.3 11.5 14.7 10.5 12.1 19.7 27.7 17.6 14.2 19.6 19.0 15.1 7.0 7.4 6.5 19.5 13.5 16.0 27.7 30.1 25.5 18.1 28.5 24.3 22.5 9.6 8.5 10.0 10.0 20.0 30.0 40.0 50.0 60.0 (e) The imbalance degree of point-expanding response lengths (standard deviation of point token lengths). Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3StableVicuna-13B UltraLM-13BVicuna-7B V1.1 Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 1.5 1.2 1.1 1.1 0.8 1.1 1.1 6.8 1.5 0.7 0.5 0.6 1.4 2.1 1.5 1.1 3.4 1.9 2.5 1.0 7.6 1.2 1.0 1.4 1.1 1.8 1.3 1.2 1.7 1.2 1.5 1.2 1.3 1.9 1.9 0.6 0.7 0.9 1.3 1.9 1.6 1.7 1.9 1.3 2.0 1.3 5.5 1.2 1.1 1.4 1.4 1.9 1.7 1.1 1.3 2.8 1.4 1.3 3.5 2.2 3.3 1.0 0.7 0.8 1.0 1.5 1.3 1.0 0.8 1.5 1.7 1.5 3.0 1.5 1.1 1.7 1.0 1.3 1.4 1.0 1.1 1.0 1.7 1.7 1.3 3.5 1.6 1.1 0.9 1.3 1.3 1.7 1.6 1.3 2.0 1.5 2.1 1.3 3.4 1.2 1.1 0.9 1.4 2.0 1.7 1.2 0.9 2.0 1.2 2.0 1.2 4.5 1.7 0.9 2.1 1.2 1.5 1.6 1.3 1.2 1.8 1.4 1.7 1.5 4.3 1.7 1.0 1.1 1.1 1.5 1.0 2.0 3.0 4.0 5.0 6.0 7.0 (f) The ratio of the final SoT answer length to the nor- mal answer length. Figure 11: The statistics of the token lengths and point numbers on the Vicuna-80 dataset. Each row corresponds to one question category, and each column corresponds to one model. G.1.2 L ATENCY BREAKDOWN : S OT STAGES AND PHASES Fig. 12 presents the absolute latencies of normal and SoT generations on Vicuna-80. Again, the speed-ups of SoT compared with normal generation is evident. We can see that the decoding phases predominantly account for the end-to-end latency. Consequently, although SoT has higher prefilling latency in the skeleton stage than the normal generation and introduces additional point-expanding 27Published as a conference paper at ICLR 2024 prefilling latency – which is expected – this has negligible impact on the overall latency and thereby the overall speed-up. 0 5000 10000 15000 20000 25000 30000 35000 40000 Latency (ms) ChatGPT-3.5 StableVicuna-13B Vicuna-7B V1.1 Vicuna-7B V1.3 LLaMA2-Chat-7B Claude UltraLM-13B Vicuna-13B V1.3 OpenChat-13B LLaMA2-Chat-13B GPT-4 Vicuna-33B V1.3 Normal (prefill) Normal (decode) SoT skeleton (prefill) SoT skeleton (decode) SoT point-expanding (prefill) SoT point-expanding (decode) (a) Average latency across all question categories except math and code on different models. 0 5000 10000 15000 20000 Latency (ms) math roleplay counterfactual common-sense coding fermi generic knowledge writing (b) Average latency across all models on different question categories. Figure 12: The latency breakdown of SoT and normal generations on the Vicuna-80 dataset. For open-source models, the latency breakdown of the prefilling and decoding phases is shown in dif- ferent colors. For API-based models, we do not record such latency breakdown information; the bar labeled as “(decode)” indicates the overall latency of prefilling and decoding phases. G.1.3 E FFICIENCY EVALUATION ON NVIDIA RTX 3090 We present the SoT speed-ups and latency breakdown on RTX 3090 in Fig. 13. We test the three 7B models, as their FP16-precision version can be run on an RTX 3090 GPU without further peak memory optimization techniques such as weight quantization (Frantar et al., 2022; Lin et al., 2023) or offloading (Sheng et al., 2023). On these three models, SoT can obtain 1.94× to 2.40× speed-up on average on Vicuna-80. For the five question categories that SoT can provide high-quality answers (i.e.,knowledge, common- sense, generic, roleplay, counterfactual), SoT can speed-up the overall answer generation process by 1.96× to 2.52× in the meantime. Note that for the math category, despite the average speed-up being 1.20× by calculating the speed-up across the three math questions, SoT does not reduce the absolute latency of processing the three questions. 0 2000 4000 6000 8000 10000 12000 14000 16000 Latency (ms) Vicuna-7B V1.3 Vicuna-7B V1.1 LLaMA2-Chat-7B 1.94× 2.26× 2.40× 0 2000 4000 6000 8000 10000 12000 14000 16000 Latency (ms) math fermi counterfactual coding roleplay knowledge common-sense writing generic 1.20× 1.70× 1.96× 2.10× 2.12× 2.37× 2.39× 2.43× 2.52×Normal (prefill) Normal (decode) SoT skeleton (prefill) SoT skeleton (decode) SoT point-expanding (prefill) SoT point-expanding (decode) Figure 13: The latency breakdown of SoT and normal decoding on the Vicuna-80 dataset. The average speed-up across questions are also marked on the figure. G.1.4 A CTUAL LATENCY TESTING This section reports the actual SoT speed-up on the Vicuna-80 with batch testing (instead of analyz- ing with pre-made profiling tables), using a single NVIDIA A100 GPU. We test the actual end-to-end latency of the SoT and normal decoding with the 9 open-source models. For each model, we run the speed-up test for five times and plot the box in Fig. 14. 28Published as a conference paper at ICLR 2024 As shown in Fig. 14a, the current SoT solution obtains a > 2× speed-up on 6 out of the 9 open- source models (i.e., Vicuna-7B V1.1, Vicuna-7B V1.3, UltraLM-13B, LLaMA2-Chat-7B, Vicuna- 13B V1.3, and LLaMA2-Chat-13B), and a> 1.7 speed-up on OpenChat-13B and Vicuna-33B V1.3. SoT achieves no speed-up on StableVicuna-13B. As shown in Fig. 14b, for the five question cate- gories that SoT can provide high-quality answers (i.e.,knowledge, common-sense, generic, roleplay, counterfactual), SoT can speed-up the overall answer generation process by 2.15 × to 2.50× in the meantime. 1.0 1.5 2.0 2.5 3.0 3.5 StableVicuna-13B Vicuna-33B V1.3 OpenChat-13B LLaMA2-Chat-13B Vicuna-13B V1.3 LLaMA2-Chat-7B UltraLM-13B Vicuna-7B V1.3 Vicuna-7B V1.1 0.97× 1.75× 1.97× 2.14× 2.19× 2.20× 2.75× 2.82× 2.88× (a) Average speed-up on different models. 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 fermi math roleplay writing counterfactual coding knowledge common-sense generic 1.63× 1.67× 2.15× 2.16× 2.18× 2.29× 2.34× 2.45× 2.50× (b) Average speed-up on different question categories. Figure 14: Speed-ups on 9 open-source models on the Vicuna-80 dataset with actual batch testing. G.2 S KELETON -OF-THOUGHT WITH ROUTER The overhead brought by the router inference is relatively small: On the Vicuna-80 dataset, the prompting and trained router have an average latency of 0.65s (0.39s ∼1.37s) and 0.04s (0.008s∼1.55s), respectively. On the WizardLM dataset, the average latency of the prompting and trained router is 0.80s (0.36s∼2.22s) and 0.03s (0.009s∼2.52s), respectively. G.2.1 S PEED -UP BREAKDOWN : MODELS Fig. 15 shows the speed-ups of SoT-R on different models on the Vicuna-80 dataset. Fig. 16 and Fig. 17 show the speed-ups of SoT-R on different models on the WizardLM dataset. We can ob- serve that on Vicuna-80, the two methods yield similar speed-ups, whereas on WizardLM, GPT-4 prompting router usually obtains higher speed-ups than the trained router, especially on GPT-4 itself. 1.0 1.2 1.4 1.6 1.8 2.0 2.2 StableVicuna-13B Claude ChatGPT-3.5 Vicuna-13B V1.3 Vicuna-7B V1.3 UltraLM-13B GPT-4 Vicuna-7B V1.1 Vicuna-33B V1.3 OpenChat-13B LLaMA2-Chat-7B LLaMA2-Chat-13B 0.98× 1.15× 1.24× 1.32× 1.39× 1.51× 1.54× 1.55× 1.62× 1.66× 1.67× 1.70× (a) Average speed-up across all question categories with prompting router. 1.0 1.2 1.4 1.6 1.8 2.0 2.2 StableVicuna-13B Claude ChatGPT-3.5 Vicuna-13B V1.3 Vicuna-7B V1.3 GPT-4 Vicuna-33B V1.3 UltraLM-13B Vicuna-7B V1.1 LLaMA2-Chat-7B LLaMA2-Chat-13B OpenChat-13B 0.99× 1.14× 1.33× 1.34× 1.42× 1.49× 1.57× 1.59× 1.59× 1.69× 1.70× 1.82× (b) Average speed-up across all question categories with trained router. Figure 15: Speed-ups of SoT-R on different models on Vicuna-80 dataset. 29Published as a conference paper at ICLR 2024 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 StableVicuna-13B Claude ChatGPT-3.5 Vicuna-7B V1.3 LLaMA2-Chat-7B LLaMA2-Chat-13B Vicuna-7B V1.1 UltraLM-13B Vicuna-13B V1.3 OpenChat-13B Vicuna-33B V1.3 GPT-4 1.13× 1.13× 1.40× 1.49× 1.51× 1.52× 1.56× 1.57× 1.59× 1.66× 1.68× 2.41× (a) Average speed-up across all question categories with prompting router. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 Claude StableVicuna-13B Vicuna-13B V1.3 UltraLM-13B Vicuna-33B V1.3 Vicuna-7B V1.3 LLaMA2-Chat-13B Vicuna-7B V1.1 LLaMA2-Chat-7B ChatGPT-3.5 OpenChat-13B GPT-4 1.09× 1.09× 1.31× 1.33× 1.33× 1.34× 1.35× 1.36× 1.37× 1.37× 1.42× 1.74× (b) Average speed-up across all question categories with trained router. Figure 16: Speed-ups of SoT-R on different models on WizardLM dataset. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Claude StableVicuna-13B Vicuna-7B V1.3 LLaMA2-Chat-7B LLaMA2-Chat-13B ChatGPT-3.5 Vicuna-13B V1.3 Vicuna-33B V1.3 OpenChat-13B Vicuna-7B V1.1 UltraLM-13B GPT-4 SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router Figure 17: Speed-ups of SoT and SoT-R on different models on the WizardLM dataset. G.2.2 S PEED -UP BREAKDOWN : CATEGORIES Fig. 18 and Fig. 19 show the speed-ups of SoT-R on different question categories of Vicuna-80 dataset. The trained router achieves slightly higher speed-up on most of the categories (except for knowledge, writing, and fermi). Fig. 20 and Fig. 21 show the speed-ups of SoT-R on different question categories of WizardLM dataset. We can observe that on 19 out of 29 categories, using the prompting router achieves higher speed-ups than using the trained router. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 math coding fermi writing roleplay counterfactual knowledge common-sense generic 0.90× 0.96× 1.01× 1.10× 1.17× 1.75× 1.95× 2.05× 2.11× (a) Speed-ups of SoT-R with prompting router on dif- ferent question categories. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 math writing coding fermi roleplay counterfactual knowledge common-sense generic 1.00× 1.00× 1.00× 1.00× 1.23× 1.79× 1.87× 2.10× 2.26× (b) Speed-ups of SoT-R with trained router on different question categories. Figure 18: Speed-ups of SoT-R on different question categories of Vicuna-80 dataset 1.0 1.5 2.0 2.5 3.0 3.5 4.0 math fermi counterfactual roleplay coding common-sense writing generic knowledge SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router Figure 19: Speed-ups of SoT and SoT-R on different question categories of the Vicuna-80 dataset. 30Published as a conference paper at ICLR 2024 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Math Physics Reasoning Code Generation Entertainment T oxicity Complex Format Multilingual Common-Sense Code Debug Biology Art Music Computer Science Roleplay Chemistry Ethics Academic Writing TruthfulQA Writting Literature Philosophy Law Sport Medicine History T echnology Economy Counterfactual 0.85× 0.94× 1.02× 1.02× 1.03× 1.12× 1.14× 1.22× 1.24× 1.25× 1.34× 1.47× 1.54× 1.54× 1.58× 1.62× 1.67× 1.69× 1.74× 1.77× 1.85× 1.90× 1.90× 1.93× 2.08× 2.10× 2.14× 2.18× 2.23× (a) Speed-ups of SoT-R with prompting router on dif- ferent question categories. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Code Generation Entertainment Art Complex Format Math Literature Code Debug Law Academic Writing Philosophy Biology Reasoning Physics History Computer Science Multilingual Music T oxicity Roleplay Common-Sense TruthfulQA Writting Economy Chemistry Ethics Sport T echnology Medicine Counterfactual 0.99× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.00× 1.07× 1.09× 1.14× 1.16× 1.17× 1.17× 1.20× 1.22× 1.36× 1.37× 1.41× 1.49× 1.65× 1.73× 1.82× 2.01× 2.17× 2.26× 2.41× (b) Speed-ups of SoT-R with trained router on different question categories. Figure 20: Speed-ups of SoT-R on different question categories of WizardLM dataset 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Entertainment Physics Reasoning Multilingual Math Common-Sense Biology Art Music T oxicity Ethics Computer Science Code Debug Chemistry Literature Academic Writing Philosophy Law TruthfulQA Roleplay Code Generation Complex Format Sport Writting Medicine History T echnology Economy Counterfactual SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router Figure 21: Speed-ups of SoT and SoT-R on different question categories of the WizardLM dataset. H O VERHEAD OF SOT IN DIFFERENT SCENARIOS Despite the optimizations made to the decoding phase, SoT brings overhead to the prefilling phase as the model needs to handle additional SoT prompts. Table 6 reports SoT’s prefilling overhead for the API-based models. These statistics are averaged across the Vicuna-80 questions that are suitable for SoT (according to our manual annotation). We can see that SoT significantly increases the number of prefilling tokens. This is because that SoT issues an independent point-expanding request for each point, with the average number of points being 6.8 on Vicuna-80 dataset across all evaluated models. Consequently, the APIs need to prefill the point-expanding request multiple times. 31Published as a conference paper at ICLR 2024 Table 6: SoT’s prefilling token overhead for API-based models. Model Prefill Phase Normal SoT Stage 1 SoT Stage 2 Ratio (SoT / Normal) Claude 10.33 155.33 730.91 85.79 ChatGPT-3.5 10.21 136.33 480.95 60.46 GPT-4 10.21 72.44 838.26 89.20 When using SoT to serve the open-source models, a simple and small trick is to prefill the common prefix of point-expanding requests with a batch size of 1 during Stage 2 (i.e., the point-expanding stage). Table 7 shows the prefilling overhead after applying the trick. Although the ratio is consid- erably smaller compared to that of the API-based models, this computational overhead remains a concern, especially during periods of high system workload. There are some possibilities to further reduce the token and computational overhead that are worth exploring in future work. To name a few: (1) When using SoT in serving systems, we can simply reuse the key-value cache containing the question and skeleton from Stage 1 during Stage 2, rather than re-prefilling them as in a multi-round conversation. (2) Generally, as LLM capabilities continue to evolve and prompt tuning techniques advance (Shin et al., 2020; Li & Liang, 2021; Lester et al., 2021; Jiang et al., 2023), the possibility of using much shorter prompts to activate the SoT mode in the future holds promise, which would significantly mitigate the token or computational overhead. Table 7: SoT’s computational overhead (in terms of the number of prefilling tokens) for open-source models. Model Prefill Phase Naive SoT Stage 1 SoT Stage 2 Ratio (SoT / Normal) LLaMA2-Chat-7B 12.52 171.41 216.49 30.98 LLaMA2-Chat-13B 12.52 171.41 216.41 30.98 OpenChat-13B 12.52 171.41 234.38 32.41 Vicuna-7B V1.3 12.52 171.41 211.61 30.59 Vicuna-13B V1.3 12.52 171.41 273.39 35.53 Vicuna-33B V1.3 12.52 171.41 258.88 34.37 StableVicuna-13B 12.52 171.41 312.03 38.61 UltraLM-13B 12.52 171.41 290.88 36.92 Vicuna-7B V1.1 12.52 171.41 196.64 29.40 I A NSWER QUALITY EVALUATION I.1 S KELETON -OF-THOUGHT I.1.1 A NSWER ANALYSIS : D IFFERENT MODELS Models on which SoT has low net win rates. We observe two types of reasons for the low net win rates. (1) OpenChat-13B, Vicuna-7B V1.1, and LLaMA2-Chat-13B. For the weak models such as OpenChat-13B and Vicuna-7B V1.1, they are not able to follow the SoT prompts precisely. For OpenChat-13B and LLaMA2-Chat-13B, sometimes the skeleton contains undesired contents (e.g., completing the skeleton points as shown below). 32Published as a conference paper at ICLR 2024 Question: How many lightning strikes occur on Earth each day? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. SoT’s answer (OpenChat-13B) (Orange texts are the skeleton.) (Only showing the skeleton) 1.Introduction. 2. The Earth’s atmosphere. 3. Lightning formation. 4. Electrical discharge. 5. Lightning strikes per second. 6. Calculating the total. Skeleton: 1. Introduction. Lightning is a beautiful and awe-inspiring natural phenomenon, but it can also be dangerous. It occurs when positively charged particles in a thundercloud discharge to the negatively charged Earth. The process is still not fully understood, but scientists have been able to estimate how many lightning strikes occur on Earth each day. Skeleton: 1. The Earth’s atmosphere. The Earth’s atmosphere plays a crucial role in the formation of lightning. It consists of different layers, with the tropo- sphere being the lowest and most active. The troposphere is where most of the weather occurs, and it is where lightning primarily forms. The atmosphere’s temperature, humidity, and wind patterns all contribute to the likelihood of a lightning strike. Skeleton: 2. Lightning formation. When warm, moist air rises, it cools and forms water <more texts are omitted...> For both OpenChat-13B and Vicuna-7B V1.1, sometimes they do not write anything in the point- expanding stage when details are needed. Question: As a superhero, how would you explain your origin story to a curious child? SoT’s answer (Vicuna-7B V1.1) (Orange texts are the skeleton.) 1. Born with extraordinary abilities. 2. Trained to use powers for the greater good. 3. Protecting the innocent and fighting villains.I use my powers to defend the defenseless and bring justice to those who have been wronged. 4. Inspired by courage, justice, and selflessness. These two issues result in bad net win rates. (2) Claude. For strong models such as Claude, we do not see major issues with SoT’s answers except the common issues across all models discussed in § 3.2.3. We hypothesize that the low net win rates are majorly due to the fact that their normal sequential generation is already very good. Models on which SoT has high net win rates. The models with high net win rates (Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B) are between the above two extremes. On one hand, they are good enough to understand SoT prompts. While we observe that they sometimes have the same issues as OpenChat-13B and Vicuna-7B V1.1 discussed before, these issues happen much less frequently. On the other hand, their normal sequential generation performance has a larger room for improvement than Claude and ChatGPT-3.5 so that the benefits from SoT are more visible on these models. I.1.2 A NSWER ANALYSIS : D IFFERENT QUESTION CATEGORIES Categories on which SoT has low net win rates. (1) Math. Math questions require step-by-step thinking. Without knowing the previous steps, it is hard to derive the following steps. Chain-of-Thoughts (Kojima et al., 2022; Wei et al., 2022) empha- size the importance of expanding the sequential thinking steps to solve these problems successfully. In contrast, SoT requires the models to (a) come up with the skeleton of the solution first and (b) deduce each individual step independently without referring to previous results. Both of these are challenging tasks. Strong models are able to get (a) correct but fail at (b). In the following example, ChatGPT-3.5 gets the skeleton steps correct. However, without knowing the results from the first step, the model starts making mistakes in the second step.7 7One additional (minor) issue of the API-based ChatGPT-3.5 is that the skeleton point “1.” is repeated in the answer. 33Published as a conference paper at ICLR 2024 Question: (Math) Solve for x in the equation 3x + 10 = 5(x - 2). ✗ SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Move all x terms to one side. 1. Move all x terms to one side by subtracting 5x from both sides, giving -2x + 10 = -10. 2. Move all constant terms to the other side. This can be done by subtracting 5x - 10 from both sides, leaving 3x - 5x = -10 - 10. 3. Simplify the equation. Combine like terms on both sides of the equation to get 3x + 10 = 5x - 10. 4. Divide both sides by the coefficient of x. Divide both sides by 2 to get x = -5. 5. Check the solution by plugging it back into the original equation.Substitute the value of x obtained in step 4 back into the original equation and verify that both sides are equal. ✓ Normal answer (ChatGPT-3.5) 3x + 10 = 5(x - 2) can be simplified as follows: 3x + 10 = 5x - 10 (distribute the 5 on the right side) 10 + 10 = 5x - 3x (subtract 3x from both sides) 20 = 2x (combine like terms on the right side) x = 10 (divide both sides by 2) Therefore, the solution for x in the equation 3x + 10 = 5(x - 2) is x = 10. For weaker models, it is hard to even get step (a) correct. For example, as shown below, the third step “Apply FOIL” in Vicuna-13B V1.3’s SoT solution comes out of nowhere. This makes SoT’s solution incorrect (though the solution from the normal generation is also incorrect). Question: (Math) Solve for x in the equation 3x + 10 = 5(x - 2). ✗ SoT’s answer (Vicuna-13B V1.3) (Orange texts are the skeleton.) 1. Simplify equation. We can simplify the equation by combining like terms: 3x + 10 = 5(x - 2) =¿ 3x + 10 = 5x - 10 =¿ x + 5 = 2 =¿ x = 2 - 5 =¿ x = -3 2. Factor 5(x - 2). 5(x - 2) = 5x - 10. 3. Apply FOIL. * Multiply the binomials in the equation, using the first term of each binomial for the first term of the product, the outside term for the second term of the product, the second term of each binomial for the third term of the product, and the remaining terms for the fourth term of the product. * 3(x - 2) + 10 = 5x + 5(-2) * 3x - 6 = 5x - 10 * 3x = 5x - 10 * x = 5x - 10 / 3 * x = 5/3x - 10/3 * x = 2/3x - 5/3 4. Solve for x. After applying the FOIL method and simplifying the equation, we get 3x = 17. Therefore, x = 17/3. ✗ Normal answer (Vicuna-13B V1.3) To solve for x in the equation 3x + 10 = 5(x - 2), we need to first distribute the 5 to multiply both sides by 5. 3x + 10 = 5(x - 2) 5(3x + 10) = 5(x - 2) 15x + 50 = 5x - 10 Add 5x to both sides: 20x + 50 = 5x Subtract 5x from both sides: 15x = 0 Divide both sides by 15: x = 0 Therefore, the solution to the equation is x = 0. (2) Fermi. Similar to math questions, questions in the fermi category require solvers to make as- sumptions and calculate the answer step-by-step. For example, to answer the question in the figure below, the solver is required to make an assumption about when life began on Earth. In step 4, ChatGPT-3.5 does not know that it has already assumed 3.5 billion years in step 3 and uses the wrong number 4.54 billion in the calculation. In step 5, ChatGPT-3.5 does not know that it has already made the above assumptions and refuses to answer “due to the uncertainties in estimating the age of the Earth”. In contrast, normal generation is able to create a consistent solution. 34Published as a conference paper at ICLR 2024 Question: (Fermi) How many times has the Earth orbited the Sun since the beginning of life? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. ✗ SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Estimate the age of the Earth. The age of the Earth is estimated to be around 4.54 billion years, based on radiometric dating of rocks and meteorites. 2. Determine when life began on Earth. Life on Earth is estimated to have begun around 3.5 billion years ago, based on the discovery of ancient microfossils. 3. Calculate the number of years since life began. The oldest known fossils suggest that life on Earth began around 3.5 billion years ago. Therefore, the number of years since life began on Earth is approximately 3.5 billion years. 4. Divide the number of years by the length of one orbit. Assuming an orbital period of 365.25 days, the Earth has orbited the Sun approximately 4.54 billion / 365.25 = 12,425,000 times since the beginning of life. 5. The Earth has orbited the Sun approximately X times since the beginning of life. Unfortunately, it is impossible to give a specific number for the amount of times the Earth has orbited the Sun since the beginning of life due to the uncertainties in estimating the age of the Earth and when life first emerged. Normal answer (ChatGPT-3.5) The beginning of life on Earth is estimated to have occurred around 3.5 to 4 billion years ago. Since that time, the Earth has been orbiting the Sun continuously. The length of one orbit, or one year, is approximately 365.25 days. This means that the Earth completes one full orbit around the Sun every 365.25 days. To calculate the number of times the Earth has orbited the Sun since the beginning of life, we can divide the total number of days by the length of one orbit: Total number of days since the beginning of life = (3.5 billion years) x (365 days per year) = 1.2775 trillion days Number of orbits = (1.2775 trillion days) / (365.25 days per orbit) = 3.5 billion orbits Therefore, the Earth has orbited the Sun approximately 3.5 billion times since the beginning of life. As the following example shows, there are also cases when SoT solves the question (with minor computation errors) by embedding the computation steps in the skeleton directly in the skeleton stage. The point-expanding stage will then add little content (except some undesired repetition of the following skeletons occasionally). This happens both on small/weak models and large/powerful models. Although this behavior is not what we originally expect, it sheds light on a promising direc- tion for making SoT a general framework: in the problem where step-by-step thinking is needed, the SoT model automatically falls back to the sequential generation mode by outputting the complete answer in the skeleton stage and skipping the point-expanding stage. Future work needs to be done to control and encourage this behavior better. Question: (Fermi) How many times does the average human blink in a lifetime? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. SoT’s answer (Vicuna-7B V1.1) (Orange texts are the skeleton.) 1. The average human blinks 12-15 times per minute. 2. There are 60 minutes in an hour. 3. Therefore, the average human blinks 60 x 12-15 = 720-900 times per hour. 4. There are 24 hours in a day.Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day. 5. Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day.This translates to blinking approxi- mately 50-65 times per hour, which is a significant portion of the day. 6. There are 365 days in a year.Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times per year. 7. Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times per year. 8. The human lifespan is typically 70-80 years.Therefore, the average human blinks 80,000-112,000 times in their lifetime. 9. Therefore, the average human blinks 5,857,600-8,124,000 / 70-80 = 80,000-112,000 times in their lifetime. Normal answer (Vicuna-7B V1.1) The average human blinks about 12-15 times per minute, which translates to about 18,200 to 20,400 blinks per day. Over the course of a lifetime, the number of blinks is estimated to be around 7,500,000 to 8,000,000. Blinking is a reflexive action that helps to keep the eyes moist and comfortable, as well as to protect the eyes from dust and other foreign particles. The frequency of blinking can vary depending on a variety of factors, including age, health, and environmental conditions. For example, people who work in dusty or dry environments may blink more frequently than those who work in a clean, moist environment. Over the course of a lifetime, the average person will spend approximately 27,375 days or 700,000 hours awake. This means that the majority of the time spent blinking will occur in the first few decades of life, with the frequency of blinking gradually decreasing as people age. It is important to note that there is a significant amount of individual variation in the frequency of blinking. Some people may blink more or less than the average, depending on a variety of factors such as age, health, and environmental condi- tions. Additionally, blinking patterns can be affected by factors such as stress, fatigue, and certain medical conditions. 35Published as a conference paper at ICLR 2024 (3) Coding. While in most cases, the models are able to decompose the coding problem into smaller tasks in the skeleton stage, the generation quality during the point-expanding stage is bad, potentially due to the fact that we did not carefully craft a special point-expanding prompt for coding. The failure cases can be grouped into two cases. (a) In some cases, the model only generates a description of how implementation can be done without giving the code. Question: (Coding) Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file. SoT’s answer (Claude) (Orange texts are the skeleton.) 1. Open file stream by declaring an ifstream object and using the open() method to open the file in read mode. 2. Read line by line from the file stream into a string using getline(). 3. Find word by searching each line for the target word using string::find and keeping track of matches. 4. Increment counter by one each time the specified word is encountered in the current line. 5. Close file The file stream should be closed after finishing reading the file to free up resources. This can be done by calling the close() method on the file stream object. This might be due to the fact that the powerful models (e.g., Claude) follow the point-expanding prompt “Write it **very shortly** in 1 ∼2 sentence” precisely. Better prompts tailed to coding could help. (b) In some other cases, the model tends to generate the complete code repeatedly in several points. Question: (Coding) Develop a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file. SoT’s answer (OpenChat-13B) (Orange texts are the skeleton.) 1. Read file line by line. 2. Store lines in a container. 3. Count occurrences of specific word. “‘cpp <more texts are omitted...> “‘ This code uses the C++ standard library to read a text file line by line, store each line in a container, and count the occurrences of a specific word. The results are printed. 4. Print results. Here’s the C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the file: “‘cpp <more texts are omitted...> “‘ Please note that this is a very basic implementation. You may need to adjust it depending on your specific requirements. This problem might also be alleviated by prompts that explicitly tell the model to only generate a “code snippet” in each skeleton point. Although better prompt engineering could lead to better results, we expect that SoT is not suitable for the coding questions in Vicuna-80. The reason is that the solutions to the problems in Vicuna- 80 are usually in a few lines with strong dependencies between each other. Without knowing the previously defined variable names or imported libraries, it is hard (or even impossible) to implement the subsequent code correctly. As a consequence, generating different parts of the answers in parallel is not suitable here. Similar to the math questions, automatically falling back to outputting all the code in the skeleton stage and not triggering the point-expanding stage might be more suitable for answering this question type. However, we expect that SoT could be helpful for larger coding tasks that involve multiple modules (e.g., functions, classes). The skeleton stage could be in charge of defining the interfaces between different modules (e.g., functionalities, names, parameters, return types). The point-expanding stage could be in charge of implementing these modules or using these modules to complete the final task, which can be done in parallel. This mimics the common practice in software engineering. (4) Writing. The writing questions usually ask for an email, a blog post, or a review in a given scenario. In the detailed evaluation output from FastChat and LLMZoo metrics, the major and common complaint is that SoT’s answers are (slightly) less detailed. This could potentially be improved by point-expanding prompts that request more details. 36Published as a conference paper at ICLR 2024 While inspecting SoT’s answers manually, we observe one additional issue. Writing problems usu- ally require a coherent passage without embedded skeleton points, whereas our current SoT pipeline (§ 2) concatenates skeleton points as part of the answer. One example is shown below. Question: (Writing) Can you help me write a formal email to a potential business partner proposing a joint venture? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Introduce yourself and your company. Dear [Recipient], I hope this email finds you well. My name is [Your Name] and I am the [Your Position] at [Your Company Name]. 2. State the purpose of the email. The purpose of this email is to propose a joint venture between our companies. 3. Explain the potential benefits of the joint venture. By combining our resources and expertise, we can increase market share and profitability for both companies. 4. Provide details on the proposed collaboration. Our proposed collaboration would involve combining our expertise in marketing with your strong manufacturing capabilities to create a new line of products. 5. Suggest a follow-up meeting or call. Please let me know your availability for a follow-up call next week to discuss this opportunity further. 6. Thank the recipient for their time and consideration.Thank you for considering this proposal and we look forward to hearing back from you soon. Normal answer (ChatGPT-3.5) Of course, I can help you with that. Here’s a sample email you can use as a guide: Subject: Proposal for Joint Venture Dear [Business Partner’s Name], I hope this email finds you well. I am writing to propose a joint venture between our companies, [Your Company Name] and [Potential Partner Company Name]. We believe that our respective expertise can create a successful partnership that will benefit both of our businesses. We have conducted thorough research on your company and admire your innovative approach to [insert industry or niche]. We think our [insert expertise or specialty] could complement your [insert potential partner’s specialty or expertise]. By collaborating on this joint venture, we believe we can create a new and innovative product that will appeal to our respective markets. We are confident that our companies would work well together and that our joint venture would be successful. We would like to set up a meeting to discuss the specifics of our proposal and explore how we can move forward with the joint venture. Thank you for considering this proposal. We look forward to hearing from you soon. Best regards, [Your Name] In such cases, simply removing the skeleton points would greatly improve the answer quality. To make SoT more general, one future direction is to let the LLM itself decide whether the point index and point skeleton should be included to make the final answer more natural and fluent. Categories on which SoT has high net win rates. (1) Counterfactual, knowledge, common sense, generic. All these four categories have the same characteristic: the ideal answer should cover several relativelyindependent points. Having the LLM generate a skeleton before expanding the details can result in more comprehensive discussions for the question. Besides, organizing the answers as a list of points makes the answers easier to read, whereas the answers from normal generation are sometimes less structured and less readable. Below, we give one example for each question category to illustrate these points. 37Published as a conference paper at ICLR 2024 Question: (Counterfactual) What if the Internet had been invented during the Renaissance period? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Advancements in communication.The invention of the Internet during the Renaissance period would have revolution- ized communication, allowing for faster and more efficient exchange of ideas and information across vast distances. 2. Increased access to knowledge.The invention of the printing press during the Renaissance period already allowed for greater access to knowledge, but the internet would have revolutionized this even further by providing instantaneous access to information from all over the world. 3. Improved dissemination of ideas.If the internet had been invented during the Renaissance period, the spread of ideas and knowledge would have been much faster and widespread than it was at the time. This could have led to a faster progression of scientific and artistic innovations. 4. Impact on art and literature.The invention of the internet during the Renaissance period would have revolutionized the way art and literature are created, shared, and consumed, leading to a more diverse and globally-influenced artistic and literary scene. 5. Potential for global trade.The invention of the internet during the Renaissance period would have greatly expanded global trade opportunities, allowing for easier communication and collaboration between merchants and traders across continents. 6. Influence on political and social structures.The invention of the internet during the Renaissance period could have led to significant changes in political and social structures, allowing for greater democratization and communication among people from different regions and classes. Normal answer (ChatGPT-3.5) If the Internet had been invented during the Renaissance period, it would have had a significant impact on the way people communicated, shared ideas, and gained knowledge. The Renaissance was a time of great intellectual and artistic growth, and the Internet would have facilitated the spread of these ideas and innovations. Scholars and artists would have been able to share their work more easily, and people from different parts of the world would have been able to connect and collaborate more easily. The Internet would have also made it easier to access information, which could have accelerated the pace of scientific and technological advancements. However, it’s important to note that the infrastructure required for the Internet to exist did not exist during the Renaissance period. There were no telecommunications networks or electricity grids, so the idea of the Internet would have been impossible to realize. Nonetheless, it’s interesting to think about how the Renaissance might have been different if the Internet had existed at that time. 38Published as a conference paper at ICLR 2024 Question: (Knowledge) What are some potential implications of using a single-use plastic bottle versus a reusable bottle on both the environment and human health? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Single-use plastic waste.Single-use plastic bottles contribute to the massive amount of plastic waste that ends up in landfills and oceans, harming wildlife and ecosystems. 2. Pollution of waterways.Single-use plastic bottles contribute to the pollution of waterways, as they often end up in rivers, lakes, and oceans, causing harm to aquatic life and the overall ecosystem. 3. Harmful chemicals leaching.Single-use plastic bottles may release harmful chemicals such as bisphenol A (BPA) and phthalates into the water or liquid they contain, posing a risk to human health. 4. Greenhouse gas emissions.The production and disposal of single-use plastic bottles contribute to greenhouse gas emissions, which contribute to climate change. Using a reusable bottle reduces the need for constant production and disposal, therefore reducing greenhouse gas emissions. 5. Microplastic ingestion.Single-use plastic bottles contribute to the accumulation of microplastics in the environment, which can be ingested by humans through food and water, leading to potential health risks. 6. Increased risk of disease.Single-use plastic bottles can harbor harmful bacteria, leading to an increased risk of disease if not disposed of properly. Reusable bottles, when cleaned regularly, pose a lower risk. Normal answer (ChatGPT-3.5) Using a single-use plastic bottle has significant implications on the environment and human health. Environmental implications: 1. Plastic pollution: Single-use plastic bottles are one of the major contributors to plastic pollution in oceans and landfills. They take hundreds of years to decompose, and during that time, they release toxic chemicals and microplastics into the environment. 2. Carbon emissions: Manufacturing and transporting single-use plastic bottles require a significant amount of energy and resources, which contribute to greenhouse gas emissions and climate change. 3. Natural resource depletion: The production of single-use plastic bottles requires large amounts of water and oil, which are finite resources. Health implications: 1. Chemical exposure: Single-use plastic bottles contain chemicals such as Bisphenol A (BPA) and phthalates, which can leach into the water and pose a risk to human health. 2. Microplastics ingestion: When plastic bottles break down, they release microplastics, which can enter the food chain and be ingested by humans. Using a reusable bottle, on the other hand, has a positive impact on the environment and human health. Environmental benefits: 1. Reduced plastic waste: Reusable bottles significantly reduce the amount of plastic waste generated. 2. Lower carbon emissions: Reusable bottles have a lower carbon footprint as they can be used multiple times. 3. Reduced resource depletion: The production of reusable bottles requires fewer resources than single-use plastic bottles. Health benefits: 1. Chemical-free: Reusable bottles are typically made from materials that are free from harmful chemicals, reducing the risk of chemical exposure. 2. Reduced microplastics: Reusable bottles do not break down easily, reducing the release of microplastics into the environment. Overall, using a reusable bottle is a more sustainable and healthier option than using a single-use plastic bottle. Question: (Common sense) How can you determine if a person is genuinely interested in a conver- sation or simply being polite? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Pay attention to body language.Notice if their posture is open or closed, if they are facing towards you or away, and if they are fidgeting or relaxed. These can all indicate their level of interest in the conversation. 2. Listen for engaged responses.If the person is genuinely interested in the conversation, they will respond with thoughtful and relevant comments or questions rather than short, generic responses. 3. Observe eye contact.If the person maintains steady eye contact and their gaze is focused on you, they are likely genuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may just be being polite. 4. Look for follow-up questions.If the person asks follow-up questions and seems genuinely interested in what you’re saying, then they are likely interested in the conversation and not just being polite. 5. Notice tone of voice.A person who is genuinely interested in a conversation will often have a more enthusiastic and animated tone of voice, compared to someone who is simply being polite and giving short, monotone responses. Normal answer (ChatGPT-3.5) As an AI assistant, I cannot determine a person’s true intentions or feelings. However, there are some social cues that can indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact, lean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the other hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are not as interested. Ultimately, it’s important to remember that everyone has different communication styles, and it can be difficult to accurately interpret someone’s intentions without directly asking them. 39Published as a conference paper at ICLR 2024 Question: (Generic) How can I improve my time management skills? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Set clear goals. Setting clear goals is important in improving time management skills as it helps you to focus on what needs to be done and what can be accomplished within a certain timeframe. 2. Prioritize tasks. Make a list of tasks and prioritize them based on their importance and deadline. This will help you focus on the most critical tasks and avoid wasting time on less important ones. 3. Create a schedule. Creating a schedule helps you allocate time for each task and ensures that you complete them within the allotted time. 4. Minimize distractions. Distractions such as social media, emails, and phone notifications can waste a lot of time. Try to limit them by turning off notifications or setting specific times to check them. 5. Take breaks. Taking breaks can actually improve productivity and help prevent burnout. It’s important to schedule short breaks throughout the day to recharge and refocus. 6. Learn to say no. It’s important to prioritize your own tasks and not take on too much at once. Saying no to requests that don’t align with your goals or schedule can help you manage your time more effectively. 7. Delegate tasks. If you have too many tasks to handle, consider delegating some of them to others who can help you. This will allow you to focus on more important tasks and manage your time more effectively. 8. Use time management tools.There are a variety of time management tools available, such as calendars, to-do lists, and productivity apps, that can help you stay organized and on track with your tasks. Normal answer (ChatGPT-3.5) Improving time management skills can be a challenge, but there are several strategies that can help. Here are a few: 1. Set clear goals and priorities: Identify your most important tasks and make sure to complete them first. 2. Make a schedule: Create a schedule that includes your most important tasks and set aside time for them. 3. Use a timer: Set a timer for a specific amount of time to help you stay focused and avoid distractions. 4. Take breaks: Take short breaks throughout the day to help you recharge and avoid burnout. 5. Eliminate distractions: Try to limit distractions as much as possible, such as by turning off your phone or closing your email. 6. Delegate tasks: If possible, delegate tasks to others to free up time for more important activities. Remember, improving time management skills takes practice and perseverance. Be patient with yourself and keep trying until you find strategies that work well for you. (2) Roleplay. Roleplay questions require LLMs to assume a character and present an introduction or speech to other people. LLMZoo and FastChat metrics usually favor the level of details of SoT when assigning it a better score. Similar to the writing category discussed before, SoT’s answers could be even better if the skeleton points are removed from the answer. Question: (Roleplay) How would you introduce yourself as a medieval knight at a royal banquet? SoT’s answer (Claude) (Orange texts are the skeleton.) 1. Name and house. My name is Sir Percival of Camelot, knight of the Round Table and protector of King Arthur’s realm. 2. Lands and titles. I hold fiefdom over the lands of Westmarch and Ravenshire, and bear the titles of Baron of the Western Marches and Knight of the Raven. 3. Military feats. I have led my men to great victories against the kingdom’s enemies, winning many battles through strength of arms and tactical cunning. 4. Allegiance to king. I pledge my sword, my loyalty, and my life to serve and protect my noble king and this great kingdom. 5. Request to join table. Your Highness, it would be the greatest honor to join your exalted table and share in this magnificent feast with my fellow knights and loyal subjects. Normal answer (Claude) Greetings, your majesty and honored guests. I am Sir Claude, a knight in service of the king. I hail from the province of Assistantia, where I was trained in the knightly virtues of chivalry, courage and courtesy. Though I am but a humble knight, I shall endeavor to serve you and this kingdom with all the strength and valor I possess. I thank you for allowing me to join you at this royal feast, and hope only that my conduct brings further honor to my liege and this noble court. Please let me know if I can be of any service during the celebrations, as it would be my honor to assist you in any way. I.1.3 A NSWER ANALYSIS : D IFFERENT METRICS Metrics on which SoT has low net win rates. (1) Coherence. In the prompt of LLMZoo evaluation, coherence is defined as “The response should be coherent and flow logically from one point to the next that is easy to read and understand without major gaps or inconsistencies. The more coherent they are, the better.” SoT’s answer is structured in a list of points. It is suitable for some questions but not all. For example, in the questions that require 40Published as a conference paper at ICLR 2024 LLMs to write a passage, email, or script, the introduction and conclusion paragraphs are usually missing, and the answers usually lack transition sentences between consecutive points. These make the answers less coherent. Below is one example from Claude. Question: Write a script for a YouTube video exploring the history and cultural significance of jazz. SoT’s answer (Claude) (Orange texts are the skeleton.) 1. Origins. Jazz originated in the late 19th and early 20th centuries among African Americans in New Orleans, emerging from a combination of African rhythmic traditions, blues, ragtime, and other influences. 2. Early jazz styles. The early styles of jazz that emerged in the late 19th and early 20th centuries included ragtime, Dixieland, and the blues. 3. Jazz age. The Jazz Age was the period during the 1920s when jazz music and dance styles like the Charleston became popular cultural phenomena in American and European society. 4. Bebop era. Bebop emerged in the early 1940s with artists like Dizzy Gillespie and Charlie Parker experimenting with faster tempos, complex chord changes, and unpredictable melodic phrasing. 5. Cool jazz. Emerging in the late 1940s and early 1950s, cool jazz was a more subdued, mellow style that broke from the high-energy virtuosic improvisation of bebop. 6. Free jazz emerged in the late 1950s and 1960s as musicians explored free improvisation and atonality, moving away from traditional jazz structures. 7. Fusion jazz. Major jazz artists such as Miles Davis and Herbie Hancock began experimenting with combining elements of rock, funk, and electronic music in the 1970s, resulting in the emergence of the subgenre known as fusion jazz. 8. Jazz festivals. Jazz festivals emerged in the 1950s and 1960s as a way to celebrate jazz music, with famous early festivals like the Newport Jazz Festival bringing thousands of fans together to hear top performers. 9. Jazz instruments. The trumpet, saxophone, piano, double bass, and drum set are some of the key instruments com- monly used in jazz. 10. Influential musicians include Louis Armstrong, Duke Ellington, Charlie Parker, Miles Davis, John Coltrane, and many others who pioneered new styles and techniques. 11. Legacy and impact. Jazz has left an indelible mark on music and culture over the past century, influencing countless artists and genres while spreading the spirit of improvisation and freedom across the globe. Normal answer (Claude) Here is a draft script for a YouTube video exploring the history and cultural significance of jazz: Introduction Hey everyone! Welcome to my channel. In today’s video, we’re going to be exploring the history and cultural impact of one of America’s original art forms - jazz music. Jazz Origins Jazz grew out of the African American communities in the southern United States in the late 19th and early 20th centuries. It was influenced by a combination of African and European musical traditions. Some key elements that fed into early jazz included blues, ragtime, spirituals, work songs, field hollers, and brass band music. New Orleans is considered the birthplace of jazz, emerging around the turn of the 20th century. The city was a melting pot of cultures and musical styles. Musicians mixed improvisation, syncopated rhythms, bent notes and a loose ensemble playing style to create a new genre that became known as jazz. Early jazz greats like Buddy Bolden, Jelly Roll Morton and Sidney Bechet began fusing these various influences into the distinct jazz sound. As the music spread via travel and recordings in the 1910s, jazz captivated listeners and began to evolve into different styles. Jazz Styles Emerge In the 1920s, jazz expanded out from New Orleans and Chicago emerged as a hotspot. Louis Armstrong’s innovative solos and expressive phrasing became influential. The swing style also developed during the 1930s and 40s, propelled by star big bands and dancers. Bebop arose in the 1940s with soloists like Charlie Parker and Dizzy Gillespie exploring new levels of harmonic and rhythmic complexity. In the 50s and 60s, jazz expanded into new fusions like cool jazz, hard bop, free jazz, modal jazz and soul jazz. Artists used jazz to express their unique regional, social and political identities. Cultural Significance Over the decades, jazz has become woven into the fabric of American society. It is equated with many values and ideals like individualism, freedom, spontaneity, innovation and social harmony. Jazz was associated with breaking down barriers between races, classes and genres. It represented modernism and urban culture. Musicians like Louis Armstrong and Duke Ellington became some of the first widely popular African American artists. Jazz also spread American culture and values abroad through performances, radio and film. It influenced music around the world and became a truly global art form. Today jazz continues to thrive and evolve both in the U.S. and internationally. Its history and sounds reflect the diversity of both American and global cultures. From its folk roots, jazz has become a sophisticated art music that retains its spirit of freedom and improvisation. Conclusion That covers some of the key points in the history and cultural impact of jazz music. Let me know in the comments about your favorite jazz styles and musicians! Be sure to like and subscribe. See you next time. While the content of SoT’s answer is good, it reads less fluent for the purpose of a YouTube script. (2) Immersion. In the prompt of LLMZoo evaluation, immersion is defined as “the response should act like the assigned role using the tone, manner, and vocabulary the role would use. The more assistant-like tones, the worse. The more in-character, the better.” The way that SoT breaks the 41Published as a conference paper at ICLR 2024 answers into a list makes its answer less in-character. Therefore, SoT receives a lower immersion score. Here is one example. Question: Pretend to be a world-famous chef. How would you describe your signature dish to a panel of judges? SoT’s answer (Claude) (Orange texts are the skeleton.) 1. Main ingredient: The incredibly fresh and tender salmon caught just this morning from the crystal clear waters of the Pacific Northwest. 2. Cooking method I slowly braise the main ingredient in a rich broth to bring out its natural flavors before searing it to a perfect golden crust. 3. Special seasoning The dish is seasoned with a blend of fresh herbs like thyme, rosemary, and oregano which provides an aromatic backbone that pairs nicely with the natural flavors of the main ingredients. I add just a touch of smoked paprika to provide a subtle smokiness without overpowering the other flavors. 4. Texture: The dish has a perfect balance of crispy on the outside and tender on the inside, with each bite yielding a delightful mix of crunchy and smooth textures that dance across your tongue. 5. Plating presentation I purposefully create height and dimension on the plate with the carefully arranged components of the dish, so that the dish appears vibrant and alive. The interplay of colors, textures and shapes is intended to delight the eye before the first bite. 6. Inspiration: This dish was inspired by my dear grandmother, who taught me the secret family recipe when I was just a child. Normal answer (Claude) *clears throat and speaks in a fancy French accent* Bonjour honored judges. Today I present to you my signature dish - la cr `eme de la cr `eme of French haute cuisine. Behold, le poulet `a la Claude! This succulent chicken is firstly marinated overnight in a bath of fine herbes - thyme, rosemary, and sage cultivated from my private garden. The young spring chicken is then roasted to perfection, its skin crisping to a beautiful caramel brown. Nestled beside the chicken is a tower of pommes dauphine - light and airy potato puffs - alongside a decadent mound of cr`eme fraˆıche infused with truffle essence. But the pi`ece de r ´esistance is the sauce. A luxurious pool of velout ´e enriched with cream and butter, brightened with a dash of lemon, and studded with morsels of morel mushrooms foraged at dawn. One bite of this ensemble will transport you to food heaven. The tender chicken, the creaminess of the potato puffs, and the rich decadence of the sauce meld together in blissful harmony. So tell me honored judges, will my poulet `a la Claude earn your coveted approval? I await your discerning verdict as I swirl more vintage Bordeaux into your glasses. Sant´e! Again, while the answer from SoT covers sufficient aspects and details, it reads rather stiffly. The normal answer, in contrast, has the opening sentence ”Bonjour honored judges. Today I present to you my signature dish...” and the conclusion statement ”So tell me honored judges, will my poulet`a la Claude earn your coveted approval? ...” which reads more like a casual talk from a chef. Metrics on which SoT has high net win rates. (1) Diversity. In the prompt of LLMZoo evaluation, diversity is defined as “The response should be comprehensive and provide a range of information that is not limited to a single perspective. More perspectives are better.” The skeleton stage in SoT encourages LLMs to think from multiple perspectives. Therefore, SoT’s answers tend to have better diversity. Below is one example showing that SoT’s answer has better diversity. 42Published as a conference paper at ICLR 2024 Question: How can you determine if a person is genuinely interested in a conversation or simply being polite? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Pay attention to body language.Notice if their posture is open or closed, if they are facing towards you or away, and if they are fidgeting or relaxed. These can all indicate their level of interest in the conversation. 2. Listen for engaged responses.If the person is genuinely interested in the conversation, they will respond with thoughtful and relevant comments or questions rather than short, generic responses. 3. Observe eye contact.If the person maintains steady eye contact and their gaze is focused on you, they are likely genuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may just be being polite. 4. Look for follow-up questions.If the person asks follow-up questions and seems genuinely interested in what you’re saying, then they are likely interested in the conversation and not just being polite. 5. Notice tone of voice.A person who is genuinely interested in a conversation will often have a more enthusiastic and animated tone of voice, compared to someone who is simply being polite and giving short, monotone responses. Normal answer (ChatGPT-3.5) As an AI assistant, I cannot determine a person’s true intentions or feelings. However, there are some social cues that can indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact, lean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the other hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are not as interested. Ultimately, it’s important to remember that everyone has different communication styles, and it can be difficult to accurately interpret someone’s intentions without directly asking them. SoT answers the question from five aspects, whereas the normal answer only mentions one aspect. (2) Relevance. In the prompt of LLMZoo evaluation, relevance is defined as “The response should be closely related to the question and answer the question accurately with sufficient details without repetition or redundancy. The more relevant they are, the better.” In the skeleton stage, LLMs are forced to propose several points around the question; and in the point-expanding stage, LLMs are required to only discuss these points. Therefore, answers from SoT tend to have better relevance. One typical example is shown below. Question: Why do some people enjoy the sensation of being scared, such as by watching horror movies or going on roller coasters, while others avoid these experiences? SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) 1. Neurological differences.Some people enjoy the sensation of being scared due to the release of adrenaline and dopamine in the brain, while others may have a lower threshold for fear due to differences in their amygdala and pre- frontal cortex. 2. Psychological factors.Some people may enjoy the sensation of being scared because it can provide a sense of excite- ment, thrill, and a temporary escape from the mundane aspects of everyday life. On the other hand, others may avoid these experiences due to past trauma or anxiety. 3. Developmental experiences.Individuals who had negative experiences with fear during childhood, such as trauma or abuse, may avoid scary situations as adults, while those who had positive experiences with fear may seek out these experiences. 4. Cultural influences.Some cultures place a greater emphasis on seeking excitement and thrill, while others may priori- tize safety and security, which can impact whether individuals enjoy scary experiences or avoid them. 5. Genetics.Research has suggested that some individuals may have a genetic predisposition towards seeking out thrilling or risky experiences, which may include enjoying the sensation of being scared. Normal answer (ChatGPT-3.5) The enjoyment of being scared is a complex psychological phenomenon. Some people may enjoy the adrenaline rush that comes with being scared, while others may enjoy the feeling of being able to conquer their fears. Additionally, some people may be more naturally inclined to seek out new experiences and sensations, including those that are scary or thrilling. On the other hand, some people may be more risk-averse or may have had negative experiences with fear or anxiety in the past, which could make them more likely to avoid scary experiences. Ultimately, the reasons why some people enjoy being scared while others do not are likely to be influenced by a range of factors, including personality, past experiences, and individual preferences. In the answer from the normal generation, the first two sentences provide little information in an- swering the question, and the last sentence only gives keywords such as “personality, past expe- riences, and individual preferences” without providing concrete explanations to each. In contrast, 43Published as a conference paper at ICLR 2024 SoT’s answer is well-structured into five reasons with sufficient explanations and it does not waste space in irrelevant contents. I.1.4 Q UALITY BREAKDOWN : Q UESTION CATEGORIES AND MODELS In the main text, we analyze how question categories and models affect SoT’s answer quality. Here, Fig. 22 show the per-model and per-category results. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BStableVicuna-13BVicuna-7B V1.1Vicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3UltraLM-13B Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average -75% -43% -71% 0% -100% -71% -86% -86% -86% -57% -100% -100% -100% -50% 0% -67% -67% -33% -100% -67% 0% -33% -33% -33% -100% -67% -34% 0% -10% 0% -30% -90% -30% -30% -20% 10% -80% -100% -30% -6% 30% -30% -30% 70% -50% -10% 10% -50% 0% -30% 10% 10% -45% -60% -30% -20% -20% -70% -40% -30% 0% 10% -100% -90% -90% -0% 0% 0% -50% 60% -50% 0% 20% 30% 30% -50% -20% 30% 14% -20% -40% -70% 80% -10% 50% 40% 10% 60% -40% 60% 50% 35% 90% 10% 30% 50% -40% 40% 60% 40% 30% -50% 70% 90% 9% -20% -40% -50% 100% -40% -20% 70% 20% 50% -20% 20% 40% -17% -3% -31% -29% 20% -58% -18% 6% -10% 11% -56% -28% -7% -100% -75% -50% -25% 0% 25% 50% 75% 100% Figure 22: Net win rates of different models and question categories. Each row corresponds to one question category, and one column corresponds to one model. (Evaluated using metric defined by the FastChat prompt, and GPT-4 as the judge.) I.2 S KELETON -OF-THOUGHT WITH ROUTER Fig. 23 shows net win rates of SoT on Vicuna-80 dataset with LLMZoo metrics, and Fig. 24 shows net win rates of SoT on WizardLM dataset with FastChat metrics. The key takeaways are: (1) In both cases, SoT-R achieves similar or better quality than SoT, and the net win rates of SoT-R are usually non-negative. This indicates that SoT-R falls back to normal decoding on the right question categories. (2) On the WizardLM dataset, we see that the trained router has better performance than the prompting router in most cases. This is reasonable, as the prompting router is limited by the capability of GPT-4, whereas the trained router is dedicated to this task. (3) Sometimes, our routers can even achieve better performance than humans. Fig. 1(b) in the main text has showed SoT’s quality and speed-up plot evaluated with the FastChat quality metric, here, Fig. 25 shows the results evaluated with the LLMZoo quality metric. -20% 0% 20% 40% 60% counterfactual generic common-sense knowledge roleplay fermi writing  SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router SoT-R w/ human router Figure 23: Net win rates of SoT and SoT-R on different question categories of Vicuna-80 dataset using the general quality metric from LLMZoo. Blue dots are from Fig. 5b. SoT-R correctly falls back to normal decoding on questions where SoT is not suitable. 44Published as a conference paper at ICLR 2024 -60% -40% -20% 0% 20% 40% Philosophy Counterfactual Ethics T echnology Literature Music Sport Roleplay History T oxicity Physics Biology Art Common-Sense Law TruthfulQA Computer Science Academic Writing Chemistry Math Economy Reasoning Writting Medicine Entertainment Code Generation Multilingual Complex Format Code Debug SoT (w/o router) SoT-R w/ prompting router SoT-R w/ trained router SoT-R w/ human router Figure 24: Net win rates of SoT and SoT-R on different question categories of WizardLM dataset using the general quality metric from FastChat. SoT-R correctly falls back to normal decoding on questions where SoT is not suitable. 1.0 1.2 1.4 1.6 1.8 Speed-up −0.2 0.0 0.2 0.4 Net win rates Vicuna-13B V1.3 StableVicuna-13B UltraLM-13B Vicuna-33B V1.3 LLaMA2-Chat-7B LLaMA2-Chat-13B Vicuna-7B V1.3ChatGPT-3.5 Claude Vicuna-7B V1.1 OpenChat-13B GPT-4 Baseline Figure 25: The net win rates and speed-ups of SoT with router (SoT-R) compared to normal gener- ation on Vicuna-80. The net win rate is the difference between the fraction of questions that SoT-R has better and worse answers than normal generation. The speed-up is the ratio between the latency of normal and SoT-R generation. (1.0, 0.0) represents normal generation. Higher is better on both axes. For most models, SoT-R not only accelerates the generation but also improves the quality of the answers (evaluated with LLMZoo metric (Chen et al., 2023c)). 45Published as a conference paper at ICLR 2024 0% 20% 40% 60% 80% 100% SoT vs. Normal SoT vs. Normal Long 59.5% 32.4% 24.3% 40.5% 16.2% 27.0% Win Tie Lose (a) ChatGPT-3.5. 0% 20% 40% 60% 80% 100% SoT vs. Normal SoT vs. Normal Long 21.6% 2.7% 75.7% 91.9% 2.7% 5.4% Win Tie Lose (b) LLaMA2-Chat-7B. Figure 26: Win/tie/lose rates of SoT v.s. longer normal generation. Evaluated only on the questions that we manually label as being suitable for SoT. Evaluated using “general” metrics from FastChat and LLMZoo. SoT/Normal SoT/Normal Long 0.5 1.0 1.5 2.0response length ratio (a) ChatGPT-3.5. SoT/Normal SoT/Normal Long 0.5 1.0 1.5 2.0response length ratio (b) LLaMA2-Chat-7B. Figure 27: Length ratios of SoT generated answer to normal generated answer. “Normal” refers to the normal generation using solely the request as the prompt; “Normal Long” refers to the normal generation using the additional “... give a long answer...” instruction in the prompt. I.3 Q UALITY COMPARISON WITH LONGER NORMAL ANSWER When assessing the answer quality, the GPT-4 judge might exhibit bias towards longer responses. To take this factor into consideration, we add a comparison between a longer sequentially generated answer and the SoT generated answer. Specifically, we add a instruction prefix to the prompt for normal generation. The prefix is “Please give a slightly long answer for the following question.” and “Please give a long answer for the following question.” for ChatGPT-3.5 and LLaMA2-Chat- 7B, respectively. Fig. 27 shows the ratios of the length of SoT answers to normal answers, and Fig. 26 shows the quality comparison. We can see that for both models, when the overall answer lengths are similar, the quality of the SoT answer is comparable to that of the long normal answer. I.4 C HATGPT-3.5 AS THE JUDGE In this section, we provide quality evaluation results with ChatGPT-3.5 as the judge in FastChat and LLMZoo metrics. Note that as prior work (e.g., (Li et al., 2023b)) shows, GPT-4-based evaluation usually aligns with human better than ChatGPT-3.5. Therefore, readers should refer to the results in the main paper (with GPT-4 as the judge) for a more accurate view of the performance of SoT. However, the takeaway messages from ChatGPT-3.5 are similar to the ones from GPT-4. 46Published as a conference paper at ICLR 2024 I.4.1 O VERALL QUALITY In Fig. 28, we show the win/tie/lose rates (the percentage of the cases when SoT wins/ties/loses compared to normal generation) across all models and questions using the two metrics from FastChat and LLMZoo that capture the general quality of the answers. We notice a discrepancy between the two metrics on when SoT is strictly better than the baseline (50.2% v.s. 12.4%). Despite that, the two metrics agree that SoT is not worse than the baseline in more than 76% of the cases. For FastChat metric, we also show the rates excluding math and coding questions that SoT is not suitable for (see § 3.2.3); SoT is not worse than the baseline in more than 89% of the cases. This result suggests that the answers of SoT maintain good quality. 0% 20% 40% 60% 80% 100% General quality (LLMZoo) General quality (FastChat) (excluding math & coding) General quality (FastChat) 50.2% 12.5% 12.4% 27.3% 76.7% 69.2% 22.5% 10.8% 18.4% Win Tie Lose Figure 28: Win/tie/lose rates of SoT v.s. normal generation using “general” metrics from FastChat and LLMZoo. SoT performs better than or equal to normal generation in around 80% of cases. (Evaluated using ChatGPT-3.5 as the judge.) I.4.2 Q UALITY BREAKDOWN : Q UESTION CATEGORIES Next, we investigate how SoT performs on different question categories. We compute net win rates (win rates minus lose rates) across all question categories in Fig. 29. Similar to Fig. 28, we see that LLMZoo tends to be more optimistic about the quality of SoT than FastChat. Nevertheless, the conclusions are consistent: SoT performs relativelywell on generic, common-sense, knowledge, roleplay, and counterfactual. SoT performs relatively badly on writing, fermi, math, and coding. -60% -50% -40% -30% -20% -10% 0% 10% 20% counterfactual roleplay knowledge generic common-sense fermi writing math coding (a) Metric: general quality (FastChat). 0% 10% 20% 30% 40% counterfactual roleplay knowledge generic common-sense fermi writing (b) Metric: general quality (LLMZoo). Figure 29: Net win rates of SoT on different question categories. (Evaluated using ChatGPT-3.5 as the judge.) I.4.3 Q UALITY BREAKDOWN : M ODELS Next, we investigate how SoT performs on different models. We compute net win rates across all models in Fig. 30. Again, we see that the two general metrics from FastChat and LLMZoo have different absolute values but similar rankings. In particular, both metrics agree that OpenChat- 13B, Vicuna-7B V1.1, Claude, ChatGPT-3.5 have low net win rates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B have high net win rates. I.4.4 Q UALITY BREAKDOWN : Q UESTION CATEGORIES AND MODELS In the main text, we analyze how question categories and models affect SoT’s answer quality. Here, we show the per-model and per-category results. For each model and question category, we compute the net win rates. The results are in Fig. 31. 47Published as a conference paper at ICLR 2024 -15% -10% -5% 0% 5% Vicuna-13B V1.3 StableVicuna-13B UltraLM-13B Vicuna-33B V1.3 GPT-4 LLaMA2-Chat-7B LLaMA2-Chat-13B Vicuna-7B V1.3 ChatGPT-3.5 Claude Vicuna-7B V1.1 OpenChat-13B (a) Metric: general quality (FastChat). -10% 0% 10% 20% 30% 40% 50% 60% Vicuna-13B V1.3 StableVicuna-13B UltraLM-13B Vicuna-33B V1.3 GPT-4 LLaMA2-Chat-7B LLaMA2-Chat-13B Vicuna-7B V1.3 ChatGPT-3.5 Claude Vicuna-7B V1.1 OpenChat-13B (b) Metric: general quality (LLMZoo). Figure 30: Net win rates of SoT on different models. (Evaluated using ChatGPT-3.5 as the judge.) Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BStableVicuna-13BVicuna-7B V1.1Vicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3UltraLM-13B Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average -63% -43% -71% 0% -100% -86% -71% -57% -86% 43% -86% -100% -100% -53% -33% -33% -67% -67% -100% -100% -67% 0% 33% -33% -100% -67% -8% 10% 20% 10% -40% -40% -30% 20% 50% 0% -30% -40% -30% 6% 20% 0% -40% 10% 0% 10% 10% 0% 0% 20% 30% 10% -12% 0% 0% -30% 10% -10% 10% 10% 0% -10% -30% 0% -90% 4% 0% 0% -10% 20% -10% 0% 10% 10% 0% 0% 0% 30% 2% -10% 0% -50% 30% 0% 0% 0% 10% 0% 0% 0% 50% 18% 0% 0% 20% 40% 20% 20% 40% 0% 0% -10% -10% 90% 2% 0% -20% -20% 20% -10% 0% 10% 0% -20% 10% 10% 40% -12% -6% -12% -21% -9% -26% -18% -3% -2% 5% -18% -23% -7% -100% -75% -50% -25% 0% 25% 50% 75% 100% (a) FastChat metric. Avgerage LLaMA2-Chat-7BLLaMA2-Chat-13BOpenChat-13BStableVicuna-13BVicuna-7B V1.1Vicuna-7B V1.3Vicuna-13B V1.3Vicuna-33B V1.3UltraLM-13B Claude ChatGPT-3.5 GPT-4 coding math fermi roleplay writing knowledge generic counterfactual common-sense Average 10% 0% -57% 0% 17% 14% -14% 43% 57% 71% -29% 14% 0% 14% 0% 67% 0% 67% -33% -100% 67% 33% 67% 33% -67% 33% 20% 40% 60% -10% 20% -50% 30% 20% 50% 30% -10% 20% 40% 28% 20% 0% -10% 90% -20% 40% 50% 40% 20% 10% 50% 50% 8% 10% -10% 30% 60% 40% 30% 0% -10% 20% -60% -10% 0% 41% 40% 10% -40% 70% 40% 50% 50% 90% 70% 40% 40% 30% 26% -40% -30% -10% 90% 10% 60% 70% 30% 70% -40% 30% 70% 47% 60% 30% 60% 10% 0% 70% 80% 40% 70% -10% 50% 100% 24% 0% -30% -40% 90% -20% 20% 80% 50% 80% -20% 20% 60% 24% 14% 4% -2% 57% -2% 21% 51% 42% 55% -9% 16% 43% -100% -75% -50% -25% 0% 25% 50% 75% 100% (b) The “general” metric from LLMZoo. Figure 31: Net win rates of different models and question categories. Each row corresponds to one question category, and one column corresponds to one model. (Evaluated using ChatGPT-3.5 as the judge.) I.4.5 Q UALITY BREAKDOWN : M ETRICS All previous evaluations use metrics about the general quality of the answer. In Fig. 32, we show more detailed metrics from LLMZoo to reveal in which aspects SoT can improve or hurt the answer quality. On average, we can see that SoT improves the diversity and relevance while hurting the immersion and coherence. 0% 20% 40% 60% 80% 100% Coherence Immersion Integrity Relevance Diversity 28.3% 32.7% 34.5% 50.0% 49.4% 31.4% 28.6% 34.9% 21.8% 29.0% 40.2% 38.7% 30.6% 28.2% 21.5% Win Tie Lose Figure 32: Win/tie/lose rates of SoT v.s. normal generations using metrics from LLMZoo. SoT per- forms well on diversity and relevance, and relatively worse on coherence and immersion. (Evaluated using ChatGPT-3.5 as the judge.) J C OMBINING SOT-R WITH MODEL QUANTIZATION Model quantization is a widely-used model-level optimization to accelerate LLM inference, which is orthogonal to SoT. In this section, we evaluate the speed-ups of open-source models with both 48Published as a conference paper at ICLR 2024 quantization and SoT on the Vicuna-80 dataset. Specifically, we adopt GPTQ (Frantar et al., 2022)8 to apply 4-bit weight-only quantization and use SoT-R instead of plain SoT. J.1 S PEED -UPS OF SOT + Q UANTIZATION ON QUANTIZED MODELS We first compare the latency of the quantized models in the normal and SoT modes to evaluate how much SoT can speed up quantized models. Fig. 33 shows the speed-ups of SoT-R on different quantized models. SoT-R obtain 1.08× to 1.99× speed-ups on all the models. Fig. 34 shows the speed-ups of SoT-R on different categories. We can see that on the five question categories for which SoT can provide high-quality answers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT-R can speed up the overall answer generation process by 1.07× to 2.38×. 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 UltraLM-13B Vicuna-7B V1.1 Vicuna-7B V1.3 Vicuna-13B V1.3 OpenChat-13B LLaMA2-Chat-7B Vicuna-33B V1.3 LLaMA2-Chat-13B 1.08× 1.20× 1.24× 1.24× 1.40× 1.68× 1.91× 1.95× (a) SoT-R with the prompting router. 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 UltraLM-13B Vicuna-13B V1.3 Vicuna-7B V1.1 Vicuna-7B V1.3 OpenChat-13B LLaMA2-Chat-7B Vicuna-33B V1.3 LLaMA2-Chat-13B 1.09× 1.26× 1.29× 1.30× 1.51× 1.80× 1.80× 1.99× (b) SoT-R with the trained router. Figure 33: Speed-ups of the quantized model with SoT-R generation w.r.t. the quantized model with normal generation on different models, on the Vicuna-80 dataset. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 math coding writing fermi roleplay common-sense counterfactual knowledge generic 0.87× 0.94× 1.01× 1.02× 1.18× 1.96× 1.98× 2.06× 2.18× (a) SoT-R with the prompting router. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 fermi writing math coding roleplay common-sense knowledge counterfactual generic 0.99× 0.99× 1.00× 1.00× 1.07× 2.03× 2.04× 2.05× 2.38× (b) SoT-R with the trained router. Figure 34: Speed-ups of the quantized model with SoT-R generation w.r.t. the quantized model with normal generation, on different question categories of the Vicuna-80 dataset. J.2 S PEED -UPS OF SOT + Q UANTIZATION ON UNQUANTIZED MODELS Here, we report the overall speed-ups of the quantization model with SoT-R generation w.r.t. the unquantized model with normal generation. Fig. 35 shows the speed-ups of SoT-R on different models. SoT-R can obtain 1.54× to 2.07× speed-ups. Fig. 36 shows the speed-ups of SoT-R on different categories. On the five question categories for which SoT can provide high-quality answers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT-R can speed up the generation by 1.33× to 3.41× with the prompting and trained routers. 8https://github.com/qwopqwop200/GPTQ-for-LLaMa 49Published as a conference paper at ICLR 2024 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Vicuna-33B V1.3 Vicuna-13B V1.3 Vicuna-7B V1.1 UltraLM-13B LLaMA2-Chat-7B OpenChat-13B LLaMA2-Chat-13B Vicuna-7B V1.3 1.59× 1.63× 1.71× 1.82× 1.84× 1.90× 1.93× 1.95× (a) SoT-R with the prompting router. 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Vicuna-33B V1.3 Vicuna-13B V1.3 Vicuna-7B V1.1 LLaMA2-Chat-7B LLaMA2-Chat-13B UltraLM-13B Vicuna-7B V1.3 OpenChat-13B 1.54× 1.64× 1.76× 1.91× 1.91× 1.92× 1.98× 2.07× (b) SoT-R with the trained router. Figure 35: Speed-ups of the quantized model with SoT-R generation w.r.t. the unquantized model with normal generation, on different models, on the Vicuna-80 dataset. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 math coding fermi writing roleplay counterfactual knowledge common-sense generic 0.92× 0.96× 1.05× 1.18× 1.33× 2.36× 2.53× 2.76× 3.07× (a) SoT-R with the prompting router. 1.0 1.5 2.0 2.5 3.0 3.5 4.0 math writing fermi coding roleplay counterfactual knowledge common-sense generic 1.00× 1.00× 1.00× 1.00× 1.49× 2.42× 2.45× 2.82× 3.41× (b) SoT-R with the trained router. Figure 36: Speed-ups of the quantized model with SoT-R generation w.r.t. the unquantized model with normal generation, on different question categories of the Vicuna-80 dataset. K A DDITIONAL SOT-R STATISTICS K.1 N UMBER OF SUITABLE QUESTIONS Overall, there are 37/80, 58/218, 371/1030 questions that are suitable for SoT in the Vicuna-80, WizardLM, and LIMA datasets (according to human assessment), respectively. Fig. 37 shows the number of questions that are suitable for SoT on Vicuna-80. On counterfactual, commen-sense, knowledge, generic categories, most questions are suitable for SoT based on the human assessment. The trained router and prompting router give out similar judgments. 0 2 4 6 8 10 generic knowledge roleplay common-sense fermi counterfactual coding math writing Trained router Prompting router Human router Figure 37: Number of questions suitable for SoT on the Vicuna-80 dataset. K.2 P EAK MEMORY OVERHEAD Fig. 38 and Fig. 39 show the peak memory overhead of SoT-R (with prompting router) on different models and different categories, respectively, on the Vicuna-80 dataset. We can see that, on all models and categories, the overhead of peak memory is quite small (<1.11×). 50Published as a conference paper at ICLR 2024 0 10 20 30 40 50 60 70 80 Peak memory (GiB) LLaMA2-Chat-7B LLaMA2-Chat-13B OpenChat-13B Vicuna-7B V1.3 Vicuna-13B V1.3 Vicuna-33B V1.3 StableVicuna-13B UltraLM-13B Vicuna-7B V1.1 1.02× 1.02× 1.02× 1.02× 1.03× 1.02× 1.03× 1.03× 1.02× Normal (prefill) SoT (prefill) (a) Peak memory in the prefilling phase. 0 10 20 30 40 50 60 70 80 Peak memory (GiB) LLaMA2-Chat-7B LLaMA2-Chat-13B OpenChat-13B Vicuna-7B V1.3 Vicuna-13B V1.3 Vicuna-33B V1.3 StableVicuna-13B UltraLM-13B Vicuna-7B V1.1 1.05× 1.04× 1.04× 1.05× 1.06× 1.04× 1.07× 1.06× 1.04× Normal (decode) SoT (decode) (b) Peak memory in the decoding phase. Figure 38: Peak memory overhead of SoT-R on different models on the Vicuna-80 dataset. 0 10 20 30 40 50 60 70 80 Peak memory (GiB) generic knowledge roleplay common-sense fermi counterfactual coding math writing 1.05× 1.04× 1.01× 1.05× 1.01× 1.04× 1.00× 1.00× 1.01× Normal (prefill) SoT (prefill) (a) Peak memory in the prefilling phase. 0 10 20 30 40 50 60 70 80 Peak memory (GiB) generic knowledge roleplay common-sense fermi counterfactual coding math writing 1.11× 1.09× 1.03× 1.10× 1.02× 1.10× 1.00× 1.00× 1.01× Normal (decode) SoT (decode) (b) Peak memory in the decoding phase. Figure 39: Peak memory overhead of SoT-R on different question categories of Vicuna-80. K.3 S PEED -UPS WITH DIFFERENT NUMBER OF POINTS Fig. 40 shows the speed-ups with different numbers of points on Vicuna-80. To maintain clarity in the figure, we’ve chosen to display statistics for only three models. Note that as SoT cannot control the overall length to be the same as that of normal generation, it is not the case that a higher number of points leads to higher speed-ups. 3 4 5 6 7 8 9 10 11 Number of Points 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5Speed-up LLaMA2-Chat-7B UltraLM-13B GPT-4 Figure 40: The speed-ups with different number of points on the Vicuna-80 dataset. L N OTES ON APPLICATION SCENARIOS In a chatbot application, one might wonder why a reduced end-to-end latency can enhance the user experience. While human reading speeds are limited, there are many situations where we do not read responses sequentially. Rather than reading the entire answer, one might prefer to (1) swiftly check the response’s structure to confirm if the chatbot comprehended the question or (2) extract specific information rapidly without waiting for the generation of prologue or preceding points. Besides, from the quality aspect, even if we would like to check the entire answer, a well-defined structure in responses assists us in quickly parsing all the information. Moreover, beyond enhancing user experience, reduced end-to-end latency can significantly benefit emerging application scenarios like agent-agent interaction. 51",
      "references": [
        "Graph of thoughts: Solving elaborate problems with large language models",
        "Language models are few-shot learners",
        "Once-for-all: Train one network and specialize it for efficient deployment",
        "Accelerating large language model decoding with speculative sampling",
        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
        "Dynamic n: M fine-grained structured sparse attention mechanism",
        "Llm zoo: democratizing chatgpt",
        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
        "Scaling instruction-finetuned language models",
        "Flashattention: Fast and memory-efficient exact attention with io-awareness",
        "Exploiting linear structure within convolutional networks for efficient evaluation",
        "Enhancing chat language models by scaling high-quality instructional conversations",
        "Glm: General language model pretraining with autoregressive blank infilling",
        "Neural architecture search: A survey",
        "Hierarchical neural story generation",
        "Turbotransformers: an efficient gpu serving system for transformer models",
        "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
        "Gptq: Accurate post-training quantization for generative pre-trained transformers",
        "Compressing large-scale transformer-based models: A case study on bert",
        "Assisted generation: a new direction toward low-latency text generation",
        "Non-autoregressive neural machine translation",
        "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
        "Large language models can self-improve",
        "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
        "Data movement is all you need: A case study on optimizing transformers",
        "Llmlingua: Compressing prompts for accelerated inference of large language models",
        "Reformer: The efficient transformer",
        "Large language models are zero-shot reasoners",
        "Quantizing deep convolutional networks for efficient inference: A whitepaper",
        "One weird trick for parallelizing convolutional neural networks",
        "Efficient memory management for large language model serving with pagedattention",
        "Gshard: Scaling giant models with conditional computation and automatic sharding",
        "The power of scale for parameter-efficient prompt tuning",
        "Fast inference from transformers via speculative decoding",
        "Camel: Communicative agents for ”mind” exploration of large scale language model society",
        "A hierarchical neural autoencoder for paragraphs and documents",
        "Prefix-tuning: Optimizing continuous prompts for generation",
        "Alpacaeval: An automatic evaluator of instruction-following models",
        "Making language models better reasoners with step-aware verifier",
        "Terapipe: Token-level pipeline parallelism for training large-scale language models",
        "Awq: Activation-aware weight quantization for llm compression and acceleration",
        "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
        "Roberta: A robustly optimized bert pretraining approach",
        "Decoupled weight decay regularization",
        "Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks",
        "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification",
        "Accelerating sparse deep neural networks",
        "Pipedream: Generalized pipeline parallelism for dnn training",
        "Memory-efficient pipeline-parallel dnn training",
        "Fast transformer decoding: One write-head is all you need",
        "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
        "High-throughput generative inference of large language models with a single gpu",
        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
        "Blockwise parallel decoding for deep autoregressive models",
        "Spectr: Fast speculative decoding via optimal transport",
        "Rethinking the inception architecture for computer vision",
        "Alpaca: A strong, replicable instruction-following model",
        "Llama: Open and efficient foundation language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Openllms: Less is more for open-source models",
        "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
        "Linformer: Self-attention with linear complexity",
        "Self-consistency improves chain of thought reasoning in language models",
        "Dice semimetric losses: Optimizing the dice score with soft labels",
        "Finetuned language models are zero-shot learners",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Learning structured sparsity in deep neural networks",
        "Smoothquant: Accurate and efficient post-training quantization for large language models",
        "A survey on non-autoregressive generation for neural machine translation and beyond",
        "Wizardlm: Empowering large language models to follow complex instructions",
        "Gspmd: general and scalable parallelization for ml computation graphs",
        "React: Synergizing reasoning and acting in language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Orca: A distributed serving system for {Transformer-Based} generative models",
        "Big bird: Transformers for longer sequences",
        "Star: Bootstrapping reasoning with reasoning",
        "Data-centric artificial intelligence: A survey",
        "Bytetransformer: A high-performance transformer boosted for variable-length inputs",
        "Cumulative reasoning with large language models",
        "Alpa: Automating inter-and {Intra- Operator} parallelism for distributed deep learning",
        "Judging llm-as-a-judge with mt-bench and chatbot arena",
        "Lima: Less is more for alignment",
        "{PetS}: A unified framework for {Parameter-Efficient} transformers serving",
        "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
        "Neural architecture search with reinforcement learning",
        "Introducing claude",
        "Tensorflow serving",
        "Long and diverse text generation with planning-based hierarchical variational model",
        "Data-to-text generation with content selection and planning",
        "Zero: Memory optimizations toward training trillion parameter models",
        "{ZeRO-Offload}: Democratizing {Billion-Scale} model training",
        "Accelerating transformer inference for translation via parallel decoding",
        "Toolformer: Language models can teach themselves to use tools"
      ],
      "meta_data": {
        "arxiv_id": "2307.15337v3",
        "authors": [
          "Xuefei Ning",
          "Zinan Lin",
          "Zixuan Zhou",
          "Zifu Wang",
          "Huazhong Yang",
          "Yu Wang"
        ],
        "published_date": "2023-07-28T06:31:34Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Skeleton-of-Thought (SoT) to reduce the end-to-end generation latency of Large Language Models (LLMs) by guiding them to first generate a response skeleton and then complete each point in parallel. SoT achieves considerable speed-ups (up to 2.39x across 12 LLMs) and can improve answer quality for several question categories. The work also introduces SoT with router (SoT-R) to adaptively apply SoT only to suitable questions, further enhancing practicality and addressing limitations on questions requiring step-by-step reasoning.",
        "methodology": "SoT involves a two-stage process: a Skeleton stage where the LLM generates a concise skeleton of the answer using a skeleton prompt template, and a Point-expanding stage where each point of the skeleton is expanded in parallel using a point-expanding prompt template. For proprietary models, parallel API calls are made, while for open-source models, batched decoding is employed. To make SoT more practical, SoT-R integrates a router (either an LLM prompting router or a trained RoBERTa model) to determine if a question is suitable for SoT or if normal sequential decoding should be used.",
        "experimental_setup": "SoT was evaluated on two assistant-style datasets: Vicuna-80 (80 questions across nine categories) and WizardLM (218 questions across more categories). Experiments were conducted with 12 LLMs, including 9 open-source models (e.g., LLaMA2-Chat, Vicuna, OpenChat, UltraLM, StableVicuna) and 3 API-based models (Claude, ChatGPT-3.5, GPT-4). Efficiency was measured by recording API call latency for API-based models and by profiling prefilling and decoding latencies on NVIDIA A100 and RTX 3090 GPUs for open-source models. Answer quality was evaluated using two LLM-based frameworks: FastChat and LLMZoo, with GPT-4 (and ChatGPT-3.5 for some evaluations) acting as the judge. Router consistency was analyzed using confusion matrices, comparing human, prompting, and trained routers.",
        "limitations": "The answer quality evaluation is imperfect due to a limited prompt set, potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM generations, with no human evaluation conducted to avoid bias. SoT is not suitable for questions requiring step-by-step reasoning (e.g., math, fermi questions, some coding problems) where latter steps depend on earlier ones. The current SoT pipeline can lead to less coherent and immersive answers for writing tasks by explicitly embedding skeleton points. SoT significantly increases prefilling token usage for API-based models, leading to higher costs. While efficient for unsaturated concurrent queries, SoT might hurt serving throughput under saturated conditions. The unpredictable and less controllable acceleration ratio of SoT (compared to model/system-level techniques) due to its dependence on prompt, model, and question, could hinder practical adoption.",
        "future_research_directions": "Future research directions include exploring how to draw from human thinking processes to facilitate more effective and efficient AI, such as organizing points as a \"Graph-of-Thoughts\" to handle dependencies between points, or dynamically adjusting the thought structure. Investigating how SoT answers can fine-tune LLMs to generate more structured answers in a self-improving manner is another avenue. Developing appropriate SoT triggering conditions based on system workloads is important. Reducing token and computational overhead for SoT, potentially by reusing key-value caches or using shorter prompts, is a valuable direction. Further exploration of data-centric efficiency optimization to make acceleration ratios more predictable and controllable is also suggested. Additionally, tailoring point-expanding prompts for specific tasks like coding to improve generation quality and allowing the LLM to decide on the inclusion of point indices for more natural answers are proposed.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
      "full_text": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Kaituo Feng 1 Changsheng Li 1 Xiaolu Zhang 2 Jun Zhou 2 Ye Yuan1 Guoren Wang1 3 Abstract Chain-of-thought distillation is a powerful tech- nique for transferring reasoning abilities from large language models (LLMs) to smaller stu- dent models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowl- edge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, re- sulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues. Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we de- velop an in-rationale progressive distillation strat- egy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to as- sess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity. Extensive experiments on four reason- ing benchmarks illustrate our KPOD outperforms previous methods by a large margin. 1. Introduction Large language models (LLMs) have demonstrated re- markable reasoning capabilities via chain-of-thought (CoT) 1Beijing Institute of Technology 2Ant Group 3Hebei Province Key Laboratory of Big Data Science and Intelligent Technology. Correspondence to: Changsheng Li <lcs@bit.edu.cn>. Proceedings of the41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). prompting (e.g., “Let’s think step-by-step”), which prompts LLMs to generate a step-by-step rationale to help reasoning (Kojima et al., 2022; Wei et al., 2022). However, such abil- ities usually emerge in extremely large models, especially those with over 100 billion parameters (Fu et al., 2023; Hoff- mann et al., 2022) , such as 175B GPT-3 (Brown et al., 2020) and 540B PaLM (Chowdhery et al., 2023). The substantial amount of parameters unavoidably leads to high inference costs and makes it challenging to deploy LLMs in environ- ments with limited computational resources (Hsieh et al., 2023). To tackle with this, a recent surge of works, known as CoT distillation, has arisen as a promising avenue to distill reasoning capabilities of LLMs to smaller student models (Li et al., 2023; Wang et al., 2023b; Fu et al., 2023). The core idea of these methods is to require the student model to mimic the step-by-step rationale generated by LLMs in response to a question. However, current CoT distillation methods often encounter the following two issues: First, in a rationale, each token car- ries different levels of importance in the reasoning process. Certain keypoint tokens play a pivotal role in reasoning, while other tokens are of less importance or even irrelevant to the reasoning process. For instance, consider a step in a rationale: “Next, we just need to simply add up the calories from the lettuce and cucumber: 30 + 80 = 110”. Here, terms like “just”, “simply” are reasoning-irrelevant, whereas the calculation “30 + 80 = 110” stands out as the keypoint for reasoning. The reasoning-irrelevant tokens can be replaced without negative effects, but even a slight deviation from the keypoint token could result in errors in reasoning. There- fore, it’s crucial for the student model to focus on the precise mimicry of these keypoint tokens. Nevertheless, previous CoT distillation methods usually treat all tokens equally during distillation (Li et al., 2023; Wang et al., 2023b). The second issue stems from the fact that previous ap- proaches usually demand the student model to consistently learn all the steps in a rationale throughout the distillation process, without distinguishing the learning order of step generation. This distillation strategy diverges from the hu- man cognitive pattern that progresses from easier tasks to more challenging ones. This deviation might lead to sub- optimal outcomes. In the process of human or biological agent learning, ability acquisition doesn’t simply stem from random tasks (Molina & Jouen, 1998). Instead, there is an 1 arXiv:2405.16064v1  [cs.CL]  25 May 2024Keypoint-based Progressive Chain-of-Thought Distillation for LLMs organized progression from easy tasks to hard tasks for them to acquire capabilities, especially for complex skills such as reasoning (Peterson, 2004; Krueger & Dayan, 2009; Benoit et al., 2013). In the field of machine learning, this ordered learning paradigm is regarded as curriculum learning (Ben- gio et al., 2009). Inspired by this, we intend to develop a progressive CoT distillation strategy to facilitate the student model acquire reasoning ability from easy to hard. However, directly applying previous curriculum learning strategies to CoT distillation could be inferior because of the following two reasons: (i) They overlook the step-by-step reasoning nature where each reasoning step within a rationale may pos- sess varying reasoning difficulty, resulting in sub-optimal difficulty assessment. (ii) As aforementioned, a step in the rationale might contain many tokens that are not crucial to the reasoning process. When assessing the difficulty of step generation, it may be dominated by these inessential tokens, thereby inaccurately reflecting the challenge of obtaining the expected outcome for a reasoning step. In this paper, we propose Keypoint-based Progressive CoT Distillation for LLMs dubbed KPOD, with the goal of ad- dressing the above two issues in a unified framework. First, we propose a rationale token weighting module to determine the token significance for distillation. It learns to generate masks for inessential tokens to the reasoning process via two distinctive loss functions: An answer prediction loss is introduced to encourage the module to utilize the ques- tion with the masked rationale to derive the answer, while a mask ratio loss is designed to maximize the ratio of masked tokens in the rationale. By doing so, the obtained proba- bility of not masking a token can serve as an indicator of its significance weight. Second, we develop an in-rationale progressive distillation strategy that orders the learning se- quence from easy reasoning to hard reasoning within the rationale of a question. This strategy begins by training the student model to generate the last few reasoning steps of the rationale, given the question with preceding steps of this rationale as input. Subsequently, it progressively extends to generate the entire rationale using only the question as input. To precisely assess each step’s reasoning difficulty, we propose a token generation loss based on the derived token significance, aiming to eliminate the negative effects of reasoning-irrelevant tokens. Finally, we design a value function to dynamically determine the number of steps taken as input at each stage, thereby automatically adjusting their learning difficulty. Meanwhile, we leverage the value func- tion to select diverse questions, so as to prevent over-fitting (Jiang et al., 2014; Liang et al., 2021). Our contributions can be summarized as: 1) We propose a general and principled framework for CoT distillation, which simultaneously considers token significance and rea- soning difficulty within a rationale during distillation. 2) We design a rationale token weighting module through mask learning to determine the token significance for reasoning. This allows the student to concentrate more on keypoint tokens. 3) We devise an in-rationale progressive CoT distil- lation strategy to schedule the learning order of reasoning steps within a rationale. This enables the student to progres- sively acquire reasoning abilities in an easy-to-hard manner. 4) Extensive experiments on four reasoning benchmarks val- idate the effectiveness of our KPOD, showcasing significant performance improvements compared to baselines. 2. Related Works Chain-of-Thought Reasoning. The concept of employing step-by-step language rationales to aid in solving reason- ing problems can be traced back to pioneering works (Ling et al., 2017). Inspired by this, chain-of-thought prompting (Wei et al., 2022) has been proposed to enable LLMs to gen- erate intermediate reasoning steps that contribute to the final answer via few-shot CoT demonstrations. This prompting approach has illustrated remarkable performance gain for LLMs in reasoning related tasks (Zhang et al., 2022; Wang et al., 2023a). In addition, researchers find that LLMs can also obtain impressive reasoning performance by zero-shot CoT (Kojima et al., 2022) without task-related demonstra- tions. This is achieved by only using a single sentence “Let’s think step by step” for prompting. Recently, a number of CoT prompting methods have demonstrated effectiveness in enhancing the reasoning performance of LLMs (Diao et al., 2023; Yang et al., 2023), such as SC-CoT (Wang et al., 2022), Auto-CoT (Zhang et al., 2022), Multimodal-CoT (Zhang et al., 2023), etc. However, the emergence of CoT reasoning capabilities in LLMs typically requires models with more than 100 billion parameters (Wei et al., 2022; Fu et al., 2023), making it resource-consuming for deployment. CoT Distillation. Knowledge distillation has been widely studied for model compression across various fields (Mag- ister et al., 2023; Feng et al., 2024). Recently, CoT Distil- lation has emerged as a promising avenue to transfer the step-by-step reasoning capabilities of LLMs to smaller stu- dent models (Hsieh et al., 2023; Ho et al., 2023). The key idea of CoT distillation is to make the student model mimic the step-by-step rationale generated by LLMs in response to a question. In this context, the rationale can be interpreted as the LLMs’ explanation of how to derive the final answer of a question, akin to the soft label used in conventional knowl- edge distillation (Hinton et al., 2015; Feng et al., 2022). The representative works of CoT distillation include: SCoTD (Li et al., 2023) introduces a symbolic CoT distillation method that enables smaller models to self-rationalize for reasoning via learning rationales from LLMs. Specialized KD (Fu et al., 2023) is proposed to train a small language model spe- cialized for reasoning in four distinct in-context scenarios. MCC-KD (Chen et al., 2023) adopts diverse rationales for 2Keypoint-based Progressive Chain-of-Thought Distillation for LLMs distillation and attempts to ensure their consistency. SCOTT (Wang et al., 2023b) designs a faithful CoT distillation strat- egy to make the student reason faithfully via counterfactual training. However, these methods fail to consider the reason- able learning order of the reasoning steps within a rationale, leading to sub-optimal performance. Curriculum Learning. Early researches in cognitive sci- ence emphasize the significance of the easy-to-hard learning pattern to acquire knowledge (Elman, 1993). Inspired by this, the pioneer work (Bengio et al., 2009) introduces the concept of curriculum learning (CL) to the machine learn- ing field by gradually including samples from easy to hard for training. In recent years, a variety of CL methods have been proposed to enhance the model performance (Kong et al., 2021; Wang et al., 2021). For instance, Adaptive CL (Kong et al., 2021) proposes to utilize the loss of the model to dynamically adjust the difficulty score of each sample. SPL (Wan et al., 2020) introduces the curriculum learning to the neural machine translation domain via introducing the token-level and sentence-level confidence score. ICL (Jia et al., 2023) devises a curriculum learning method that organizes the curriculum within the token sequence of a sample for natural language generation tasks. However, as aforementioned, applying these CL methods directly to CoT distillation could yield inferior performance. 3. Proposed Method 3.1. Preliminaries and Problem Setting The goal of CoT distillation is to transfer the reasoning ca- pability of large language models (LLMs) to smaller student models via distilling the rationales produced by LLMs. We denote the dataset as D = {(x(i), y(i))}, where x(i) is the i-th reasoning question and y(i) is the corresponding answer. Following previous CoT distillation works (Ho et al., 2023; Chen et al., 2023) , we adopt zero-shot CoT (Kojima et al., 2022) to prompt the teacher LLMs to generate step-by-step rationale r(i) for each question x(i). The reasoning template takes the following format: “ Q: <x(i)> A: <p> <r(i)> Therefore, the answer is<y(i)>”, where <p> is the zero- shot CoT prompt such as “Let’s think step by step”. Then, the student is trained to generate the concatenated sequence of rationale tokens r(i) and answer tokens y(i), given the question x(i) as input. The standard negative log-likelihood loss for training the student model can be formulated as: L= − X i X j logP(r(i) j |r(i) <j, x(i); θs) − X i X j logP(y(i) j |y(i) <j, r(i), x(i); θs), (1) where r(i) j and y(i) j represent the j-th token in the rationale sequence r(i) and the answer sequence y(i), respectively. θs denotes the parameters of the student model. The first term of Eq.(1) enables the student to mimic the rationale produced by LLMs, while the second term aims to train the student to output the final answer based on the rationale. By minimizing this loss, the student model can learn to generate the step-by-step rationale for deriving the final answer. 3.2. Framework Overview As aforementioned, there are two key issues for CoT distilla- tion methods: (i) Equally treating each token for distillation may make the student fail to mimic keypoint tokens accu- rately, leading to reasoning errors. (ii) Distilling the steps within a rationale without explicitly considering the learning order of step generation might lead to sub-optimal outcomes. To tackle these two issues, we propose a new CoT distil- lation framework KPOD, as illustrated in Figure 1. Our framework mainly consists of two components: a rationale token weighting component based on mask learning is pro- posed to determine the token significance for distillation. This encourages the student to faithfully replicate the cru- cial keypoint tokens; A progressive distillation component within the rationale is designed to establish a structured learning order for the reasoning steps. This guides the stu- dent model to progressively develop its reasoning abilities from simpler to more complex tasks, aligning with the profi- ciency of teacher LLMs. It’s worth noting that the obtained token significance weight fulfills two distinct functions in our framework: firstly, it encourages precise mimicry of key- point tokens during distillation, and secondly, it mitigates the negative effects of inessential tokens when assessing step difficulty. Next, we will primarily delve into the detailed introduction of the two components in our framework. 3.3. Rationale Token Weighting In this section, we introduce our rationale token weighting module, which determines the significance of each token via learning to mask reasoning-irrelevant token. Weight Generation. First, we intend to generate distinct significance weights for different tokens by leveraging their embeddings. This facilitates the estimation of their impor- tance according to their characteristics. To achieve this, we feed the rationale tokens into a pre-trained input embedding layer, followed by a self-attention layer to encode in-context information. This process is formulated as: e(i) = Att(Emb(r(i))), (2) where Emb and Att denote the input embedding layer and the self-attention layer, respectively. e(i) is the embedding matrix containing the embeddings for each token in r(i). Subsequently, the embedding e(i) j of each token is fed into a weight generator, producing the significance weight as: w(i) j = σ(fw(e(i) j )), (3) 3Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Question A club opens up ...James  2 rounds...  costs $14.   How much did he  spend? Let’s think step-by-step. Teacher LLM Rationale Step 1: He buys 2*5=10 drinks . …… Step 4: The tip… 110*.3=33. Step 5: So … 20+110+33=$163. Therefore, the answers is 163. Smaller Student  Model Keypoint-based Progressive Distillation Input Embedding + Attention tokens embeddings Weight Generator mask mask mask Transformer Layers Answer Input Embedding + Attention tokens embeddings Weight Generator mask mask mask Transformer Layers Answer Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Diversity Generation Probability  Step  Difficulty weighted Value Function increase difficulty or not ? Stage t-1 Stage tInput Embedding + Attention tokens embeddings Weight Generator mask mask mask Transformer Layers Answer Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Step 1 AnswerStep 2 Step 3 Step 4 Step 5 Input Output Question Diversity Generation Probability  Step  Difficulty weighted Value Function increase difficulty or not ? Stage t-1 Stage t Rationale Token Weighting In-rationale Progressive Distillation Strategy token significance  for distillation Figure 1. An illustration of our KPOD framework. KPOD first determines the keypoint tokens for distillation through designing a rationale token weighting module based on mask learning. Then, an in-rationale progressive distillation strategy is devised to organize the learning order within rationale, so as to enable the student to acquire the reasoning capabilities in an easy-to-hard manner. where w(i) j is the probability of the j-th token not being masked, serving as an indicator of its significance level. fw is the weight generator and σ is the sigmoid activation function (Narayan, 1997). In this paper, we employ a simple two-layer MLP as the weight generator. Reasoning-irrelevant Mask Learning. To optimize the weight generator, we formulate two loss functions: an an- swer prediction loss that encourages the module to utilize the question with the masked rationale for answer deriva- tion, and a mask ratio loss aiming to maximize the ratio of masked tokens in the rationale. This allows the weight gen- erator to generate low values of w(i) j for tokens irrelevant to reasoning and high values for keypoint tokens. Next, we will introduce these two losses in detail. Firstly, considering that sampling the discrete mask pol- icy m(i) j ∈ {0, 1} from the distribution of w(i) j is non- differentiable, we adopt the Gumbel-Softmax sampling (Jang et al., 2016) to avoid this issue: m(i) j = GumbelSoftmax(w(i) j ), (4) where GumbelSoftmax represents the Gumbel-Softmax sampling (Jang et al., 2016). m(i) j = 0 denotes that the j-th token is masked, while m(i) j = 1 denotes that the j-th token is not masked. By applying the mask m(i) j to each token r(i) j in rationale r(i), we can obtain the masked rationale, denoted as r[m](i). Then, we input r[m](i) into the transformer layers, with the goal of obtaining the correct answer by using the masked rationale. Here we initialize the transformer layers using the pre-trained FlanT5-Large (Chung et al., 2022). Considering that certain steps of the rationale sometimes contain the reasoning results of previous steps, shortcuts may be taken for the answer prediction via neglecting previous steps. To eliminate this phenomenon, we expect the transformer to predict the answer based on the question with any prefix of the masked rationale. The answer prediction loss Lp for question x(i) can be written as: Lp = − X k X j log P(y(i) j |y(i) <j, r[m](i) <k, x(i); θw), (5) where y(i) is the answer and x(i) is the question. θw repre- sents the parameters of this rationale token weighting mod- ule. r[m](i) <k represents the precedingk tokens of the masked rationale r[m](i). By optimizing Lp, the transformer can be used to predict the answer by taking as input the ques- tion and the prefix of the masked rationale. Meanwhile, the weight generator is encouraged to generate large weights for the keypoint tokens, preventing them from being masked to facilitate the answer prediction. Moreover, to eliminate redundant tokens for reasoning, a mask ratio loss Lm is presented as: Lm = X j m(i) j . (6) By optimizing Lm, we enable the weight generator to iden- tify the insignificant tokens in the reasoning process and generate lower weights for them. Finally, the overall loss function for training this module can be expressed as: Lk = Lp + αLm, (7) 4Keypoint-based Progressive Chain-of-Thought Distillation for LLMs where α is a balancing hyper-parameter. By optimizing Lk, we can achieve the goal of determining the significance weight w(i) j for each token within a rationale. 3.4. In-rationale Progressive Distillation In this section, we elaborate our proposed in-rationale pro- gressive distillation strategy, which schedules the learning order within a rationale. Step Difficulty Assessment. Firstly, we assess the difficulty of each reasoning step in the rationale, so as to facilitate the learning order scheduling. In this work, we utilize the symbol “.” to separate steps in a rationale. As mentioned above, there could exist many reasoning-irrelevant tokens, and it is crucial to ensure that the difficulty evaluation is not influenced by them. Therefore, we propose a weighted token generation loss to calculate the difficulty value d(i) k of the k-th reasoning step in the rationale r(i) as: d(i) k = − qkX j=pk ˆw(i) j log P(r(i) j |r(i) <j, x(i); θs), (8) where pk and qk denote the start position and end posi- tion of the k-th step in the rationale, respectively. Here we directly use the pre-trained student model θs (e.g., LLaMA- 7B (Touvron et al., 2023)) before distillation to evaluate the generation probability P(r(i) j |r(i) <j, x(i); θs). ˆw(i) j = softmax(w(i) j ) represents the significance weight normal- ized by softmax (Bridle, 1989) within the token weights in the k-th step. In this way, the obtained step difficulty can be more concentrated on the difficulty of generating keypoint tokens, providing a more faithful reflection of the difficulty in deriving the correct outcome of each reasoning step. Progressive Distillation. Based on the step difficulty scores, we devise an in-rationale progressive distillation strategy to guide the student model learning each rationale in an easy-to-hard fashion. This strategy initiates with training the student model to generate the final few reasoning steps of the rationale using previous steps combined with the question as input, and progressively expands to produce the complete rationales. Supposed that we schedule the student model to output the last ni − ci(t) steps of the i-th rationale at stage t, The difficulty hi(S(t)) of generating these steps can be formulated as: hi(S(t)) = niX j=ci(t)+1 d(i) j , (9) where ni is the total number of steps in the i-th rationale r(i) and ci(t) is the scheduled number of input steps of r(i) at stage t. d(i) j is the difficulty of the j-th step in r(i). S(t) is used to decide the value of ci(t) at stage t, which will be introduced later. In this paper, we treat each training epoch as a stage. To facilitate selecting diverse questions to increase difficulty at each stage, we configure an overall learning difficulty D(t) for stage t rather than a hard threshold for each ques- tion. This means that the difficulty sum of all questions should not exceed D(t) at stage t. We set the growth rate of D(t) to be dD(t) dt = utp, where p >0 and u >0 are the parameters to control the growth rate. By integrating the growth rate with respect to t, we can derive D(t) as: D(t) = utp+1 p + 1 + C0, (10) where C0 represents the initial overall learning difficulty at stage 0. By letting D(t) achieve the maximum difficulty B of the dataset at stage T: D(T) = B = P i Pni j=1 d(i) j , we can derive u = (B−C0)(p+1) Tp+1 , where p and C0 are the pre-defined hyper-parameters. When entering stage t from stage t − 1, it’s required to select a set of questions to increase difficulty. We achieve this by reducing a number of input steps ∆s for the selected questions as: ci(t) = ci(t − 1) − qi(t) · ∆s, s.t.∆H(S(t)) ≤ ∆D(t), (11) where ci(t) is the scheduled number of input steps of the i-th question at stage t. Let S(t) denote the selected question set for increasing difficulty at stage t. Then, qi(t) ∈ {0, 1} represents whether i belongs to S(t). If i ∈ S(t), then qi(t) = 1 ; otherwise, qi(t) = 0 . ∆s is the pre-defined number for reducing input steps. ∆H(S(t)) =P i hi(S(t)) −P i hi(S(t −1)) is the sum of the increased difficulty and ∆D(t) = D(t) − P i hi(S(t − 1)) is the ceiling magnitude for the increased difficulty. Then, in order to determine whether a question should in- crease difficulty, we design a value function F. The goal of this value function is two-fold: One is to align the increased difficulty as closely as possible with the defined magnitude, and the other is to ensure a diverse set of questions for es- calating difficulty to prevent overfitting (Jiang et al., 2014). The value function F is designed as: F(S(t)) = −(∆D(t)−∆H(S(t)))+β KX k=1 p |Ck∩S(t)|, (12) where β is a trade-off hyper-parameter. The first term mea- sures the closeness of ∆H(S(t)) to ∆D(t) and the second term measures the diversity of selected question set based on clustering. Specifically, Ck is the question set of the k-th cluster and K is the number of clusters. In this paper, we conduct K-means clustering (Bradley et al., 2000) to cluster the question based on its embedding, which is calculated by the average of the GloVe (Pennington et al., 2014) word embedding. S(t) is the selected question set. By using the square root operation, our aim is to promote a balanced distribution of questions within each cluster in the selected 5Keypoint-based Progressive Chain-of-Thought Distillation for LLMs question set. This approach ensures that the diversity of the chosen question set is maintained. The optimization of F(S(t)) can be formulated as: max S(t) F(S(t)), s.t.∆H(S(t)) ≤ ∆D(t). (13) By maximizing F(S(t)), we can achieve the goal of se- lecting diverse questions to increase difficulty with close proximity to ∆D(t). However, this is a combination opti- mization problem subject to the knapsack constraint, and solving it is known to be NP-hard. Fortunately, we can prove that F(S(t)) satisfies the condition of monotone and submodular. Therefore, it can be approximately solved by a submodular maximization algorithm FTGP (Li et al., 2022) in linear time with an approximation ratio guarantee, as formulated in Proposition 3.1. The proof of Proposition 3.1 can be found in Appendix D. Proposition 3.1. The optimization ofmaxS(t) F(S(t)) sub- ject to the knapsack constraint∆H(S(t)) ≤ ∆D(t) can be approximately solved inO(nϵ−1 log ϵ−1) time complex- ity with a1 2 − ϵ approximation ratio guarantee, wheren represents the scale of the data. After obtaining the scheduled input step ci(t) by solving Eq.(13), the rationale distillation loss at stage t can be for- mulated as: Lr(t) = − X i qniX j=pci(t)+1 log P(r(i) j |r(i) <j, x(i); θs), (14) where pci(t)+1 is the start position of the (ci(t) + 1)-th step in the rationale r(i), and qni is the end position of the last step in the rationale r(i). For each rationale, ni is fixed and ci(t) is gradually decreased to 0. In this way, the student model could learn the rationale of each question in an easy- to-hard manner. 3.5. Training Procedure To train our whole framework, we first optimize the rationale token weighting module by Eq.(7) to determine the token significance. Then, we assess the step difficulty and derive the progressive distillation strategy by solving Eq.(13). Fi- nally, by integrating these two modules, the overall loss for distilling the rationale at stage t can be written as: Lo(t) = − X i qniX j=pci(t)+1 w(i) j · log P(r(i) j |r(i) <j, x(i); θs). (15) By optimizing Lo(t), the student model is encouraged to mimic the keypoint tokens precisely, as well as acquiring reasoning capabilities in an easy-to-hard manner. Note that we have omitted the inclusion of the prediction loss term for y(i) (referring to the second term in Eq. (1)), for the sake of clarity, as it remains constant. The pseudo-code of our training procedure is listed in Appendix B. 4. Experiments 4.1. Experiment Setup In this section, we introduce our experiment settings. The implementation details can be found in Appendix A. Datasets. We evaluate our method on both mathematical reasoning tasks and commonsense reasoning tasks, follow- ing (Hsieh et al., 2023; Fu et al., 2023). For mathematical reasoning, we adopt three benchmark datasets for evaluation: GSM8K (Cobbe et al., 2021), ASDiv (Patel et al., 2021) and SV AMP (Miao et al., 2021). For commonsense reason- ing, CommonsenseQA benchmark (Talmor et al., 2019) is employed to evaluate our method. Additionally, we con- duct out-of-distribution (OOD) evaluation via training our method on GSM8K while testing it on ASDiv and SV AMP, following (Fu et al., 2023). The dataset splits can be found in Appendix A. Models and Baselines. We adopt GPT-3.5-Turbo (Ye et al., 2023) as the teacher model to generate the rationale for each question in the dataset via zero-shot CoT prompting (Kojima et al., 2022), following (Chen et al., 2023). This is accessed via the OpenAI’s public API for ChatGPT. As for the student model, we adopt three widely-used pretrained language models of different architectures: LLaMA-7B (Touvron et al., 2023), FlanT5-XL (Chung et al., 2022) and FlanT5- Large (Chung et al., 2022), similar to (Fu et al., 2023; Chen et al., 2023). The parameter counts of LLaMA-7B, FlanT5- XL, FlanT5-Large are 7B, 3B, 760M respectively. As for baselines, we employ four state-of-the-art CoT distillation methods for comparison: Specialized KD (Fu et al., 2023), SCOTT (Wang et al., 2023b), SCoTD (Li et al., 2023), MCC- KD (Chen et al., 2023). Following previous works (Fu et al., 2023), we use the accuracy (%) metric for evaluating the performance of our method and baselines. 4.2. Overall Performance In this section, we evaluate the overall performance of our method. We compare our method with four recent state- of-the-art CoT distillation methods as mentioned before. The GPT-3.5-Turbo serves as the teacher model. Table 1 illustrates the results. The symbol “-” denotes the model without using CoT distillation methods. First, we can ob- serve that CoT distillation methods consistently boost the performance of smaller student models on reasoning tasks, underscoring the effectiveness of distilling rationales. In addition, it’s evident that our proposed KPOD outperforms previous methods by a large margin. For example, compared to MCC-KD, achieving the second best results when using LLaMA-7B as the student model, our approach achieves 5.16%, 5.26%, 4.00%, 1.48% performance gains on the GSM8K, ASDiv, SV AMP, CommonsenseQA datasets, re- spectively. This highlights the effectiveness of promoting 6Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Table 1.Performance comparison of our method and baselines. Models # Params. Distillation Methods Datasets GSM8K ASDiv SV AMP CommonsenseQA GPT-3.5-Turbo unknown - 73.98 79.64 75.14 74.35 LLaMA-7B 7B - 11.00 40.20 32.80 33.90 SCoTD 38.54 63.38 62.67 71.33 Specialized KD 39.15 64.01 63.33 72.32 SCOTT 40.97 62.74 61.33 74.45 MCC-KD 41.58 65.76 64.67 76.41 KPOD (ours) 46.74 71.02 68.67 77.89 FlanT5-XL 3B - 13.50 20.70 17.70 72.70 SCoTD 21.85 25.16 26.67 79.61 Specialized KD 23.22 28.03 25.33 81.16 SCOTT 21.09 25.48 24.67 83.62 MCC-KD 24.28 31.35 30.00 82.88 KPOD (ours) 25.19 33.76 34.67 88.04 FlanT5-Large 760M - 6.90 10.10 6.80 67.60 SCoTD 19.42 20.06 19.33 76.58 Specialized KD 20.03 23.25 20.67 77.23 SCOTT 18.21 21.66 18.67 77.48 MCC-KD 18.36 23.89 21.33 78.13 KPOD (ours) 22.46 27.39 25.33 81.41 Table 2.Ablation study of our method. Models Settings Datasets GSM8K CommonsenseQA LLaMA-7B KPOD-w.o.-sig 42.64 75.18 KPOD-w.o.-sig-dif 44.01 76.49 KPOD-w.o.-prog 43.25 74.61 KPOD-w.o.-div 44.16 75.76 KPOD-ACL 43.55 75.51 KPOD-SPL 42.94 75.84 KPOD-ICL 43.85 75.35 KPOD 46.74 77.89 FlanT5-XL KPOD-w.o.-sig 22.46 85.26 KPOD-w.o.-sig-dif 23.82 86.08 KPOD-w.o.-prog 23.22 84.28 KPOD-w.o.-div 23.98 86.73 KPOD-ACL 23.52 86.40 KPOD-SPL 22.76 85.59 KPOD-ICL 22.91 85.83 KPOD 25.19 88.04 precise mimicry of keypoint tokens and implementing a learning schedule that progresses from easy to challeng- ing tasks. Such an approach facilitates the acquisition of reasoning capabilities by the student model. 4.3. Ablation Study We conduct ablation study to verify the effectiveness of the components in our proposed method. Specifically, we design several variants of our proposed KPOD: KPOD-w.o.- sig denotes our method wherein each token is treated equally, without incorporating the token significance weight for dis- tillation. KPOD-w.o.-sig-dif represents our method without using the token significance weight for calculating the step difficulty. KPOD-w.o.-prog means our method without us- ing the proposed progressive distillation strategy. KPOD- w.o.-div denotes our method without using the diversity term in the value function to select the question set. Besides, we compare our method with three representa- tive curriculum learning methods: Adaptive CL (Kong et al., 2021), SPL (Wan et al., 2020) and ICL (Jia et al., 2023). We design three variants of our method: KPOD- ACL, KPOD-SPL, KPOD-ICL respectively denote replac- ing our in-rationale progressive distillation strategy by Adap- tive CL, SPL and ICL. The results are listed in Table 2. As shown in Table 2, KPOD-w.o.-sig obtains inferior per- formance than KPOD, illustrating the effectiveness of em- phasizing the precise mimicry of keypoint tokens in our method. Besides, KPOD outperforms KPOD-w.o.-sig-dif. This shows that it’s essential to utilizing the token signifi- cance weight for the step difficulty calculation. The perfor- mance of KPOD-w.o.-prog is worse than KPOD, illustrating the effectiveness of scheduling an easy-to-hard learning or- der for CoT distillation. Moreover, KPOD obtains better performance than KPOD-w.o.-div. This demonstrates that ensuring a diverse question set to increase difficulty is effec- tive. Finally, we can find that KPOD surpasses KPOD-ACL, 7Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Table 3.OOD performance of our method and baselines. Model Methods In-distribution OOD GSM8K ASDiv SV AMP LLaMA-7B SCoTD 38.54 55.09 45.33 Specialized KD 39.15 53.82 38.67 SCOTT 40.97 53.50 42.00 MCC-KD 41.58 57.64 41.00 KPOD (ours) 46.74 57.96 47.33 FlanT5-XL SCoTD 21.85 25.48 22.67 Specialized KD 23.22 26.11 24.67 SCOTT 21.09 25.20 25.33 MCC-KD 24.28 28.98 26.67 KPOD (ours) 25.19 32.48 29.33 KPOD-SPL and KPOD-ICL, showing the superiority of our in-rationale progressive distillation strategy compared to previous curriculum learning methods. 4.4. OOD Performance Following (Fu et al., 2023), we examine the out-of- distribution (OOD) generalization ability of the student model trained by our method and baselines. We use the in-distribution mathematical dataset GSM8K for training and adopt OOD mathematical datasets ASDiv, SV AMP for testing, similar to (Fu et al., 2023; Chen et al., 2023). As shown in Table 3, our proposed KPOD consistently obtains superior performance compared to the baselines, indicating that the student model trained by our method has stronger OOD generalization capabilities. 4.5. Visualizations In this section, we visualize the token significance weight w(i) j generated by the weight generator, to intuitively show the effectiveness of the rationale token weighting module. Figure 2 illustrates the visualization results on the GSM8K dataset. First, we can find that the digit tokens and oper- ation tokens obtain the highest weights. This is because these tokens are usually of vital importance in the reasoning process, where even a slight deviation could cause errors. Additionally, several tokens that contribute significantly to the reasoning also exhibit relatively high weights. Tokens such as “twice”, “total”, “adding”, and “dividing” provide instructional cues for the reasoning steps. Besides, meaning- ful subjects like “Mark” and “Jennifer” can play a crucial role in reasoning, as their relationships should be consid- ered during the reasoning process. Furthermore, it could be observed that some tokens of less importance for the reasoning are given low weights, such as “can”, “say”, “fit”, “received”, “got”, etc. These visualizations demonstrate our rationale token weighting module can effectively determine the significance of rationale tokens, thereby facilitating the student to accurately mimic crucial keypoint tokens. If Tony got twice of what Ken received , then Tony received 2 *$ 1 7 5 0 = $ 3 5 0 0 The total amount shared is $ 3 5 0 0 +$ 1 7 5 0 = $ 5 2 5 0 . If two students can fit on each of a hotel ’ s two queen size b eds , then the total number of students that can fit in one room is 2 + 2 = 4 students . Add ing one student sleep ing on the pull - out c ouch , we can say that one room can fit a maximum of 4 + 1 = 5 students . D ivid ing the number of students in the class by the number of students that can fit in one room , we get 3 0 / 5 = 6 . If Mark purchased 5 0 can s of milk , the total number of can s of milk that Jenn ifer purchased before meeting Mark is 4 0 . Then , Jenn ifer bought 6 can s for every 5 can s Mark bought , so she bought 6 / 5 * 5 0 = 6 0 can s of milk . Figure 2. Visualizations of token significance weights produced by the weight generator. The intensity of red corresponds to the significance weight assigned to each token, with a deeper red indicating higher weight. (a) performance with varying α  (b) performance with varying β Figure 3.Parameter sensitivity study of α and β. 4.6. Parameter Sensitivity Analysis We perform experiments to analyze the effect of two impor- tant hyper-parameters α and β in our method on GSM8K with LLaMA-7B as the student model. Figure 3 shows the results. First, we analyze the effect of hyper-parameter α in the mask ratio loss. We can observe that the performance of our method is not sensitive to α in a relatively large range. Second, we study the influence of hyper-parameter β in the diversity term for question set selection. Similarly, our method is not sensitive to β in a relatively large range. Thus it’s easy to set them in practice. We analyze the sensitivity of other hyper-parameters in Appendix C. 5. Conclusion In this paper, we proposed a keypoint-based progressive chain-of-thought distillation framework for LLMs. Specif- ically, we devised a rationale token weighting module to encourage the student model to accurately mimic keypoint tokens during the distillation process. Besides, we proposed an in-rationale progressive distillation strategy to enable the student model to acquire reasoning capabilities from the teacher LLMs in an easy-to-hard manner. Extensive experi- ments validated the effectiveness of our proposed method. 8Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Acknowledgment This work was supported by the NSFC under Grants 62122013, U2001211. This work was also supported by the Innovative Development Joint Fund Key Projects of Shandong NSF under Grants ZR2022LZH007. Impact Statement This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Bengio, Y ., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In International Conference on Machine Learning, pp. 41–48, 2009. Benoit, L., Lehalle, H., Molina, M., Tijus, C., and Jouen, F. Young children’s mapping between arrays, number words, and digits. Cognition, 129(1):95–101, 2013. Bradley, P. S., Bennett, K. P., and Demiriz, A. Constrained k-means clustering. Microsoft Research, Redmond, 20, 2000. Bridle, J. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Annual Conference on Neural Information Processing Systems, 2, 1989. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Annual Conference on Neural Information Processing Systems, 33:1877–1901, 2020. Chen, H., Wu, S., Quan, X., Wang, R., Yan, M., and Zhang, J. MCC-KD: Multi-CoT consistent knowledge distilla- tion. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 6805–6820. Association for Computational Linguistics, December 2023. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Diao, S., Wang, P., Lin, Y ., and Zhang, T. Active prompting with chain-of-thought for large language models. arXiv preprint arXiv:2302.12246, 2023. Elman, J. L. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. Feng, K., Li, C., Yuan, Y ., and Wang, G. Freekd: Free- direction knowledge distillation for graph neural net- works. In Proceedings of the 28th ACM SIGKDD Con- ference on Knowledge Discovery and Data Mining, pp. 357–366, 2022. Feng, K., Li, C., Ren, D., Yuan, Y ., and Wang, G. On the road to portability: Compressing end-to-end mo- tion planner for autonomous driving. arXiv preprint arXiv:2403.01238, 2024. Fu, Y ., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Spe- cializing smaller language models towards multi-step rea- soning. International Conference on Machine Learning, 2023. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Ho, N., Schmid, L., and Yun, S.-Y . Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14852–14882. Association for Computational Linguistics, July 2023. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Hsieh, C.-Y ., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y ., Ratner, A., Krishna, R., Lee, C.-Y ., and Pfister, T. Distill- ing step-by-step! outperforming larger language models with less training data and smaller model sizes. In Find- ings of the Association for Computational Linguistics: ACL 2023, pp. 8003–8017. Association for Computa- tional Linguistics, July 2023. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 9Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Jia, Q., Liu, Y ., Tang, H., and Zhu, K. In-sample curriculum learning by sequence completion for natural language generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pp. 11937–11950. Association for Computational Linguistics, July 2023. Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Haupt- mann, A. Self-paced learning with diversity. Annual Conference on Neural Information Processing Systems, 27, 2014. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa, Y . Large language models are zero-shot reasoners. Annual Conference on Neural Information Processing Systems, 35:22199–22213, 2022. Kong, Y ., Liu, L., Wang, J., and Tao, D. Adaptive curriculum learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5067–5076, 2021. Krueger, K. A. and Dayan, P. Flexible shaping: How learn- ing in small steps helps. Cognition, 110(3):380–394, 2009. Li, L. H., Hessel, J., Yu, Y ., Ren, X., Chang, K.-W., and Choi, Y . Symbolic chain-of-thought distillation: Small models can also “think” step-by-step. In Proceedings of the 61st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pp. 2665– 2679. Association for Computational Linguistics, July 2023. Li, W., Feldman, M., Kazemi, E., and Karbasi, A. Submodu- lar maximization in clean linear time. Annual Conference on Neural Information Processing Systems, 35:17473– 17487, 2022. Liang, C., Jiang, H., Liu, X., He, P., Chen, W., Gao, J., and Zhao, T. Token-wise curriculum learning for neu- ral machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3658– 3670, 2021. Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Pro- gram induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017. Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 2: Short Papers), pp. 1773–1781. Association for Computational Linguistics, July 2023. Miao, S.-Y ., Liang, C.-C., and Su, K.-Y . A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021. Molina, M. and Jouen, F. Modulation of the palmar grasp behavior in neonates according to texture property. Infant Behavior and Development, 21(4):659–666, 1998. Narayan, S. The generalized sigmoid activation function: Competitive supervised learning. Information Sciences, 99(1-2):69–82, 1997. Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing, pp. 1532–1543, 2014. Peterson, G. B. A day of great illumination: Bf skinner’s dis- covery of shaping. Journal of the experimental analysis of behavior, 82(3):317–328, 2004. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Com- monsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wan, Y ., Yang, B., Wong, D. F., Zhou, Y ., Chao, L. S., Zhang, H., and Chen, B. Self-paced learning for neural machine translation. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Pro- cessing (EMNLP), pp. 1074–1080. Association for Com- putational Linguistics, November 2020. Wang, H., Wang, R., Mi, F., Deng, Y ., Wang, Z., Liang, B., Xu, R., and Wong, K.-F. Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms. In Findings of the Association for Compu- tational Linguistics: EMNLP 2023, pp. 12047–12064, 2023a. Wang, P., Wang, Z., Li, Z., Gao, Y ., Yin, B., and Ren, X. SCOTT: Self-consistent chain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics (Volume 1: Long Papers), pp. 5546–5558. Association for Computational Linguistics, July 2023b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency im- proves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 10Keypoint-based Progressive Chain-of-Thought Distillation for LLMs Wang, Y ., Wang, W., Liang, Y ., Cai, Y ., and Hooi, B. Cur- graph: Curriculum learning for graph classification. In Proceedings of the Web Conference 2021, pp. 1238–1248, 2021. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Annual Conference on Neural Information Processing Systems, 35:24824–24837, 2022. Yang, C., Wang, X., Lu, Y ., Liu, H., Le, Q. V ., Zhou, D., and Chen, X. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y ., Zhou, Z., Gong, C., Shen, Y ., et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420, 2023. Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in lan- guage models. arXiv preprint arXiv:2302.00923, 2023. 11Keypoint-based Progressive Chain-of-Thought Distillation for LLMs A. Implementation Details We perform our experiments using GeForce RTX 3090 GPUs. In order to accelerate training, we employ LoRA (Hu et al., 2021) to train the student model. Following previous CoT distillation works (Chen et al., 2023), the rank of LoRA is set to 64 for LLaMA-7B and 128 for FlanT5-XL. We use Adam optimizer for optimization with a learning rate of 1 × 10−5 for LLaMA-7B and 5 × 10−5 for FlanT5 models. The batch size is set to 4. In terms of LLaMA-7B, the epoch number for training the student model is set to 20 for the GSM8K, CommonsenseQA datasets, and 40 for the ASDiv, SV AMP datasets. As for FlanT5 models, the epoch number is set to 100 because they require more optimization steps for convergence. The input embedding layer in this module aligns with the pretrained student model’s input embedding layer for the consistency of tokenizer. The hyper-parameters α that balances the answer prediction loss and mask ratio loss is set to 0.5. As for the progressive distillation strategy, we simply treat each epoch as a training state in this paper. The stage T that achieves the maximum difficulty is set as half of the epoch number. The initial overall learning difficulty C0 is set to 30% of the maximum difficulty B. We set exponential p = 0.5 in D(t) that controls the growth rate of the learning difficulty. The hyper-parameter β of the diversity term in the question set selection is set to 12. The number of clusters is set as 5 for clustering the question. We follow previous CoT distillation works to split the datasets (Chen et al., 2023; Fu et al., 2023), the datasets statistics are summarized in Table 4. Table 4.Dataset statistics. Datasets Train Size Validation Size Test Size GSM8K 7473 660 659 ASDiv 1462 313 314 SV AMP 700 150 150 CommonsenseQA 8520 1221 1221 B. Training Pseudo-code Algorithm 1 outlines the training procedure of our KPOD. Initially, we employ the CoT prompt (Kojima et al., 2022) to instruct the teacher LLM to generate step-by-step rationales for each question in the dataset. Subsequently, the rationale token weighting module receives these rationales as input and is trained to determine the significance weights for each token. Following this, we compute the difficulty of each step in the rationale based on these weights. We then utilize the FTGP algorithm (Li et al., 2022) to maximize Eq.(13) to schedule the question set for increasing difficulty at each stage. Once scheduled, we train the student model using Eq.(15) based on the established learning order and token significance weights. Before epoch T, we progressively escalate the learning difficulty to the maximum difficulty. Post-epochT, the student is trained to generate the complete rationale for each question. This approach allows the student model to precisely mimic the keypoint tokens while progressively acquiring reasoning capabilities in an easy-to-hard fashion. Algorithm 1 The training procedure of KPOD Input: a teacher LLM, dataset D = {(x(i), y(i))}, epoch number Ne for training student, epoch number T for achieving the maximum difficulty, hyper-parameter settings; Output: a trained smaller student model θs; prompt the teacher LLM to generate rationale for each question x(i) in D; optimize the rationale token weighting module by Eq.(7) to obtain the token significance weight w(i) j ; calculate the step difficulty based by Eq.(8) based on w(i) j ; run FTGP algorithm (Li et al., 2022) to solve Eq.(13) to derive S(t) for each stage. for each epoch e from 1 to Ne do Let ci(t) = 0 for every sample; if e ≤ T then obtain ci(t) by Eq.(11) based on S(t); end if train the student model θs to generate the rationale by Eq.(15) based on w(i) j and ci(t); end for 12Keypoint-based Progressive Chain-of-Thought Distillation for LLMs (a) performance with varying p  (b) performance with varying K  (c) performance with varying C0 Figure 4.Parameter sensitivity study of p, K and C0 on GSM8K. C. Additional Experiments We additionally analyze the sensitivity of three hyper-parameters of p, K and C0. Figure 4 (a)(b)(c) show the performance of LLaMA-7B on GSM8K with varying p, K, C0 respectively. First, we analyze the effect of p that controls the growth rate the learning difficulty. We can find that the performance of our method is relatively stable to this hyper-parameter. Besides, we study the influence of the number of clusters K for clustering the questions. It can be observed that our method is not sensitive to K in a relatively large range. In addition, we investigate the sensitivity of C0 which is the initial learning difficulty. In Figure 4(c), r% denotes setting C0 to r percentage of the maximum difficulty B. Our method is still not sensitive to this hyper-parameter. D. Proof In this section, we prove the Proposition 3.1. First, we introduce Theorem D.1 proposed in FTGP algorithm (Li et al., 2022). Theorem D.1. If a functionf : 2N → R is monotone and submodular. Then, the optimization ofmaxS F(S) that subjects to a knapsack constraint can be approximately solved inO(nϵ−1 log ϵ−1) time complexity by FTGP algorithm (Li et al., 2022) with an approximation ratio guarantee, wheren represents the scale of the data andϵ is a hyper-parameter. IfSopt is the optimal solution andˆS is the approximate solution of FTGP , thenF( ˆS) ≥ (1 2 − ϵ)F(Sopt) holds. According to Theorem D.1, if we could prove that our value function F is monotone and submodular, then Proposition 3.1 is proved. In the next, we will prove that our value function F satisfies these two conditions. Definition 1. (Monotonicity) A function f : 2N → R is monotone if for ∀A ⊆ B ⊆ N where N is the universal set of all elements, it holds that F(A) ≤ F(B). Lemma 1. Our value function F in Eq.(13) is monotone. Proof. We define two question sets A(t), B(t) for increasing difficulty at stage t that satisfy A(t) ⊆ B(t) ⊆ N. Let ∆ = F(B(t)) − F(A(t)). We have: ∆ = −(D(t) − D(t)) + ∆H(B(t)) − ∆H(A(t)) + β KX k=1 p |Ck ∩ B(t)| −β KX k=1 p |Ck ∩ A(t)| ≥ β KX k=1 p |Ck ∩ B(t)| −β KX k=1 p |Ck ∩ A(t)| = β KX k=1 ( p |Ck ∩ B(t)| − p |Ck ∩ A(t)|) ≥ 0 Thus, we have: ∆ = F(B) − F(A) ≥ 0. (16) 13Keypoint-based Progressive Chain-of-Thought Distillation for LLMs ⇒ F(A) ≤ F(B). (17) Definition 2. (Submodularity) A function f : 2N → R is submodular if for ∀A ⊆ B ⊆ N and ∀x ∈ N\\B, it holds that F(A ∪ {x}) − F(A) ≥ F(B ∪ {x}) − F(B). Lemma 2. Our value function F in Eq.(13) is submodular. Proof. We define two triad sets A, Bthat satisfy A ⊆ B ⊆ N. Let T = B\\A. Define ∆ = ( F(A ∪ {x}) − F(A)) − (F(B ∪ {x}) − F(B)). Then we have: ∆ = (∆H(A(t) ∪ {x}) − ∆H(A(t))) − (∆H(B(t) ∪ {x}) − ∆H(B(t))) + (β KX k=1 p |Ck ∩ (A(t) ∪ {x})| −β KX k=1 p |Ck ∩ A(t)|) − (β KX k=1 p |Ck ∩ (B(t) ∪ {x})| −β KX k=1 p |Ck ∩ B(t)|) = ∆H({x}) − ∆H({x}) + (β KX k=1 p |Ck ∩ (A(t) ∪ {x})| −β KX k=1 p |Ck ∩ A(t)|) − (β KX k=1 p |Ck ∩ (B(t) ∪ {x})| −β KX k=1 p |Ck ∩ B(t)|) = (β KX k=1 p |Ck ∩ (A(t) ∪ {x})| −β KX k=1 p |Ck ∩ A(t)|) − (β KX k=1 p |Ck ∩ (B(t) ∪ {x})| −β KX k=1 p |Ck ∩ B(t)|). (18) Given that x ∈ N\\B and A ⊆ B, it follows that x /∈ A and x /∈ B. Then, we have: ∆ = (β KX k=1 p |Ck ∩ A(t)| + |Ck ∩ {x})| −β KX k=1 p |Ck ∩ A(t)|) − (β KX k=1 p |Ck ∩ B(t)| + |Ck ∩ {x})| −β KX k=1 p |Ck ∩ B(t)|). (19) For convenience, we denote xk = |Ck ∩ A(t)|, yk = |Ck ∩ B(t)|, zk = |Ck ∩ {x})|. Then, we have: ∆ = β KX k=1 ((√xk + zk − √xk) − (√yk + zk − √yk)) = β KX k=1 ((√xk + zk − √xk) − (√yk + zk − √yk)(√xk + zk + √xk) + (√yk + zk + √yk) (√xk + zk + √xk) + (√yk + zk + √yk) ) = β KX k=1 (xk + zk − xk − (yk + zk) + yk + 2√xk + zk √yk − 2√xk √yk + zk (√xk + zk + √xk) + (√yk + zk + √yk) ) = β KX k=1 ( 2√xk + zk √yk − 2√xk √yk + zk (√xk + zk + √xk) + (√yk + zk + √yk)) = β KX k=1 ( 2√xkyk + ykzk − 2√xkyk + xkzk (√xk + zk + √xk) + (√yk + zk + √yk)) Since A ⊆ B, it’s evident that yk = |Ck ∩ B(t)| ≥ |Ck ∩ A(t)| = xk. Therefore, we conclude: ∆ = β KX k=1 ( 2√xkyk + ykzk − 2√xkyk + xkzk (√xk + zk + √xk) + (√yk + zk + √yk)) 14Keypoint-based Progressive Chain-of-Thought Distillation for LLMs ≥ β KX k=1 ( 2√xkyk + xkzk − 2√xkyk + xkzk (√xk + zk + √xk) + (√yk + zk + √yk)) = 0 Then, we can derive: ∆ = (F(A ∪ {x}) − F(A)) − (F(B ∪ {x}) − F(B)) ≥ 0. (20) ⇒ F(A ∪ {x}) − F(A) ≥ F(B ∪ {x}) − F(B). (21) 15",
      "references": [
        "Curriculum learning",
        "Young children’s mapping between arrays, number words, and digits",
        "Constrained k-means clustering",
        "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters",
        "Language models are few-shot learners",
        "MCC-KD: Multi-CoT consistent knowledge distillation",
        "Palm: Scaling language modeling with pathways",
        "Scaling instruction-finetuned language models",
        "Training verifiers to solve math word problems",
        "Active prompting with chain-of-thought for large language models",
        "Learning and development in neural networks: The importance of starting small",
        "Freekd: Free-direction knowledge distillation for graph neural networks",
        "On the road to portability: Compressing end-to-end motion planner for autonomous driving",
        "Specializing smaller language models towards multi-step reasoning",
        "Distilling the knowledge in a neural network",
        "Large language models are reasoning teachers",
        "Training compute-optimal large language models",
        "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
        "Lora: Low-rank adaptation of large language models",
        "Categorical reparameterization with gumbel-softmax",
        "In-sample curriculum learning by sequence completion for natural language generation",
        "Self-paced learning with diversity",
        "Large language models are zero-shot reasoners",
        "Adaptive curriculum learning",
        "Flexible shaping: How learning in small steps helps",
        "Symbolic chain-of-thought distillation: Small models can also “think” step-by-step",
        "Submodular maximization in clean linear time",
        "Token-wise curriculum learning for neural machine translation",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
        "Teaching small language models to reason",
        "A diverse corpus for evaluating and developing english math word problem solvers",
        "Modulation of the palmar grasp behavior in neonates according to texture property",
        "The generalized sigmoid activation function: Competitive supervised learning",
        "Are nlp models really able to solve simple math word problems?",
        "Glove: Global vectors for word representation",
        "A day of great illumination: Bf skinner’s discovery of shaping",
        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
        "Llama: Open and efficient foundation language models",
        "Self-paced learning for neural machine translation",
        "Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms",
        "SCOTT: Self-consistent chain-of-thought distillation",
        "Self-consistency improves chain of thought reasoning in language models",
        "Cur-graph: Curriculum learning for graph classification",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "Large language models as optimizers",
        "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
        "Automatic chain of thought prompting in large language models",
        "Multimodal chain-of-thought reasoning in language models"
      ],
      "meta_data": {
        "arxiv_id": "2405.16064v1",
        "authors": [
          "Kaituo Feng",
          "Changsheng Li",
          "Xiaolu Zhang",
          "Jun Zhou",
          "Ye Yuan",
          "Guoren Wang"
        ],
        "published_date": "2024-05-25T05:27:38Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Keypoint-based Progressive Chain-of-Thought Distillation (KPOD), a unified framework for distilling reasoning abilities from large language models (LLMs) to smaller student models. KPOD addresses two main issues in CoT distillation: (i) unequal treatment of tokens within a rationale, which can lead to reasoning errors by failing to accurately mimic keypoint tokens, and (ii) the lack of a structured learning order for step generation, diverging from the human cognitive progression of learning from easy to hard tasks. KPOD proposes a rationale token weighting module to identify and emphasize keypoint tokens during distillation, and an in-rationale progressive distillation strategy to schedule the learning order of reasoning steps from easy to hard.",
        "methodology": "KPOD employs a rationale token weighting module based on mask learning. This module generates significance weights for each token by learning to mask reasoning-irrelevant tokens. It utilizes an answer prediction loss to encourage the use of masked rationales for answer derivation and a mask ratio loss to maximize the ratio of masked tokens, thus identifying keypoint tokens. For progressive distillation, KPOD assesses step difficulty using a weighted token generation loss, incorporating the derived token significance to mitigate the impact of irrelevant tokens. An in-rationale progressive distillation strategy trains the student model to generate final reasoning steps first, gradually expanding to the entire rationale. A value function, designed to align increased difficulty with a defined magnitude and ensure diverse question selection, dynamically determines the number of input steps at each stage. This value function is maximized using the FTGP algorithm, a submodular maximization algorithm.",
        "experimental_setup": "The effectiveness of KPOD was evaluated on four reasoning benchmarks: three mathematical reasoning tasks (GSM8K, ASDiv, SVAMP) and one commonsense reasoning task (CommonsenseQA). GPT-3.5-Turbo was used as the teacher model to generate rationales via zero-shot CoT prompting. Three widely-used pre-trained language models served as student models: LLaMA-7B, FlanT5-XL, and FlanT5-Large. The performance was measured using accuracy (%). Out-of-distribution (OOD) evaluation was also conducted by training on GSM8K and testing on ASDiv and SVAMP. LoRA was used for training acceleration, Adam optimizer with specified learning rates and batch sizes were employed. Hyperparameters such as the balancing parameter \\alpha for mask ratio loss, the growth rate parameter p, initial overall learning difficulty C0, and the diversity term parameter \\beta were analyzed for sensitivity.",
        "limitations": "Previous CoT distillation methods often treat all tokens within a rationale equally, failing to accurately mimic crucial keypoint tokens and leading to reasoning errors. Additionally, these methods do not distinguish the learning order of step generation, which diverges from the human cognitive pattern of progressing from easier to more challenging tasks, potentially resulting in sub-optimal outcomes. Directly applying existing curriculum learning strategies to CoT distillation is inferior due to their oversight of the step-by-step reasoning nature and the influence of inessential tokens on difficulty assessment.",
        "future_research_directions": "The paper does not explicitly state future research directions. However, potential extensions could involve exploring more sophisticated methods for token significance estimation, investigating alternative progressive distillation strategies, applying KPOD to a wider range of reasoning tasks and larger student models, and further optimizing the training efficiency and scalability of the framework.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs",
      "full_text": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models Zhiyuan Hu1∗ Chumin Liu2 Xidong Feng3 Yilun Zhao4 See-Kiong Ng1 Anh Tuan Luu2 Junxian He5 Pang Wei Koh6 Bryan Hooi1 1 National University of Singapore 2 Nanyang Technological University 3 University College London 4 Yale University 5 The Hong Kong University of Science and Technology 6 University of Washington Abstract In the face of uncertainty, the ability to seek information is of fundamental im- portance. In many practical applications, such as medical diagnosis and trou- bleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approachwhich enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty- based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the ‘20 Questions’ game, UoT achieves an average performance improvement of 38.1% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task). Our code are released2. 1 Introduction As the capabilities of large language models (LLMs) grow, they are being increasingly deployed in challenging real-world settings involving uncer- tainty and ambiguity. In particular, recent work aims to develop LLM agents or assistants Xi et al. (2023); Park et al. (2023) that effectively com- plete tasks in interactive environments, leading to a growing need for LLMs that can actively seek the information they need to solve a task by asking questions in conversational settings. For example, in medical diagnosis, patients often do not initially report their symptoms in full detail. In such situa- tions, a doctor’s ability to ask effective questions is crucial, as a successful diagnosis often depends on revealing important details that the patient did not initially provide (Figure 1). Haveyouobservedanyvisionchanges,orsensitivitytolight? I'vebeenhavingabadheadachesinceyesterday Oh,nowyoumentionit,Ihavebeenmoresensitivetolightrecently. Ah,Ididhitmyheadafewdaysago,butitdidn’tseemserious. Patient DoctorHave you injured your head recently? Youmayhavepost-concussionsyndrome. Figure 1: The importance of information seek- ing in medical diagnosis. The patient initially only complains of a headache, but by asking the right questions, the doctor uncovers the crit- ical information needed for a correct diagnosis. ∗Corresponding to: Zhiyuan Hu, zhiyuan_hu@u.nus.edu 2https://github.com/zhiyuanhubj/UoT Preprint. Under review. arXiv:2402.03271v3  [cs.CL]  13 Nov 2024Recent techniques aim to improve LLMs’ reasoning or planning abilities based on the given informa- tion rather than enabling LLMs to seek information efficiently. For example, Chain-of-Thought (CoT) (Wei et al., 2022) and Tree-of-Thoughts (ToT) (Yao et al., 2023) allow LLMs to express intermediate ‘thoughts’ and reason over them. Unlike these methods, our focus is on enabling the LLM to ask questions effectively by explicitly guiding the model toward reducing uncertainty, which these do not consider. Thus, they lack effective signals for questions that better reduce uncertainty by revealing critical information. To enhance LLMs in actively seeking information, we introduce Uncertainty of Thoughts (UoT), a plug-and-play approach that improves LLMs’ abilities to ask useful questions by modeling their own uncertainty. UoT is a principled approach relying on uncertainty-based rewards motivated by information gain, which incentivizes a model to seek information in a way that maximally reduces the amount of information it does not know. To utilize these rewards, we develop anuncertainty-aware simulation framework, enabling the model to simulate possible future scenarios along with how likely they are to occur. Given these scenarios, we utilize a reward propagation schemeto select the optimal question to ask in a way that maximizes the expected reward. Additionally, most standard benchmarks for LLMs, particularly in question answering, assume that all necessary information to solve a task is provided at the outset, and thus do not evaluate the model’s active information-seeking capabilities. To close this gap, we first introduce a benchmark comprising 5 datasets3 on 3 tasks: 20 Questions, a simplified medical diagnosis task, and a basic troubleshooting task. These tasks are designed to measure the model’s ability to ask questions effectively to gather the information they need. For example, the 20 Questions game, also studied by Noever et al.Noever & McKee (2023), requires the model to ask ‘yes’ or ‘no’ questions to determine an unknown object or entity. This scenario serves as a clear and easily analyzed test case, isolating the model’s ability to recognize its own uncertainty, and to ask questions that guide it to the correct answer. Our work is a step toward LLMs that can effectively operate in settings with high uncertainty and ambiguity, beyond conventional QA settings where all the information needed to solve the task is provided to the model at the outset. To the best of our knowledge, UoT is the first approach for enabling LLMs to ask effective questions by explicitly modeling and seeking to reduce their uncertainty. Our key contributions are as follows: 1. We introduce Uncertainty of Thoughts (UoT), a plug-and-play approach enabling LLMs to explicitly model and seek to reduce their uncertainty. UoT utilizes a principled approach based on an uncertainty-aware framework for simulating possible futures, rewards motivated by information gain, and a reward propagation scheme to select the optimal question to ask. 2. We introduce a benchmark of 3 tasks and 5 datasets, designed to evaluate the ability of LLMs to seek the information they need by asking questions. 3. Experiments show that UoT improves the success rate of multiple LLMs by 38.1% on average compared with direct prompting, achieving top performance on both task success and efficiency. Our benchmark and code are publicly available. 2 Methodology 2.1 Problem Formulation The problem setting involves two roles: the Questioner and the Answerer, performed by the LLM and a human, respectively. The goal of the Questioner is to deduce an unknown piece of infor- mation. We formulate this using a possibility space Ω, which is the set of all possible options, of which a single element ω ∈ Ω, is the true option in each given scenario 4. For example, in a medical diagnosis setting, Ω is the set of all possible diseases relevant in the context, e.g., Ω = {Bronchitis, Flu, . . . ,Hypertension}, and for each patient, ω is the actual disease of the patient. 3We also incorporate the efforts of prior datasets Srivastava et al. (2022); Xu et al. (2019); Liu et al. (2022); Raghu et al. (2021), through further work and refinement to construct this benchmark. Details are introduced in section 3 Experiments and Appendix I.2. 4Under the measure-theoretic formulation of probability, the sample point ω is an element of the sample space Ω, and all random variables are defined to be functions ofω. While we conform to this formulation, we try to avoid unnecessary measure-theoretic background for ease of understanding; hence, it is sufficient for readers to understand ω as the ‘true option’ in each scenario. 2What disease does the patient have? Do you have fever? I already vomited twiceNo, I didn't  Yes, severeI'm fever-free Gastritis, food poisoning… Enteritis, Flu, Anemia … Did you vomit? Flu, Pneumonia, Rubella… Norovirus, Hypoglycemia, Anemia …  ………… Do you have stomach pain?…… …… (a) Question Generation and Simulation  Simulate Possible Futures (b) Uncertainty-based Reward Gastritis, food poisoning, Enteritis, Flu… Gastritis, food poisoning…  Enteritis, Flu…  Did you vomit? Information GainUncertainty Based Reward       : 0.8 (c) Reward Propagation Scheme Expected RewardAccumulated RewardPossible Question Generation LLM as questioner LLM simulate responses Gastritis, Flu, Rubella, Anemia… Questioner(LLM)Estimate remaining candidates Potential Disease Candidates I already vomited twice No, I do not have = 0.6 = 0.8= 1.2= 1.1 =+=1.4 = 0.7 = 0.6 =1.5Conditional Probability of Answer:  Figure 2: UoT Overview: UoT includes three components: (a) Question Generation and Simulation, where an LLM proposes candidate questions and simulates future scenarios; (b) Uncertainty-based Rewards, measuring the uncertainty reduction from answers to a question, and (c) Reward Propagation computing accumulated rewards Ra over past questions, and expected rewardsRe capturing expected future gains. The process ends by choosing questions with the highest expected reward. The interaction between the Questioner and the Answerer occurs over multiple turns. For instance, the Questioner may ask, “Do you have a fever?\", to which the Answerer responds, “Yes, I’ve had a high fever for the past two days.\" The Questioner then asks another question such as “Have you vomited?\" This exchange continues either until the Questioner correctly determines the final answer, or the conversation reaches a maximum number of turns. At this point, the interaction ends, and the Questioner is successful if it has correctly determined the true option ω. Most of the description of our approach focuses on the closed set scenario, in which we assume that the Questioner starts with knowledge of the possibility space Ω, e.g., the set of all possible diseases in medical diagnosis. In our extension section 2.7, we adapt our approach to the open set scenario, in which this knowledge is absent. Moreover, as the questioning progresses, we use an LLM to gradually refine this set of possibilities to those that are consistent with the current answers given so far by the Answerer. Define the current possibility set Ωi as the subset of Ω that is consistent with all answers given by the Answerer before the start of the ith interaction step. As we discuss more later, we focus on applications where answers can be grouped into a small number of semantically distinct categories (in our case, affirmative and negative responses), as this allows us to compute meaningful uncertainty metrics in a simpler way. Conceptually, our framework can straightforwardly be extended to allow for a wider selection of answers. 2.2 Uncertainty of Thoughts: Overview As Figure 2 shows, to effectively reduce uncertainty, our UoT method first generates multiple questions as candidates to ask, and simulates possible futures for each one in the form of a tree structure. Next, uncertainty-based rewards, motivated by information gain, are used to assess the questions within the simulation. Finally, a reward propagation scheme is used to compute the expected reward from asking each candidate question, allowing us to select the one with highest expected reward, to ask the Answerer. 2.3 Question Generation and Simulation UoT starts by using an LLM to generate several candidate questions, then simulates future scenarios for each one. This simulation process allows us to measure how much information we can expect to gain in the next few steps from each question, and thus to choose the most suitable question. Question Generation Recall that our setting involves sequential interactions between a Questioner (e.g., a chatbot) and an Answerer (e.g., a human patient). During the ith interaction step, the Questioner generates candidate questions, then selects one of these to ask, denoted as qi. To generate candidate questions to ask, UoT uses two inputs: (1) the history of past interactions hi = {q1, a1, q2, a2, . . . , qi−1, ai−1}, comprising the sequence of past questions and answers; and (2) the current possibility set Ωi. These two inputs are combined to form a prompt that includes instructions explaining the nature of the task (e.g., how the 20 Questions game works), provides the current history hi and the current possibility set Ωi, and asks an LLM to generate m candidate next questions, conditioned on the previous context. This prompt, denoted as Promptgen(hi, Ωi), is fed to 3our generator LLMgen, which then generates m candidate questions, denoted q1 i , q2 i , . . . , qm i : q1 i , q2 i , . . . , qm i = LLMgen(Promptgen(hi, Ωi)) (1) Multistep Simulation As shown in Figure 2 (a), the Question Generation stage generates candidate questions such as q1 i = “Did you vomit?\" Next, during Simulation stage, for each such generated candidate question, we simulate possible futures for a few steps, forming a tree of possibilities. This process enables us to compute rewards for each question, helping us to decide which question to ask. Each node of the tree can be one of two types: Answerer Nodes where it is the Answerer’s turn to answer a question, and Questioner Nodes where it is the Questioner’s turn to ask a question. At the root, a question has just been asked (e.g., q1 i ), so the root is an Answerer Node. Next, we explain how to construct tree by recursively expanding (or ‘branching’) each node to construct its children, i.e., starting from the root, then proceeding to its children, and so on. • At each Answerer Node, a question has just been asked. Next, we need to further ‘branch’ the tree based on the possible answers to the current question. Rather than allowing completely open-ended answers, we instead focus on affirmative and negative responses5, as this allows us to compute meaningful uncertainty metrics, as we discuss later. Hence, we branch the node into two children, corresponding to affirmative and negative answers. • At each Questioner Node, we prompt an LLM to generate m questions using the current history and possibility set, in the same way as in the Question Generation step. Note that while the generation procedure is similar, the purpose is different: the Question Generation step generates candidate questions to select from, while here we are generating simulated questions to form a tree for the purpose of evaluating the current question. The resulting m generated questions are added to the tree as children of the current node. In this way, we recursively generate tree nodes, stopping at a fixed number of levels (i.e., depth). While generating this tree, we also recursively compute the current possibility set Ωv at each node v. Specifically, let hv be the current conversation history up to node v, combining both the actual conversation history hi and the simulated conversation up to node v. Then the current possibility set at this node, denoted Ωv, is the subset of the possibility space consistent with hv. At the root, the current possibility set is only limited by the actual conversation history, i.e., Ωi. Then, as we proceed over the simulated tree, note that the current possibility set only changes at Answerer nodes, when an answer is added to the current history. Hence, at each Answerer node v, we prompt a new LLM (an ‘Answerer Simulator’ LLMans), to determine the further subset ΩA v ⊆ Ωv for which the answer to the current question is affirmative, and the corresponding ΩN v = Ωv \\ ΩA v for which the answer is negative.6 This allows us to recursively compute the possibility sets of the children of v (which themselves correspond to the affirmative and negative answers). ΩA v , ΩN v = LLMans(Promptans(hv, Ωv)) (2) In this way, we can recursively compute the possibility set on each node of the tree. 2.4 Uncertainty-Based Reward Calculation To develop suitable information-seeking approaches, a critical question is how to evaluate the effectiveness of a question, i.e., its contribution to reducing uncertainty . To address this, we turn to information theory, specifically the concept of information gain, which measures the amount by which uncertainty decreases after a particular observation. To reward information-seeking behavior, we assign rewards to questions based on how much they reduce the model’s uncertainty about the unknown random variable. These reward signals are used by our UoT framework to determine which question to select, to maximize the reduction of uncertainty. Entropy. Entropy and information gain are well-known concepts in information theory Shannon (1948). In our work, we use these concepts to measure how much information is gained (or equiva- lently, how much uncertainty is reduced) by asking a question, to formulate our rewards. Entropy 5As shown Figure 2 (a), for question ‘Did you vomit?’, possible affirmative responses include ‘yes’ or ‘I already vomited twice’, while negative responses could be ‘no’ or ‘I don’t have’. 6In practice, allowing overlap between ΩA v and ΩN v may be more realistic. However, in this work, we consider only the simplified scenario where they are disjoint. 4measures the level of uncertainty in a random variable: higher entropy indicates greater uncertainty. The entropy of a discrete random variable X taking values x1, ..., xn is: H(X) = − Xn i=1 p(xi) logp(xi) (3) Since our goal is to reduce the uncertainty in the unknown ω ∈ Ω, we use entropy to measure this uncertainty. Formally, let Ω = {ω1, ··· , ωn}, and we define an additional set of arbitrary real numbers X = {x1, ··· , xn} ⊆R which we will associate with each of these possibilities. Define a random variable X : Ω → Xsuch that X(ωi) = xi. Intuitively, X is a discrete random variable that takes the value xi if the ith possibility is true, i.e., if ω = ωi. X serves to capture our uncertainty about ω, since observing X is equivalent to observing the true option ω. As a simple example, suppose our possibility space is Ω = {ω1, ω2, ω3}; we accompany these with real numbersx1, x2, x3, and have a distribution for our random variable X reflecting prior beliefs over these possibilities: e.g., p(x1) = 0 .2, p(x2) = 0 .3, p(x3) = 0 .5. Conceptually, our framework allows for any prior probability distribution over the possibilities (i.e., p(xi)), but in our experiments, we assume a uniform distribution over them due to the lack of an informative prior. Before asking any questions, our uncertainty about the unknown ω is given by H(X), as in Eq. (3). At any node v of the trees described in the previous section, recall that we have a conversation history hv which contains some answers given by the Answerer. This history limits the current possibility set to those in Ωv ⊆ Ω, thereby reducing our uncertainty. We model this using the standard notion of conditional probability on an event: since Ωv ⊆ Ω, thus Ωv is an event which we can condition on: p(xi|Ωv) = p(xi)/p(Ωv) ∀ i such that ωi ∈ Ωv (4) where p(Ωv) is the sum of probabilities of the elements in Ωv. To illustrate, we continue from the earlier example, where p(x1) = 0.2, p(x2) = 0.3, p(x3) = 0.5. If the conversation history hv at node v is only consistent with x1 and x2, i.e., Ωv = {ω1, ω2}, we can adjust probability distribution by conditioning: e.g., the adjusted probability of x1 is p(x1)/p(Ωv) = 0.2/(0.2 + 0.3) = 0.4. Next, to quantify the uncertainty at node v, note that since X is conditionally distributed based on p(·|Ωv), the entropy of this distribution is: Hv(X) := X i:ωi∈Ωv p(xi|Ωv) logp(xi|Ωv) (5) Intuitively, Hv(X) is the remaining uncertainty in X at node v (i.e., after observing the history hv). Information Gain at a Node We now quantify the uncertainty reduction when receiving answers at an Answerer node v. Recall that the answer given at v partitions Ωv into two disjoint subsets: Ωv = Ω A v ∪ ΩN v , where ΩA v and ΩN v are the subsets of possibilities resulting in affirmative and negative answers to last asked question. Given an affirmative answer, the remaining entropy becomes: HA v (X) := X i:ωi∈ΩAv p(xi|ΩA v ) logp(xi|ΩA v ) (6) We define HN v (X) analogously for negative answers. Let pA v = p(ΩA v )/p(Ωv) and pN v = p(ΩN v )/p(Ωv) be the conditional probabilities of affirmative and negative answers at node v. To compute the expected entropy after receiving the answer at node v, since we have a pA v probability of receiving an affirmative answer and pN v of a negative answer, the expected entropy is: pA v · HA v (X) + pN v · HN v (X) (7) As such, the expected information gain at node v is the difference in entropies before and after receiving the answer: IGv(X) := Hv(X) − pA v · HA v (X) − pN v · HN v (X) (8) We can simplify this: as proven in Appendix A, the above equation reduces to: IGv(X) = −pA v log pA v − pN v log pN v (9) This represents the expected reduction of uncertainty in X when receiving an answer at node v. Note that it has an entropy-like expression, and is therefore nonnegative. 5Reward Formulation A natural approach would be to define the reward function Ru(v) at node v as the information gain IGv(X): that is, the reward from the question at node v is the expected information gain IGv(X) from receiving its answer. In practice, we find that a slightly modified function fIGv(X) is preferable. In particular, we find that IGv(X) does not result in sufficiently sharp differences in reward over the typical ranges we encounter. Hence, we introduce an additional hyperparameter λ ≥ 0 which helps to sharpen the rewards using a scaling approach. We compare other scaling methods and determine the current design is optimal in performance and their corresponding benefits. Details are in the Appendix B. Ru(v) = fIGv(X) := (−pA v log pA v − pN v log pN v )/(1 + λ−1|pA v − pN v |) (10) This definition ensures that Ru(v) falls within the range [0, 1], providing a normalized and consistent reward to measure uncertainty reduction. The reward function reaches its maximum when the subsets ΩA v and ΩN v have equal probability, reflecting the maximum reduction in uncertainty. It reaches its minimum when one of the subsets has zero probability, indicating no reduction in uncertainty. Appendix G plots the reward function curve across values of pA v and pN v . 2.5 Question Selection Via Reward Propagation Single-step rewards often fall short in dynamic settings as they only consider immediate impact, overlooking long-term effects. To overcome this, our method uses a reward propagation scheme across simulation trees by defining ‘accumulated rewards’ that gather rewards over multiple simulation steps to reflect the effectiveness of past decisions. These accumulated rewards help compute ‘expected rewards’, indicating the likely benefits of the questions and guide the selection of candidate questions. Accumulated Reward We first define the accumulated reward at each node v, which accumulates the rewards at v and all its ancestors on the tree, defined recursively as: Ra(v) := Ru(v) + \u001a 0 v is root Ra(Parent(v)) otherwise Here Ru(v) is the uncertainty-based reward at node v defined in Eq. (10), and Ra(Parent(v)) is the accumulated reward of the parent of v. We compute these accumulated rewards by starting at the root and propagating down to the leaves. Intuitively, the accumulated reward at each leaf node represents the total reward we end up with at the end of the conversation at that node. Expected Reward Next, we compute the expected reward for each node Re(v), which represents the expected total value of rewards received on expectation on a node and all its descendants on tree. Re(v) :=    Ra(v) if v is a leaf; otherwise: pA v Re(vA) + pN v Re(vN ) if v is an Answerer Node 1 m Pm w∈Children(v) Re(w) if v is a Questioner Node For the case where v is an Answerer Node, recall that pA v and pN v are the conditional probabilities of affirmative and negative answers at node v, defined in section 2.4. vA and vN are its children, corresponding to the affirmative and negative answers. For the case wherev is a Questioner Node, we assign equal probability to the m questions asked from this node. In this way, we propagate the expected rewards from the leaves up to the root, allowing us to compute the expected gain at the root. We compare different reward propagation schemes and find that using cumulative rewards from all paths enhances long-term decision-making benefits. See Appendix C for details. Determining the Optimal Question Finally, to decide the question to ask, we select the question with highest expected reward (and therefore, the highest expected information gain, considering both immediate and future information gains): qi = arg max n=1 Re(qn i ) (11) 62.6 UoT Summary UoT first generates candidate questions q1 i , q2 i , . . . , qm i based on the history hi and current possibility set Ωi. Then, we conduct multistep simulation to generate a tree for each candidate question qn i . Next, we compute the uncertainty-based rewards Ru(v), and propagate over the trees to compute accumulated reward Ra(v) and expected reward Re(v). Lastly, the optimal question qn i with highest expected reward will be selected as qi to interact with the Answerer. UoT generates candidate questions q1 i , q2 i , . . . , qm i based on history and the current possibility set Ωi. It simulates a tree for each question, calculates uncertainty-based rewards Ru(v), and computes expected rewards Re(v). The question qn i with the highest expected reward is chosen for interaction. 2.7 Extensions and Discussion Open Set UoT. Recall that in the closed set scenario, the Questioner starts with knowledge of the possibility space Ω. In practice, the possibility space is often unknown, resulting in the open set setting. To adapt UoT to this case, we prompt Questioner to initialize the possibility spaceΩ and then reinitialize the possibility set Ωi according to current history hi. Then, the rest of UoT is unchanged. The generalization in open-end answers. The UoT framework enables LLMs to update possibilities after each interaction, including affirmative/negative or open-ended responses. Thus, it can be applied to open-ended answers scenarios. Pruned UoT. To enhance efficiency during simulation, pruning akin to Beam Search can be employed when constructing the simulation trees, which limits the number of paths to explore over the tree to a predetermined size. 3 Experiments 3.1 Experimental Setup Models We test various LLMs to evaluate the generality of our method, includingLlama-3-70B- Instruct AI@Meta (2024), Mistral-Large Mistral.AI (2024), Gemini-1.5-Pro Reid et al. (2024), Claude-3-Opus Anthropic (2024) and GPT-4 OpenAI (2023b). We also validate the performance of earlier released LLMs (Refer to Appendix D) including Llama 2-70B-Chat Touvron et al. (2023), Cohere Cohere (2023), PaLM 2 Anil et al. (2023), Claude 2 Anthropic (2023) and GPT-3.5-turbo OpenAI (2023a). Baselines Direct Prompting (DP) prompts an LLM directly to generate the next response. Plan- ning Prompting (PP) is motivated by Wang et al.Wang et al. (2023). We leverage another LLM to plan the future and, consequently, determine the question to ask. Chain-of-Thought (CoT) Wei et al. (2022) improves reasoning in LLMs by detailing reasoning steps. CoT-SC (Self-Consistency) Wang et al. (2022) an is an ensemble method, explores multiple reasoning paths. We standardize sampling counts for fair computational cost comparison with other methods. Reflexion Shinn et al. (2023) lets agents propose actions and self-assess to foster new ideas. Tree-of-Thoughts (ToT)Yao et al. (2023) enables LLMs to make decisions by exploring and evaluating multiple reasoning paths over a tree structure. We examine ToT under two setups: Original-ToT, which uses the standard approach of generating and evaluating questions, and Adapted-ToT (Ad.-ToT), where we integrate heuristic experience into prompt for question generation and evaluation, focusing on questions that halve the search space. We matched the tree depth to the simulation steps in our UoT method for a fair comparison. We evaluate methods and LLMs in both open set (OS) and closed set (CS) settings. In open set, models are tested without prior knowledge of outcomes; in closed set, they are given complete information about all possible outcomes. For details, see Appendix I.1 for experimental settings and Appendix L for prompts. Scenarios and Datasets 20 Questions is a game where the answerer thinks of an item and the questioner asks up to 20 yes-or-no questions to guess it. We use two datasets, Common (collected by us, refer to Appendix I.2 for more details) and Things Hebart et al. (2019), including 111 and 1854 items separately. In this scenario, the maximal turns is set to 20. In Medical Diagnosis, the doctor needs to ask questions to patients about their symptoms, to determine an accurate diagnosis. We use two datasets: DX Xu et al. (2019), with 104 doctor-patient dialogues and 5 diseases in test set, and MedDG Liu et al. (2022) with over 17K conversations across 15 disease types. We manually selected 500 high-quality samples for evaluation (see Appendix I.3 for selection process).Importantly, 7Table 1: Results from three different scenarios, assessing Success Rate (SR), Mean Conversation Length in Successful Cases (MSC), and Mean Conversation Length (MCL). Model Method 20 Questions Medical Diagnosis Troubleshooting Common Thing DX MedDG FloDial SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ Llama3-70B DP (OS) 34.2 13.9 17.9 15.5 14.9 19.2 26.0 3.6 4.6 25.7 3.6 4.6 11.1 15.4 19.5UoT(OS) 36.9 12.4 17.3 21.0 13.6 18.7 35.6 2.6 4.1 50.6 2.3 3.6 26.1 9.1 17.2DP (CS) 51.4 14.6 17.2 15.0 13.8 19.1 83.7 3.5 3.7 60.2 3.5 4.1 28.8 15.7 18.8UoT (CS) 55.9 12.6 15.9 25.0 13.0 18.3 90.4 1.0 1.4 64.3 1.4 2.7 47.1 7.6 14.2 Mistral-Large DP(OS) 20.7 13.1 18.6 12.5 13.6 19.2 18.3 3.4 4.7 28.3 3.2 4.5 11.1 15.8 19.5UoT(OS) 27.0 15.1 18.7 15.0 13.1 19.0 24.0 2.5 4.4 50.0 2.9 4.0 19.6 11.3 18.3DP (CS) 26.1 13.4 18.3 13.0 12.6 19.0 38.5 3.3 4.3 46.7 3.3 4.2 14.2 16.0 19.4UoT (CS) 31.5 9.8 16.8 18.5 13.2 18.7 48.1 2.2 3.6 60.0 1.9 3.2 30.1 10.9 17.3 Gemini-1.5-Pro DP (OS) 36.0 16.8 18.8 17.5 14.4 19.0 26.9 3.5 4.6 23.7 4.0 4.8 9.15 15.6 19.6UoT(OS) 39.7 14.6 17.9 22.0 13.4 18.5 39.4 2.4 4.0 38.6 2.9 4.2 19.0 12.1 18.5DP (CS) 47.7 17.0 18.6 28.5 15.0 18.6 69.2 3.2 3.8 51.4 3.2 4.1 30.1 14.0 18.2UoT (CS) 60.4 13.9 16.3 32.0 14.0 18.1 81.7 2.1 2.6 81.4 2.1 2.6 53.6 11.5 15.4 Claude-3-Opus DP(OS) 45.0 14.2 17.4 16.5 13.8 19.0 33.7 3.4 4.5 54.3 3.2 4.0 31.4 15.7 18.6UoT(OS) 63.1 14.4 16.5 23.5 13.3 18.4 45.9 2.6 3.9 61.5 2.3 3.3 35.9 11.0 16.8DP (CS) 52.3 13.8 16.8 33.5 14.1 18.0 75.0 3.3 3.7 73.3 3.3 3.8 48.4 16.0 18.1UoT (CS) 66.7 6.9 11.3 41.5 13.9 17.5 81.7 2.2 2.7 79.3 2.4 2.9 56.2 6.2 12.2 GPT-4 DP(OS) 48.6 14.0 17.1 16.5 12.6 18.8 44.2 3.5 4.9 45.7 4.2 4.6 38.4 13.0 17.3CoT(OS) 13.5 18.6 19.8 6.00 16.4 19.8 18.3 3.8 4.8 9.71 4.0 4.9 30.7 10.3 17.0Ad.-ToT(OS) 45.0 17.8 19.0 21.0 15.2 19.0 45.2 2.4 3.8 51.4 2.7 3.8 35.3 13.3 17.7UoT(OS) 55.3 15.1 17.4 28.0 14.9 18.6 49.1 2.4 3.7 67.4 2.5 3.5 43.5 12.0 16.8DP (CS) 50.5 13.1 16.5 30.5 13.1 17.9 91.3 3.0 3.3 72.3 4.2 4.4 43.7 13.4 17.1PP (CS) 38.7 14.9 18.0 18.0 14.5 19.0 58.6 2.5 3.5 62.3 3.8 4.3 39.2 14.2 17.7CoT (CS) 20.7 16.0 19.2 10.0 16.2 19.6 33.7 3.7 4.4 20.0 3.8 4.3 32.8 10.1 16.8CoT-SC (CS) 55.1 14.0 16.7 18.5 14.8 19.0 48.5 3.6 4.3 26.7 4.2 4.8 42.5 11.0 16.2Reflexion (CS) 67.6 12.0 14.6 31.5 13.6 18.0 52.5 3.7 4.3 30.3 4.0 4.7 28.6 11.5 17.8Original-ToT (CS) 28.8 15.5 18.7 18.5 15.1 19.1 70.3 3.3 3.8 60.3 3.2 3.9 40.4 11.6 16.6Ad.-ToT (CS) 42.6 12.2 16.1 25.0 13.0 18.3 92.1 1.9 2.2 78.0 3.0 3.4 60.3 8.2 12.9Pruned UoT (CS) 62.210.8 14.3 34.0 14.9 18.3 92.1 1.9 2.1 83.3 2.7 3.1 63.2 8.2 12.5UoT (CS) 71.2 10.8 13.5 37.5 14.4 17.9 97.0 2.0 2.1 88.0 2.6 2.9 67.3 7.8 11.8 Open-ended responses from patient are allowed in MedDG to validate UoT’s generalization in open- ended scenarios. Both datasets are limited to 5 turns. Troubleshooting is a scenario where a customer support technician interacts with customers to identify and resolve faults or issues within computer systems, electronic devices, machinery, or other complex systems. Raghu et al.Raghu et al. (2021) introduce FloDial with 894 dialogues, containing 153 faults and we also conduct the data preprocessing of FloDial (See Appendix I.4 for details). We evaluate using a maximum of 20 turns. The answerer, simulated by GPT-4, is prompted with the patient’s actual disease and conversation details for each case. For more details, refer to Appendix I.2 and see examples of these scenarios in Appendix K. UoT (Open Set) Setup We iteratively update LLMs’ perceived possibilities based on conversational history, rather than defining them all upfront. In medical diagnosis and troubleshooting, initial descriptions from symptoms or issues help set up initial possibilities. In the 20-question game, we start with broad inquiries using the Direct Prompting method for the first three rounds to gather more information. The ToT tree structure method employs a similar strategy. Setup details in Appendix I.5. Evaluation Metrics To measure efficacy and efficiency, we use:Success Rate (%): SR = S/T, where S is the number of successful cases, and T is the total number of cases; Mean Conversation Length in Successful Cases: MSC = Rs/S, where Rs is the total rounds in successful cases; Mean Conversation Length: MCL = R/T, where R is the total rounds in all cases. MCL measures efficiency based on the resources used in both successes and failures. 3.2 Performance 20 Questions As illustrated in Table 5, for all types of LLMs, those equipped with UoT outperform the baselines in both open set and close settings. Among the methods used on GPT-4 to enhance planning and reasoning, CoT (CS) and PP (CS) show inferior performance even compared to GPT-4 alone. UoT (OS) demonstrates superior performance, with with an average 8.7% improvement than Adapted-ToT (OS) in success rate. Moreover, UoT (CS) achieves the highest success rate, surpassing the second-best Reflexion by an average of 4.3%. Medical Diagnosis UoT (CS) outperforms baselines in simplified medical diagnostics, achieving a 97.0% success rate on the DX dataset with GPT-4. On the MedDG dataset, UoT (CS) on Gemini- 1.5-Pro and GPT-4 achieve success rates of 81.4% and 88.0%. It also reduces conversation lengths 8to an average MSC of 2.0 on GPT-4 for DX, lower than 3.5 and 3.0 for DP methods.These results demonstrate the versatility of our UoT in handling both binary and open-ended interactions effectively. Troubleshooting UoT (CS) with GPT-4 similarly achieves the highest SR of 67.3%, and the lowest MSC of 7.8. It also shows a remarkable improvement from 43.7% to 67.3% in Success Rate. Overall Performance On average, UoT enhances the success rate by 38.1% compared to DP across 5 datasets and 5 different LLMs, including open source and commercial models. Notably, Success Rate increases 46.6% for Llama3-70B. Furthermore, UoT outperforms CoT-SC by 33.8% and Reflexion by 29.9%. Even compared to tree structure methods like Original-ToT and Adapted-ToT, UoT still shows superior performance with gains of 28.3% and 12.4% respectively. Additionally, Pruned UoT, our pruning method to improve efficiency, outperforms Adapted-ToT by 7.36%. Addi- tionally, our study shows that UoT’s one-step planning is effective due to effective reward design and question selection. We limit simulations to three steps for budgetary reasons, balancing efficiency and effectiveness (see Appendix E for further details on simulation depth). To determine whether the differences in success rates between the two methods were statistically significant, we performed a t-test. The results and details are in Appendix H. Case Studies and Reliability of GPT-4 as answerer Figure 3 shows UoT, compared to direct prompting, more effectively reduce uncertainty and narrow down candidates, avoiding overly specific queries. After gaining initial information (e.g., stomach pain), it generates targeted questions about related issues rather than general inquiries. Additionally, GPT-4’s accuracy as answerer is evaluated by analyzing 10% of interactions from each dataset, consistently showing reliable responses. For quantitative details, see Appendix F. 3.3 Analysis 3.3.1 Comparing Model Performance at Equal Computational Efficiency We compare the performance of approaches with similar computational costs in a closed set setting, in terms of token consumption. To do so, we first prune our UoT as described in section 2.7. Secondly, we expand exploration depth of Adapted-ToT method to bring its token cost in line with that of UoT. As shown in the top half of Table 2, the Pruned UoT method, despite its reduced efficacy compared to UoT, still outperforms ToT and other methods. Also, the bottom part of Table 2 shows that even when increasing the depth of Adapted ToT (Adapted-ToT (D = 4)) to match the token cost of UoT (D = 3), it still underperforms compared to UoT. 3.3.2 Effectiveness of Uncertainty Rewards To further demonstrate the effectiveness of our uncertainty-based reward, we compare it with the self-evaluation reward used in the original ToT based on GPT-4 model. We implement the uncertainty- based reward in place of the self-evaluation reward in ToT, creating a variant we call ToT (+UR). The results, as shown in left side of Figure 4, indicate that our reward significantly enhances planning efficacy by an average of 5.9%. Additionally, we use the heuristic self-evaluation reward in Adapted- ToT to replace our current uncertainty-based reward in UoT, a variant we refer to as UoT (-UR). Q1:IsXalivingthing?A1:Yes,Xisalivingthing.…Q8:IsXlargerthanahuman?A8:No,Xisnotlargerthanahuman.Q9:IsXfoundinNorthAmerica?A9:No,XisnotfoundinNorthAmerica.Q10 :IsXfoundinAfrica?A10:No,XisnotfoundinAfrica.Q11:IsXfoundinAsia?A11:Yes,XisfoundinAsia.…Q14:IsXaleopard?A14:Yes,Xisatypeofleopard.Q15:IsXasnowleopard?A15:Youguessedit.Xis'Snowleopard'. Q1:IsXaman-madeobject?A1:No,Xisnotaman-madeobject.Q2:IsXananimal?A2:Yes,Xisananimal.Q3:IsXacarnivore?A3:Yes,Xisacarnivore.Q4:IsXamammal?A4:Yes,Xisamammal.Q5:IsXamammalthatistypicallyassociatedwithwater?A5:No,Xisnotassociatedwithwater....Q9:IsXaleopard?A9:Yes,Xisatypeofleopard.Q10:IsXasnowleopard?A10:Youguessedit.Xis'Snowleopard'. Direct PromptingUoT  Q1:Haveyouexperiencedanynauseaorvomiting?A1:No,Ihaven'texperiencedanynauseaorvomiting.Thepainstartedaroundsixo'clock ….Q2:Doesthepainradiatetoanyotherpartofyourbody,suchasyourbackorshoulder?A2:No,thepaindoesn'tradiatetoanyotherpartofmybody.It'sjustinthemiddleofmyabdomen,rightabovemybellybutton.…Q5:Doyouexperiencepainaftereating,particularlyfattyorgreasyfoods?A5:No,Idon'tfeelanypainaftereating.ButIstartedfeelingpainaround6o'clock…. Q1:Doyouexperienceaburningorgnawingsensationinyourstomach,especiallyonanemptystomach?A1:Yes,Idohavethissensation,especiallyonanemptystomach.Q2:Haveyounoticedanydark,tarrystoolsorvomitthatlookslikecoffeegrounds?A2:Ihaven'tnoticedthecolorofmystool,butIhaven'tvomited.A3:You areprobably experiencingGastriculcer? Self-Report: Stomachache. Pain in the upper abdominal area. No diarrhea. (Female, 27 years old)Direct PromptingUoT Figure 3: Case studies from the 20 Questions game (left) and simplified medical diagnosis (right). 9Table 2: Average success rates for 20Q, MD, and TB at comparable efficiency, measured by GPT-4 token use. k is sampling count, D is tree depth. Method Tokens 20Q MD TB CoT-SC(k = 33) 4.6k 32.6 37.6 42.5 Orig-ToT(D = 3) 4.5k 23.7 65.3 40.4 Adapt-ToT(D = 3) 4.5k 33.8 85.1 60.3 Pruned UoT(D = 3) 4.7k 48.1 88.4 63.2 Adapt-ToT(D = 4) 9.3k 40.9 86.7 63.7 UoT(D = 3) 9.2k 54.4 92.5 66.0 33.8 85.160.345.3 85.666.0 0102030405060708090 20Q MD TB ToT ToT (+UR) 48.5 89.0 66.054.4 91.767.3 0102030405060708090100 20Q MD TB UoT (-UR) UoT Figure 4: Success rate comparison between Adapted-ToT and Adapted-ToT using uncer- tainty reward, and between UoT and UoT with- out uncertainty reward. This change results in a performance decrease shown in the right part of Figure 4, further validating the effectiveness of our uncertainty-based reward. Moreover, the performance of UoT (-UR) still surpasses that of Adapted-ToT illustrated in Table 5, 4 Related Work Planning and Reasoning of LLMs LLMs show prowess in planning and reasoning. Wei et al.Wei et al. (2022) introduced CoT prompting for intermediate reasoning; Yao et al.Yao et al. (2023) proposed ToT prompting using DFS/BFS. Besta et al.Besta et al. (2023) present GoT to solve elaborate problems. Feng et al.Feng et al. (2023) illustrated TS-LLM’s tree-search guided decoding. ReAct Yao et al. (2022) offers acting-based prompting, while Reflexion Shinn et al. (2023) enhances this with feedback reflection. Zhou et al.Zhou et al. (2023) unify reasoning and planning. Decision-making and Information-seeking by LLMs LLMs have evolved as decision-making tools, with models like LLM+P Liu et al. (2023) and LLM-DP Dagan et al. (2023) combining external planners and LLMs for natural language-based programming. RAP Hao et al. (2023) goes beyond structured language, using LLMs with Monte Carlo Tree Search (MCTS) Chaslot et al. (2008) for dynamic decision-making. This approach is also seen in the work of Zhao et al.Zhao et al. (2023), applying MCTS and LLM knowledge for complex tasks like robot control. However, MCTS struggles in uncertain scenarios due to its reliance on terminal states and specific modules for rewards and action selection. Additionally, to enhance LLMs’ questioning abilities, Deng et al.Deng et al. (2023) introduce the Rephrase and Respond method. A VIS Hu et al. (2023) represents an autonomous visual question answering system that uses external tools. Pan et al.Pan et al. (2023) introduce KwaiAgents for processing queries, following guidelines, and accessing external documents. Frameworks such as MEDIQ Li et al. (2024) and MDAgents Kim et al. (2024) improve the reliability of LLMs in clinical settings by strengthening information-seeking capabilities and agent systems, thereby supporting more realistic diagnostic processes. Bertolazzi et al. Bertolazzi et al. (2023) explore Chatgpt’s information seeking strategy in 20-questions game. 5 Limitation and Future Work In practice, ΩA v and ΩN v might overlap, as different answers (such as “yes\" or “no\") may lead to the exclusion of different sets of possibilities. Another similar limitation is that some questions or answers may not fully eliminate certain possibilities (e.g.,“I don’t have a fever\" does not 100% eliminate the possibility of having COVID-19). Furthermore, compared to completely open-ended interaction in medical diagnosis or troubleshooting, our current benchmark represents a simplified scenario. In theory, such cases could be handled using the method of converting interactions into probability estimations and applying some kind of Bayesian update to the probabilities of each possibility, rather than just eliminating some subset. 6 Conclusion and Discussion This paper presents the Uncertainty of Thoughts (UoT) algorithm, significantly improving LLMs in tasks requiring active information seeking through tree-based simulation, uncertainty-based rewards 10and a reward propagation scheme. On five datasets UoT increases success rate by 38.1% on average, establishing a new benchmark for evaluating LLMs in active information-seeking tasks. We evaluate UoT on simplified scenarios; more realistic scenarios raise challenges like allowing incomplete elimination of possibilities by answers, and others which we leave for future work. 7 Acknowledgment Pang Wei Koh is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Innovation under the AI Visiting Professorship Programme (award number AIVP-2024-001). References AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob /main/MODEL_CARD.md. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Anthropic. Claude 2, 2023. URL https://www.anthropic.com/index/claude-2. Anthropic. Introducing the next generation of claude. 2024. URL https://www.anthropic.com/ news/claude-3-family. Leonardo Bertolazzi, Davide Mazzaccara, Filippo Merlo, and Raffaella Bernardi. Chatgpt’s in- formation seeking strategy: Insights from the 20-questions game. In Proceedings of the 16th International Natural Language Generation Conference, pp. 153–162, 2023. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023. Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-carlo tree search: A new framework for game ai. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 4, pp. 216–217, 2008. Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023. Cohere. Cohere for ai, 2023. URL https://cohere.com/. 11Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023. Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205, 2023. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree- search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris I Baker. Things: A database of 1,854 object concepts and more than 26,000 naturalistic object images. PloS one, 14(10):e0223792, 2019. Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, and Alireza Fathi. Avis: Autonomous visual information seeking with large language model agent. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Adaptive collaboration strategy for llms in medical decision making. arXiv preprint arXiv:2404.15155, 2024. Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, and Yulia Tsvetkov. Mediq: Question-asking llms for adaptive and reliable medical reasoning. arXiv preprint arXiv:2406.00922, 2024. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023. Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In CCF International Conference on Natural Language Processing and Chinese Computing, pp. 447–459. Springer, 2022. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023. arXiv preprint arXiv:2303.16634. Mistral.AI. Mistral large, our new flagship model. 2024. URL https://mistral.ai/news/mist ral-large/. David Noever and Forrest McKee. Chatbots as problem solvers: Playing twenty questions with role reversals. arXiv preprint arXiv:2301.01743, 2023. OpenAI. Gpt-3.5 turbo: A high-performance language model, 2023a. URL https://www.openai .com/research/gpt-3-5-turbo . Whitepaper. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023b. Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, and Bing Qin. Kwaiagents: Generalized information-seeking agent system with large language models. arXiv preprint arXiv:2312.04889, 2023. Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 1–22, 2023. Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, and Mausam. End-to-end learning of flowchart grounded task-oriented dialogs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 4348–4366, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 357. URL https://aclanthology.org/2021.emnlp-main.357. 12Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948. Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh- ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023. Lin Xu, Qixian Zhou, Ke Gong, Xiaodan Liang, Jianheng Tang, and Liang Lin. End-to-end knowledge-routed relational dialogue system for automatic diagnosis. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 7346–7353, 2019. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. arXiv preprint arXiv:2305.14078, 2023. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. 13A Derivation of Information Gain Formula Recall that the information gain at nodev is defined as the expected change in uncertainty (or entropy) when receiving an answer at this node, which we defined as: IGv(X) := Hv(X) − pA v · HY v (X) − pN v · HN v (X) (12) We now show that: Proposition 1. The information gain at node v is equal to: IGv(X) = −pA v log pA v − pN v log pN v (13) Proof. Note that for any outcome xi, we have by the rules of conditional probability: p(xi|ΩA v ) = p(xi|Ωv) p(ΩAv |Ωv) = p(xi|Ωv) pAv (14) Now the information gain is: IGv(X) = Hv(X) − pA v · HA v (X) − pN v · HN v (X) = − X i:ωi∈Ωv p(xi|Ωv) logp(xi|Ωv) + pA v X i:ωi∈ΩAv p(xi|ΩA v ) logp(xi|ΩA v ) + pN v X i:ωi∈ΩNv p(xi|ΩN v ) logp(xi|ΩN v ) = X i:ωi∈ΩAv p(xi|ΩA v )(log p(xi|ΩA v ) − log p(xi|Ωv)) + X i:ωi∈ΩNv p(xi|ΩN v )(log p(xi|ΩN v ) − log p(xi|Ωv)), where the last equality holds bypA v ·p(xi|ΩA v ) = p(xi|Ωv), and similarly for pN v . We further compute that X i:ωi∈ΩAv p(xi|ΩA v )(log p(xi|ΩA v ) − log p(xi|Ωv)) = X i:ωi∈ΩAv p(xi|ΩA v ) log p(xi|ΩA v ) p(xi|Ωv) = − X i:ωi∈ΩAv p(xi|ΩA v ) logpA v = −pA v log pA v Analogously the remaining term is −pN v log pN v . Finally we conclude that IGv(X) = −pA v log pA v − pN v log pN v (15) In fact, this proposition can also be proven using some properties of information theory, particularly the definitions of conditional entropy and mutual information. As the more computational proof shown here is still relatively short and does not require defining certain additional probability distributions, we provide the computational proof here instead. 14B Comparison of Various Scaling Methods in Reward Function Design We also consider multiple scaling schemes, including Logarithmic Transformation Scaling, Sigmoid Transformation Scaling and Piecewise Function Scaling. The results demonstrate that our current setting is the optimal one. Additionally, our current design, particularly setting lambda > 0, is intended as a straightforward method to incorporate our preference for a sharper reward, as it accelerates the decay of rewards as we move away from 0.5. Furthermore, it is also intended to penalize questions that are too specific when the set of possibilities remains relatively large as |pA v − pN v | will be large. We elaborate all the scaling methods and their corresponding results below. Vanilla Expected Information Gain (IG) IGv(X) = −pA v log pA v − pN v log pN v (16) Logarithmic Transformation Scaling (LTS), wherek = 1 L(IGv(X)) = log(1 + k · IGv(X)) log(1 + k) (17) Sigmoid Transformation Scaling (STS), where τ = 10 and θ = 0.5 S(IGv(X)) = 1 1 + e−τ(IGv(X)−θ) (18) Piecewise Function Scaling (PFS), where λ = 0.5 P(IGv(X), pA v ) = ( IGv(X) λ · pA v if pA v ≤ λ IGv(X) 1−λ · (1 − pA v ) if pA v > λ (19) Uncertainty-based Reward (UR) Ru(v) = fIGv(X) := −pA v log pA v − pN v log pN v 1 + λ−1|pAv − pNv | (20) In particular, in this experiment, we use 20Q-BIG-bench (introduced in §I.2) and Common dataset instead of Thing dataset in 20 Question scenario. Datasets are the same as the main chapters in other scenarios. Model 20Q-BIG-bench Common DX MedDG FloDial IG 51.7 41.4 90.4 81.1 67.9 LTS 51.7 40.5 91.3 78.0 65.4 STS 51.3 35.1 89.4 82.3 63.4 PFS 37.9 36.9 89.4 81.3 67.1 UR 51.7 44.2 92.1 81.3 67.1 Table 3: Performance(Successful Rate) comparison of different reward methods based on GPT-3.5 C Comparison of Different Reward propagation Schemes We also consider different reward propagation schemes and introduce their benefits as well as drawbacks. Cumulative Reward Path Selection (CRPS): We used the strategy of calculating and comparing the cumulative reward for each path (from the root node to the leaf node), which involves multiplying the rewards of all nodes along the path and then selecting the path with the highest cumulative reward for the first question to interact with the user. This method focuses on identifying the single path that is most likely to yield a high reward. Its main limitation is that it may rely too heavily on the performance of a single path, neglecting the exploration of the overall problem space. 15UoT-Max: Similar to the reward propagation scheme we are currently using, we considered adopting the approach of selecting the maximum reward among the children nodes (when the node is a questioner node) in the calculation of the expected reward. Opting for the maximum child node reward tends to pursue high rewards more aggressively, which may be more effective in some situations but could also overlook the need for exploration, potentially not always being optimal in the long run. Re(v) :=    Ra(v) if v is a leaf; otherwise: pA v Re(vA) + pN v Re(vN ) if v is an Answerer Node maxw∈Children(v) Re(w) if v is a Questioner Node In particular, in this experiment, we use 20Q-BIG-bench (introduced in §I.2) and Common dataset instead of Thing dataset in 20 Question scenario. Datasets are the same as the main chapters in other scenarios. Models Method 20Q-BIG-bench Common DX MedDG FloDial GPT-3.5 CRPS 62.1 47.7 92.1 81.3 56.2 UoT-Max 48.3 41.4 92.1 80.3 60.1 UoT 65.5 62.2 92.1 83.3 63.2 GPT-4 CRPS 75.9 68.5 94.2 82.9 61.4 UoT-Max 79.3 63.7 95.1 83.1 62.6 UoT 79.3 71.2 97.0 88.0 67.3 Table 4: Performance (Success Rate) comparison of different reward propagation schemes. The results also demonstrate the superiority of our current reward propagation scheme. Compared to the other two reward propagation schemes, the existing approach takes into account the cumulative rewards of all paths, providing a more holistic and balanced decision-making mechanism. Instead of merely relying on the maximum short-term rewards or the performance of a single path, it is designed to capture long-term benefits, focusing on sustainable outcomes rather than immediate short-term gains. D Experimental Performance for Earlier Released LLMs In these experiments, we use 20Q-BIG-bench (introduced in §I.2) and Common dataset instead of Thing dataset in 20 Question scenario. Datasets are the same as the main chapters in other scenarios. Table 5: Results from three different scenarios, assessing Success Rate (SR), Mean Conversation Length in Successful Cases (MSC), and Mean Conversation Length (MCL). Model Method 20 Questions Medical Diagnosis Troubleshooting20Q in BIG-bench Common DX MedDG FloDialSR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ Llama2-70BDP(OS) 6.90 12.0 19.5 1.80 11.0 19.8 13.4 3.1 4.8 23.7 3.4 4.6 11.1 15.1 19.5DP(CS) 17.2 13.5 18.9 6.31 12.0 19.7 29.8 3.0 4.4 28.0 3.5 4.6 24.2 14.5 18.7UoT(CS)20.7 13.2 18.6 10.8 15.6 19.5 51.9 1.8 3.4 33.9 1.4 3.8 31.4 15.8 18.7 Cohere DP(OS) 3.45 15.0 19.8 1.80 14.0 19.9 19.8 3.7 4.7 25.0 3.6 4.7 16.3 16.7 19.5DP(CS) 6.90 12.0 19.4 1.80 12.5 19.8 35.6 3.3 4.4 33.3 4.0 4.7 27.5 16.3 19.0UoT(CS)34.3 8.50 16.0 16.2 11.7 18.6 45.5 2.6 3.9 75.7 2.7 3.3 41.4 8.7 15.3 PaLM 2 DP(OS) 37.9 13.5 17.5 35.1 14.4 18.0 7.69 3.9 4.9 11.3 4.0 4.9 22.6 15.2 19.0DP(CS) 51.7 13.2 16.5 53.1 13.9 16.8 7.92 3.4 4.9 34.0 4.4 4.8 30.1 15.0 18.5UoT(CS)72.4 7.0 10.6 62.1 12.5 15.3 75.0 2.1 2.8 80.7 2.2 2.7 48.4 7.6 14.0 Gemini-1.0-ProDP(OS) 10.3 8.3 18.8 11.7 10.0 18.8 12.5 3.2 4.8 30.7 3.7 4.6 2.61 13.0 19.8DP(CS) 20.7 14.8 18.9 12.6 12.0 19.0 64.4 3.3 3.9 40.7 3.5 4.4 5.23 16.1 19.8UoT(CS)31.0 7.8 16.2 18.9 4.0 17.0 67.3 2.1 3.7 75.0 1.4 2.7 14.2 10.6 18.6 Claude2 DP(OS) 48.3 9.8 15.1 29.7 13.8 18.2 45.2 3.0 4.1 60.7 4.1 4.5 39.7 14.3 17.7DP(CS) 72.4 11.6 13.9 43.2 13.8 17.3 97.1 2.4 2.5 83.0 4.3 4.4 42.9 15.7 18.2UoT 75.9 5.1 8.69 61.3 9.8 13.7 98.0 2.3 2.4 88.3 2.7 2.9 52.6 6.3 12.8 GPT-3.5 DP(OS) 36.0 12.6 17.3 32.6 14.6 18.2 18.8 3.5 4.7 25.0 3.5 4.6 19.4 12.3 18.5UoT(OS) 41.4 13.8 17.4 34.2 14.7 18.2 37.5 2.4 4.0 61.0 2.3 3.3 26.1 11.3 17.7DP(CS) 44.8 13.2 17.0 40.0 14.8 17.8 49.5 2.7 3.3 42.3 3.8 4.5 22.6 13.3 18.5UoT(CS)51.7 5.3 12.4 44.2 10.9 16.0 92.1 2.1 2.4 81.3 2.4 2.9 67.1 6.9 11.2 16E Effect of Simulation Depth In this experiment, we use 20Q-BIG-bench (introduced in §I.2) and Common dataset instead of Thing dataset in 20 Question scenario. Datasets are the same as the main chapters in other scenarios. As the below figure illustrates, we analyze the impact of simulation steps. Even with one-step reasoning and planning, our method can still have a strong performance, further indicating the effectiveness of our reward design and question selection mechanism. With the increase of the step, the performance can gradually rise. However, due to the constraints of computation resources and OpenAI API budgets, we only explore the simulation to the third step and argue that it can be the practical tradeoff between performance and efficiency. 1 2 360 65 70 75 80 85 90 95 66.1 67 75.2 82.6 86.7 92.5 63.5 64.7 67.3 Depth Success Rate (%) 20 Questions Medical Diagnosis Troubleshooting F Reliability of GPT-4 as the Environment As the impressive understanding ability of LLMs, previous research has validated the effectiveness of evaluators served by ChatGPT or GPT-4 Chiang & Lee (2023); Liu et al.. Consequently, we also adopt GPT-4 as the environment to provide feedback on our work. Prompts can be found in Appendix L.4. To assess the accuracy and reliability of employing GPT-4 as the environment simulator, we randomly sample 10% interaction records (including the final judgment and intermediate feedback from the environment) from each dataset. As Figure 6 shows, GPT-4 can provide completely accurate judgment and also keep a high level of accurate feedback during the interaction. These experimental results can further support the effectiveness of our method. Table 6: Human evaluation results for the accuracy of environment feedback served by GPT-4. IF represent the Accuracy of Intermediate Feedback. Scenario Judgement IF 20 Questions 100 93.7 Medical Diagnosis 100 94.4 Troubleshooting 100 92.9 170.0 0.2 0.4 0.6 0.8 1.0 pA v 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Ru(v) Uncertainty Reward Function Ru(v) with = 0.4 Ru(v) with = 0.4 Figure 5: Curve of uncertainty based reward on Eq 10, where pN v can be replaced by (1 − pA v ). The horizontal axis pA v is conditional probabilities of affirmative at node v, which are introduced in Section §2.4. G Reward Function Details and Its Curve Refer to Figure 5 for the curve of uncertainty-based reward function. H Experimental Statistical Significance We conduct three experiments on five datasets using Llama 3 and GPT-4 to compare the performance of Direct Prompting (DP) and UoT methods in a closed-set setting for significance testing. Due to LLM API quota limitations, the number of experiments are restricted. To determine whether the differences in success rates (SR) between the two methods were statistically significant, we performed a t-test. The results are presented below. GPT-4 Results Table 7: GPT-4 Comparison of DP and UoT on Success Rates (SR) Dataset DP (%) UoT (%) t-Statistic p-Value Significance Conclusion Common 49.0 70.9 -10.8 0.00041 Significant (p < 0.05) Thing 30.8 36.8 -8.04 0.00129 Significant (p < 0.05) DX 89.4 97.0 -3.11 0.03581 Significant (p < 0.05) MedDG 74.9 87.9 -7.33 0.00185 Significant (p < 0.05) FloDial 42.5 67.8 -19.8 0.00004 Significant (p < 0.05) Llama 3 Results Table 8: Llama 3 Comparison of DP and UoT on Success Rates (SR) Dataset DP (%) UoT (%) t-Statistic p-Value Significance Conclusion Common 47.7 56.5 -4.39 0.01180 Significant (p < 0.05) Thing 14.8 24.8 -16.0 0.00009 Significant (p < 0.05) DX 80.1 90.1 -4.65 0.00966 Significant (p < 0.05) MedDG 61.3 64.6 -4.15 0.01426 Significant (p < 0.05) FloDial 29.9 46.4 -10.5 0.00047 Significant (p < 0.05) 18The t-test results indicate that UoT significantly outperform DP five datasets (p < 0.05), as evidenced by their higher mean scores. I Experimental Setups I.1 Baselines Setup Chain-of-Thought (CoT) We adapt the typical CoT prompt which instruct LLM to generate the explanation or motivation for the proposed question first, then give the question to ask. Chain of Thought with Self-Consistency (CoT-SC) To make the method spend comparable compute to our approach for a fair comparison, we sampled 33 times before deciding on each action with the LLM’s temperature of 0.7. The final selected question is the one repeated most times among 33 samples. Planning Prompting To measure whether LLMs’ planning ability can be enhanced through some crafted prompts like CoT, ToT or Reflexion. We design the prompt to enable LLM to simulate multiple different sets of future interactions between questioner and answerer, then let LLM choose one most promising interaction (question) to ask. Tree of Thoughts In the case of Original-ToT, a sampling method is employed to generate 3 questions from each answer node, and the self-evaluation method is utilized for reward calculation. Subsequently, breadth-first search will be used and 10 nodes from each step will be selected for later simulation. Additionally, the temperature of the LLM is configured to 0.7, consistent with the settings in original ToT paper. In the case of Adapted-ToT, we provide more heuristical hints in prompt to generate the questions, e.g. ‘you should try to propose the question to halve the probability set’. Likewise, each answer node generates 3 questions, and the LLM selects 10 nodes with higher self-evaluation rewards to further simulation. The simulation steps are also 3. Reflextion This approach involves the LLM agent suggesting questions iteratively until the question reward exceeds the threshold of 0.7 or reaches the maximum limit of 3 questions. The reward score s is calculated using the formula s = min(pA, pN )/ max(pA, pN ). This heuristic is based on the principle of whether the question can effectively halve the probability set. If a candidate question achieves a score above the threshold, the process of proposing questions is concluded, and that question is selected. In cases where no question meets the threshold, the one with the highest score is chosen. Uncertainty of Thoughts Pruned After generating the candidate question based on the possibility set Ωi, we sorted these question nodes by uncertainty based reward and reserved half of them, serving the purpose of pruning. In subsequent steps of the simulation, this pruning operation will be continued. Other settings were the same as UoT, described in Section §I.6. I.2 Scenarios Settings and Datasets 20 Questions game is a classic guessing game where the answerer thinks of an object, person, place, or other, and the questioner, possessing no prior knowledge about the chosen entity, proceeds to pose a series of up to 20 yes-or-no questions to determine what the secret item is. The questions are designed to narrow the possibilities and ultimately guess the secret item within the 20 questions. 20 Questions in BIG-bench : It is the sub-task of BIG-bench and can be found on the GitHub website7, consist of 29 items. Common Dataset Construction: We came across an official website8 that introduces a 20 Questions game, which mentions that common target categories in this game include animals, places, food, and objects. Therefore, we extracted and manually screened the targets mentioned on this website, resulting in a dataset named \"Common\" comprising 111 targets, each belonging to one of the four aforementioned categories. Thing Dataset: It is a collection of 1,854 varied object concepts, carefully selected from tangible and easily identifiable nouns in American English by Martin at al.Hebart et al. (2019), which is publicly available on their official website9. 7https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/twenty_questions 8https://blog.prepscholar.com/20-questions-game 9https://osf.io/jum2f 19Medical Diagnosis In this scenario, the patient will simply describe their symptom first which we call a ‘Self-report’, then doctor acted by LLM will start to ask questions to interact with patient to determine the disease. Troubleshooting In FloDial dataset, trouble includes faults of car and laptop. Similar to Medical Diagnosis, the customer first describes some simple fault symptoms, then the customer support technician will chat with customer to further check the specific issues of device. LLMs Serve as Questioner (Patient or Customer) In simulated interactions involving questioner and answerer scenarios, particularly for medical diagnosis and troubleshooting, the response given by an LLM acting answerer is guided by scenario instructions and real-world dialogue examples. This approach makes the responses of answerer more human-like and enhances its accuracy in diagnosing diseases or identifying faults. While, in the game of 20-question, where the objective is to guess common items, the LLM acting as the answerer only needs to provide simple ‘yes’ or ‘no’ answers. Therefore, incorporating real-world dialogue into the LLM’s prompts for this game is not necessary. I.3 Dataset selection criteria and process for MedDG In the original MedDG dataset, numerous conversations lacked a clear diagnosis, often concluding with advice for the patient to rest or seek further tests. This ambiguity arose from patients not detailing their symptoms sufficiently or doctors lacking the information or confidence to diagnose. Consequently, these conversations hinder LLMs from accurately understanding disease and symptom information for effective patient role simulation. To address this, we curated our final evaluation set to include only conversations with explicit disease diagnoses. Furthermore, to ensure a balanced representation across the 8 disease categories, we selected roughly 40 dialogues for each disease. We also excluded conversations that were too brief (1-2 turns) or excessively lengthy (over 10 turns). The curation process involved two annotators: one for initial selection and another for verification. Given these criteria, we finally pick 500 conversations for our evaluation set, aiming to maintain the evaluation’s reliability and quality. We will also clarify this and add the details into the following version. I.4 Data Preprocessing of FloDial We process the dataset, FloDial, to convert troubleshooting flowcharts into a set of troubleshooting faults. The dialogue is grounded in specific faults, which correspond to the leaf nodes (descriptions and solutions of faults) in the flowcharts. After reviewing all the leaf nodes, we identify 153 faults that had corresponding dialogues. We then use GPT-4 to generate a clear name for each fault based on the descriptions and solutions of faults, and randomly selected one corresponding dialogue history to construct the current dataset. I.5 UoT (Open Set) Setup To initialize the possibility set as the start of the algorithm, in medical diagnosis and troubleshooting, initial descriptions from patients or customers about symptoms or issues enable UoT to establish a possibility set right from the start. For the game of 20 Questions, where initial information is scant, prematurely establishing this set could misdirect the inquiry. Therefore, for the first three rounds, we employ Direct Prompting in Open-Set (DPOS) approach to gather information and feedback. After these initial rounds, UoT takes over, refreshing the possibility set each round to refine the questioning strategy. For datasets Common, Things, DX, MedDG and FloDial, we configure the size of the possibility set for each update round, setting them at 10, 10, 5, 5, and 5, respectively. This parameter should prevent the increase in cognitive load and decrease in efficiency that comes with larger sizes, while also avoiding the limitations of focusing on a few specific items that come with smaller sizes. We experiments with values(size) between 5 and 50 based on this rationale, and the final selection of these hyperparameters is guided by empirical performance evaluations. 20I.6 Implementation Empirically, we set the plan (simulation) steps as 3 and the number of questions during the simulation is 3. The hyperparameter λ in uncertainty-based reward is 0.4. J Impact Statement This paper aims to enhance LLMs’ information seeking abilities, allowing them to make better decisions in challenging real-world settings involving uncertainty and ambiguity, and to interact more effectively with humans in human-AI settings. On the whole, we expect that information seeking capabilities should allow models to behave in a manner that is more reliable and well-aligned with human expectations, as it allows them to better resolve their uncertainty. There may be some potential societal consequences related to LLMs in general, such as privacy and trustworthiness issues. However, we do not believe that any of these must be specifically highlighted here. K Examples in Scenarios Some examples of conversation in different scenarios are provided in Table 9-12. Table 9: Example for 20 Question. Q stands for Questioner and A stands for Answerer. Target: Glass micropipette Conversation 1: Q: Is X a tangible object? A: Yes. 2: Q: Is X a living organism? A: No. 3: Q: Is X a household item? A: No. 4: Q: Is X a type of transportation? A: No. 5: Q: Is X used in construction? A: No. 6: Q: Is X a wearable item? A: No. 7: Q: Is X a type of toy? A: No. 8: Q: Is X a ’Glass micropipette’? A: You guessed it. X is a ’Glass micropipette’. Table 10: Example for Medical Diagnosis. D stands for Doctor (Questioner) and P stands for Patient (P). Disease: Allergic rhinitis Self-report Hello doctor, my son’s throat easily becomes red and swollen as soon as winter comes. He has been coughing and his throat has been red and swollen for the past half month. Conversation 1: D: Do you experience a runny or stuffy nose? P: Yes. 2: D: You may have ’Allergic rhinitis’. Table 11: Example for MedDG.D stands for Doctor (Questioner) andP stands for Patient (Answerer). Disease: Gastric ulcer Self-report Stomachache. Pain in the upper abdominal area. No diarrhea. (Female, 27 years old) Conversation 1: D: Do you experience a burning or gnawing sensation in your stomach, especially on an empty stomach? P: Yes, I do have this sensation, especially on an empty stomach. 2: D: Have you noticed any dark, tarry stools or vomit that looks like coffee grounds? P: I haven’t noticed the color of my stool, but I haven’t vomited. 3: D: You may have ’Gastric ulcer’. 21Table 12: Example for Troubleshooting. T stands for customer support technician and C stands for customer. Fault: Brake fluid overflow Self-report Lately, I have been having trouble with the brakes of my car. I have checked it again and again but can’t figure out what’s causing this. Please help me. Conversation 2: T: Does the issue affect the rotation or movement of the wheels? C: No. 3: T: Does the issue seem to be related to the overall braking system? C: Yes. 4: T: Are you experiencing a brake fluid leak? C: No. 5: T: Are your brake pads worn out? C: No. 6: T: Is the brake pedal feeling spongy or soft when you press it? C: Yes. 7: T: Is there an issue with the brake lines being air-filled or damaged? C: No. 8: T: Your car may have a brake fluid overflow issue. L Prompts L.1 DP(OS) and DP(CS) Prompt The specific prompts for the DP(OS) and DP(CS) baselines in each scenario are provided in Table 13-15. Table 13: DPOS and DPCS Prompt for 20 Question game. The two settings are the same except for the content noted in red, which is only for DPCS Prompt. The first time the content marked in red is mentioned at the beginning of interaction is to inform the questioner of the entire probability set. The second mention serves as a reminder for the questioner to determine the target as soon as possible when the conversation is nearing its end. We establish two phases with the aim of encouraging the questioner to guess the target as soon as possible when the second phase of interaction is nearing its conclusion, in order to avoid failure at the end of interaction. Prologue Let us play the game of 20 questions. I am impersonating the thing, X. You will ask me up to 20 questions that start with ’Is X’ and can only be answered by yes or no, and I will answer each one truthfully based on being X. Let us begin. Ask me the first question. Reply to me with your first question. The X may be one of the items below: [list of items] Phase 1 (Q1 - Q13) None Phase 2 (Q14 - Q20) The X may be one of the items below: [list of items] L.2 Planning Prompt The specific prompts for Planning Prompt baselines in each scenario are provided in Table 28-27. As planning prompt method is close set setting, hence the probability set will also be informed in the prompt as DPCS prompt. We do not repeat it in the tables. L.3 UoT Prompt The detailed prompts for our UoT method in each scenario are attached in Table 19-21. L.4 Questioner Prompt The specific prompt to guide the examiner in all scenarios is provided in Table 22-24. 22Table 14: DPOS and DPCS are medical diagnostic prompts with similar structures. The key difference is the inclusion of probability set information in red text for DPCS. This red text appears twice: initially to inform the questioner about all potential diagnoses and again towards the end of the interaction as a reminder to quickly confirm the disease. We establish two phases with the aim of encouraging the questioner to confirm the disease as soon as possible when the second phase of interaction is nearing its conclusion, avoiding failure at the end of interaction. Prologue You are a doctor and your patient reports that: [self-report] You should ask your patient questions in English with symptoms which can only be answered by yes or no, to find what disease this patient suffers. Let us begin. Ask me the first question. Based on the symptoms above, if you find out the disease, please ask ’Are you a [disease name]?’ The patient may suffer from one of the diseases below: [list of diseases] Phase 1 (Q1) None Phase 2 (Q3 - Q5) Based on the symptoms above, if you find out the disease, please ask ’Are you a [disease name]?’ The patient may suffer from one of the diseases below: [list of diseases] Table 15: DPOS and DPCS are troubleshooting prompts with similar structures, but DPCS includes unique content highlighted in red. This red content appears first at the beginning, outlining all potential faults, and again towards the end as a reminder to swiftly identify the fault. The two-phase structure of these prompts aims to ensure quick fault confirmation, especially in the final stages of the interaction, to prevent failure. Prologue You are a technician and your client reports that: [self-report] You should ask your client questions about a specific situation which can only be answered by yes or no, in order to find where the issue this client faces with located. Let us begin. Ask me the first question. The client may face one of the issues below: [list of issues] Phase 1 (Q1 - Q13) None Phase 2 (Q14 - Q20) Based on the situations above, if you find out the issue, please ask ’Are you a [issue name]?’ The client may face one of the issues below: [list of issues] 23Table 16: Planning Prompt for 20 Question game. [C1] is the count of questions asked and [C2] is the count of questions remaining. The ‘information gained’ marked blue represents the previous interaction history. We divide it into three phases to discuss the probability set as quickly as possible, conduct simulation for planning, and remind the questioner to guess the answer. Prologue Same as prompts in Appendix L.1 Phase 1 (Q1 - Q4) The next question should narrow down the possible range of X, preferably in half. Phase 2 (Q5 - Q15) We are playing the 20 Question game, [C1] questions have been asked. And now we know: [information gained] Based on the features of X above, please guess what X exactly is and tell me your top 3 most likely answers. For these three candidate X, please separately complete the remaining [C2] questions and answer yes/no by yourself. Notably, you must guess the corresponding X before the last question. Phase 3 (Q16 - Q20) Note that you should guess what X exactly is from now on. The question must start with ’Is X ...’ Table 17: Planning Prompt for Medical Diagnosis. [ C1] is the count of questions asked and [ C2] is the count of questions remaining. The ‘information gained’ marked blue represents the previous interaction history. We divide it into three phases to discuss the probability set as quickly as possible, conduct simulation for planning, and remind the questioner to confirm the disease. Prologue Same as prompts in Appendix L.1 Phase 1 Skip because of the limited QA rounds in this scenario Phase 2 (Q1 - Q3) You are the doctor asking questions to diagnose, [C1] questions have been asked. And now we know about the patient: [information gained] Based on the symptoms of the patient above, please think about what disease the patient suffers from and tell me your top three most likely answers. For these three candidate diseases, please separately complete the remaining [C2] questions and answer yes/no by yourself. Notably, you must determine the corresponding disease before the last question. Phase 3 (Q4 - Q5) Note that you should determine what disease the patient suffers from now. The question must start with ’Are you a [disease name]?’ 24Table 18: Planning Prompt for Troubleshooting. [ C1] is the count of questions asked and [ C2] is the count of questions remaining. The ‘information gained’ marked blue represents the previous interaction history. We divide it into three phases to discuss the probability set as quickly as possible, conduct simulation for planning, and remind the questioner to confirm the fault. Prologue Same as prompts in Appendix L.1 Phase 1 (Q1 - Q4) The next question should narrow down the possible range of trouble issues, preferably in half Phase 2 (Q5 - Q15) You are a technician to troubleshoot, [C1] questions have been asked. And now we know: [information gained] Based on the situation your client faces, please think about what the issue exactly is and tell me your top 3 most likely answers. For these three candidate issues, please separately complete the remaining [C2] questions and answer yes/no by yourself. Notably, you must determine the corresponding issue before the last question. Phase 3 (Q16 - Q20) Note that you should determine what issue your client faces from now on. The question must start with ’Are you a [issue name]?’ Table 19: UoT Prompt for the 20 Questions Game: As it is based on a closed-set setting, information about probabilities will be given at the beginning of the interaction and will be reminded after Q14. Since it is similar to previous prompts, we will not repeat it here. In the ’Prompt for Question Generation and Simulation’, the count of YES/NO indicates the number of items that are consistent with the affirmative/negative response. Prologue Let us play the game of 20 questions. I am impersonating the thing, X. You will ask me up to 20 questions that start with ’Is X’ and can only be answered by yes or no, and I will answer each one truthfully based on being X. Let us begin. Ask me the first question. Reply to me with your first question. Prompt for Question Generation and Simulation Please design a question about X and can only be answered by YES or NO. asked Then classify the possible X above based on this question. If the answer is ’YES’, put this X into ’YES: ...’, otherwise to ’NO: ...’. Finally calculate how many X in YES and NO. Notably, this question should fulfill that the count of YES and NO are almost the same with a permissible discrepancy of no more than one! You should think about best n questions to respond to. And your answer should be: Question 1: Is X ...? YES: item1, item2, ... Count of YES: ... NO: item1, item2, ... Count of NO: ... Additional Reminder in Q14 - Q20 Note that you should guess and ask what X exactly is from now on. X is possible a: [item_list_str], or other. The question must start with ’Is X ... 25Table 20: UoT Prompt for medical diagnosis: As it is based on a closed-set setting, information about probabilities will be given at the beginning of the interaction and will be reminded after Q3. Since it is similar to previous prompts, we will not repeat it here. In the ’Prompt for Question Generation and Simulation’, the count of YES/NO indicates the number of diseases that are consistent with the affirmative/negative response. Prologue You are a doctor and your patient reports that: [self-report] You should ask your patient questions in English with symptoms which can only be answered by yes or no, to find what disease this patient suffers. Let us begin. Ask me the first question. Prompt for Question Generation and Simulation Please design a question to ask your patient with symptoms about disease and can only be answered by YES or NO. Then classify the possible disease above based on each question. If the answer is ’YES’, put this disease into ’YES: ...’, otherwise to ’NO: ...’. Finally calculate how many X in YES and NO. Notably, this question should fulfill that the count of YES and NO are almost the same with a permissible discrepancy of no more than one! You should think about best n questions to respond to. And your answer should be: Question 1: ...? YES: disease1, disease2, ... (disease names only) Count of YES: ... NO: disease1, disease2, ... (disease names only) Count of NO: ... Additional Reminder in Q3 - Q5 Note that you should point out and ask what disease the patient suffers from now. The patient may suffer from one of diseases below: [list of disease], or other. The question must be ’You may have a [disease name]?’ 26Table 21: UoT Prompt for troubleshooting: As it is based on a closed-set setting, information about probabilities will be given at the beginning of the interaction and will be reminded after Q14. Since it is similar to previous prompts, we will not repeat it here. In the ’Prompt for Question Generation and Simulation’, the count of YES/NO indicates the number of faults which are consistent with the affirmative/negative response. Prologue You are a technician and your client reports that: [self-report] You should ask your client questions about a specific situation which can only be answered by yes or no, in order to find where the issue this client faces with located. Let us begin. Ask me the first question. Prompt for Question Generation and Simulation Please design a question to ask your client with specific situation and can only be answered by YES or NO. asked Then classify the possible issue above based on this question. If the answer is ’YES’, put this issue into ’YES: ...’, otherwise to ’NO: ...’. Finally calculate how many issues in YES and NO. Notably, this question should fulfill that the count of YES and NO are almost the same with a permissible discrepancy of no more than one! You should think about best n questions to respond to. And your answer should be: Question 1: ...? YES: issue1, issue2, ... (issue names only) Count of YES: ... NO: issue1, issue2, ... (issue names only) Count of NO: ... Additional Reminder in Q14 - Q20 Note that you should point out and ask what disease the patient suffers from now. The patient may suffer from one of diseases below: [list of disease], or other. The question must be ’You may have a [disease name]?’ Table 22: Prompt for Answerer in 20 questions game. [target] is the name of the final target for each dialogue. Prologue Let us play the game of 20 questions. You are the answerer and I am the guesser. X is ’[target]’. I will ask you up to 20 questions and you should answer each one truthfully based on being X. If I guess correctly what is X, answer me \"You guessed it. X is ’[ target]’.\" Note that never directly tell me what X is all the time. Let us begin. Here is my first question. Table 23: Prompt for Answerer in medical diagnosis. [disease] is the name of the final disease for each dialogue. Prologue You are the patient suffering ’[target]’ and I am the doctor. I will ask you up to 5 questions and you should answer each one truthfully based on your disease. If I point out correctly what disease you experience, answer me \"You are right. I am experiencing ’[target]’.\" Note that never directly tell me what disease is all the time. Let us begin. Here is my first question. Table 24: Answerer Prompt in troubleshooting. [fault] is the name of the final fault for each dialogue. Prologue You are the client with a device that has ’[target]’ and I am the technician. I will ask you up to 20 questions and you should answer each one truthfully based on the issue of your device. If I point out correctly what your issue is, answer me \"You are right. My device has ’[target]’.\" Note that never directly tell me what the issue is all the time. Let us begin. Here is my first question. 27Table 25: ToT Prompt for 20 Question game. [C1] is the count of questions asked. The ‘information gained’ marked blue represents the previous interaction history. Standard Prompt You are playing the game of 20 questions. I am impersonating the thing, X. You will ask me up to 20 questions that start with ’Is X’ and can only be answered by yes or no, and I will answer each one truthfully based on being X. [C1] questions have been asked. And now we know: [information gained] Design a question about X and can only be answer by YES or NO. Additional Reminder in Q14 - Q20 Same as prompts in Appendix L.3 Table 26: ToT Prompt for Medical Diagnosis. [C1] is the count of questions asked. The ‘information gained’ marked blue represents the previous interaction history. Standard Prompt You are a doctor and your patient reports that: [self-report] You should ask your patient questions in English with symptoms which can only be answered by yes or no, to find what disease this patient suffers. [C1] questions have been asked. And now we know: [information gained] Design a question to ask your patient with symptoms about disease and can only be answered by YES or NO. Additional Reminder in Q14 - Q20 Same as prompts in Appendix L.3 Table 27: ToT prompt for Troubleshooting. [C1] is the count of questions asked. The ‘information gained’ marked blue represents the previous interaction history. Standard Prompt You are a technician and your client reports that: [self-report] You should ask your client questions about a specific situation which can only be answered by yes or no, in order to find where the issue this client faces with located. [C1] questions have been asked. And now we know: [information gained] Design a question to ask your client with specific situation and can only be answered by YES or NO. Additional Reminder in Q14 - Q20 Same as prompts in Appendix L.3 28Table 28: CoT Prompts. Prologue Same as prompts in Appendix L.1 Prompt for Generating Question and Explanation What’s your next question? Let’s think step-by-step and reply me with your explanation. Your answer should be: Explanation: [insert step-by-step analysis here] Question: [next question] Additional Reminder in Q14 - Q20 Same as prompts in Appendix L.3 29",
      "references": [
        "Llama 3 model card",
        "Palm 2 technical report",
        "Claude 2",
        "Introducing the next generation of claude",
        "Chatgpt’s information seeking strategy: Insights from the 20-questions game",
        "Graph of thoughts: Solving elaborate problems with large language models",
        "Monte-carlo tree search: A new framework for game ai",
        "Can large language models be an alternative to human evaluations?",
        "Cohere for ai",
        "Dynamic planning with a llm",
        "Rephrase and respond: Let large language models ask better questions for themselves",
        "Alphazero-like tree-search can guide large language model decoding and training",
        "Reasoning with language model is planning with world model",
        "Things: A database of 1,854 object concepts and more than 26,000 naturalistic object images",
        "Avis: Autonomous visual information seeking with large language model agent",
        "Adaptive collaboration strategy for llms in medical decision making",
        "Mediq: Question-asking llms for adaptive and reliable medical reasoning",
        "Llm+ p: Empowering large language models with optimal planning proficiency",
        "Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation",
        "G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023",
        "Mistral large, our new flagship model",
        "Chatbots as problem solvers: Playing twenty questions with role reversals",
        "Gpt-3.5 turbo: A high-performance language model",
        "Gpt-4 technical report",
        "Kwaiagents: Generalized information-seeking agent system with large language models",
        "Generative agents: Interactive simulacra of human behavior",
        "End-to-end learning of flowchart grounded task-oriented dialogs",
        "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
        "A mathematical theory of communication",
        "Reflexion: an autonomous agent with dynamic memory and self-reflection",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
        "Llama 2: Open foundation and fine-tuned chat models",
        "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
        "Self-consistency improves chain of thought reasoning in language models",
        "Chain-of-thought prompting elicits reasoning in large language models",
        "The rise and potential of large language model based agents: A survey",
        "End-to-end knowledge-routed relational dialogue system for automatic diagnosis",
        "React: Synergizing reasoning and acting in language models",
        "Tree of thoughts: Deliberate problem solving with large language models",
        "Large language models as commonsense knowledge for large-scale task planning",
        "Language agent tree search unifies reasoning acting and planning in language models"
      ],
      "meta_data": {
        "arxiv_id": "2402.03271v3",
        "authors": [
          "Zhiyuan Hu",
          "Chumin Liu",
          "Xidong Feng",
          "Yilun Zhao",
          "See-Kiong Ng",
          "Anh Tuan Luu",
          "Junxian He",
          "Pang Wei Koh",
          "Bryan Hooi"
        ],
        "published_date": "2024-02-05T18:28:44Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Uncertainty of Thoughts (UoT), a plug-and-play algorithm that enhances large language models (LLMs) with active information-seeking capabilities by asking effective questions. UoT combines uncertainty-aware simulation, information gain-motivated rewards, and a reward propagation scheme. It also introduces a new benchmark for evaluating LLMs in active information-seeking tasks across three scenarios (20 Questions, medical diagnosis, troubleshooting). UoT achieves an average performance improvement of 38.1% in task completion rate across multiple LLMs compared to direct prompting, while also improving efficiency.",
        "methodology": "UoT (Uncertainty of Thoughts) is a three-component approach: 1) **Question Generation and Simulation**: An LLM generates candidate questions and simulates future scenarios in a tree structure, exploring possible answers (affirmative/negative) and subsequent questions. 2) **Uncertainty-Based Reward Calculation**: Information gain, quantified by entropy reduction, is used to assign rewards to questions. A modified function, fIGv(X), with a hyperparameter λ is used to sharpen these rewards and normalize them within [0,1]. 3) **Question Selection via Reward Propagation**: Accumulated rewards (sum of rewards at a node and its ancestors) and expected rewards (expected total value from a node and its descendants) are computed. The question with the highest expected reward is selected. The framework also has extensions for open-set scenarios (unknown possibility space) and pruned UoT for efficiency.",
        "experimental_setup": "The evaluation was conducted on various LLMs including Llama-3-70B-Instruct, Mistral-Large, Gemini-1.5-Pro, Claude-3-Opus, and GPT-4, as well as earlier LLMs like Llama 2-70B-Chat, Cohere, PaLM 2, Claude 2, and GPT-3.5-turbo. Baselines for comparison included Direct Prompting (DP), Planning Prompting (PP), Chain-of-Thought (CoT), CoT-SC (Self-Consistency), Reflexion, and Tree-of-Thoughts (ToT) (Original and Adapted). The benchmark comprises three tasks and five datasets: **20 Questions** (Common and Things datasets), **Medical Diagnosis** (DX and MedDG datasets), and **Troubleshooting** (FloDial dataset). Experiments were conducted in both open-set (OS) and closed-set (CS) settings. GPT-4 was used as the Answerer in simulated interactions. Evaluation metrics included Success Rate (SR), Mean Conversation Length in Successful Cases (MSC), and Mean Conversation Length (MCL). Statistical significance was assessed using t-tests.",
        "limitations": "The current simplified scenario assumes disjoint subsets for affirmative and negative answers (ΩA v and ΩN v). In reality, these subsets might overlap, and responses may not definitively eliminate possibilities (e.g., \"I don't have a fever\" doesn't 100% rule out COVID-19). The benchmark represents a simplified version of real-world medical diagnosis and troubleshooting, which often involve more complex, open-ended interactions. The approach is limited to simplified scenarios, and handling cases with incomplete elimination of possibilities or more realistic open-ended responses would require further development, potentially involving Bayesian updates for probability estimations. Computational resource constraints limited simulations to three steps.",
        "future_research_directions": "Future work could focus on extending the UoT framework to more realistic scenarios, including allowing for overlapping possibility subsets (ΩA v and ΩN v) and handling cases where answers do not entirely eliminate certain possibilities. This might involve converting interactions into probability estimations and applying Bayesian updates. Further research could also explore completely open-ended interactions in medical diagnosis and troubleshooting. Additionally, exploring the impact of deeper simulation steps with increased computational resources could be an area of future investigation to potentially further enhance performance.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Privileged Sensing Scaffolds Reinforcement Learning",
      "full_text": "Published as a conference paper at ICLR 2024 PRIVILEGED SENSING SCAFFOLDS REINFORCEMENT LEARNING Edward S. Hu1 James Springer1 Oleh Rybkin2 Dinesh Jayaraman1 1University of Pennsylvania 2UC Berkeley hued@seas.upenn.edu ABSTRACT We need to look at our shoelaces as we first learn to tie them but having mas- tered this skill, we can do it from touch alone. We call this phenomenon “sen- sory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose Scaffolder, a reinforcement learning approach which effectively ex- ploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of prac- tical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/ 1 I NTRODUCTION It is well-known that Beethoven composed symphonies long after he had fully lost his hearing. Such feats are commonly held to be evidence of mastery: for example, novice typists need to look at the keyboard to locate keys but with practice, can graduate to typing without looking. Thus, sensing requirements may be different during learning versus after learning. We refer to this as “sensory scaffolding”, drawing inspiration from the concept of scaffolding teaching mechanisms in psychology that provide temporary support for a student (Wood et al., 1976; Vygotsky et al., 2011), like training wheels when learning to ride a bicycle. For artificial learning agents such as robots, sensory scaffolding permits decoupling the observation streams required at test time from those that are used to train the agent. The sensors available in a deployed robot are often decided by practical considerations such as cost, robustness, size, compute requirements, and ease of instrumentation, e.g., autonomous cars with only cheap and robust RGB camera sensors. However, those considerations might carry less weight at training time, so a robot learning practitioner may choose to scaffold policy learning with privileged information (Vapnik & Vashist, 2009) from extra sensors available only at training. In the case of the cars above, the manufacturer might equip a small fleet of training cars with expensive privileged sensors like lidar to improve RGB-only driving policies for customers to install in their cars. What learning mechanisms might permit artificial agents to improve by exploiting such privileged, training-time sensors? Considering reinforcement learning (RL) algorithms, we observe that while their primary output is usually a policy, they often employ an elaborate training apparatus with value functions, representation learning objectives, reward estimators, world models, and data collection policies. While the output policy must only access pre-determined target sensors, our key insight is that this training apparatus offers natural routes for privileged observation streams to influence policy learning. 1 arXiv:2405.14853v1  [cs.LG]  23 May 2024Published as a conference paper at ICLR 2024 Figure 1: Learning a policy to operate from partial observations can be aided by access to privileged sensors exclusively during training. Scaffolder improves world models, critics, exploration, and representation learning objectives to synthesize improved target policies. This insight directly motivates Scaffolder, a novel model-based reinforcement learning (MBRL) ap- proach that “scaffolds” each training component of RL by providing it access to privileged sensory information. Figure 1 shows a schematic. MBRL algorithms learn an environment simulator, or world model, from experience, and then train policies on synthetic experiences collected within this simulator, potentially mediated by value functions and reward estimators. In Scaffolder, rather than training a low-fidelity world model on the impoverished target observations, we train a “scaffolded world model” with privileged observations that more accurately models environment dynamics. This enables more realistic training experience synthesis, and better credit assignment using scaffolded value functions and reward estimators. Further, exploratory data collection, both in the real environ- ment and within the world model, can now be better performed by employing a scaffolded explo- ration policy. Finally, the scaffolded observations also enable learning improved representations of the test-time target sensor observations. Our key contributions are: (1) We study policy learning with privileged information in a novel and practically well-motivated “sensory scaffolding” setting, where extra sensors are available to an agent such as a robot at training time. (2) We propose Scaffolder, a MBRL method that exten- sively utilizes privileged observations to scaffold each auxiliary component of RL. (3) We validate Scaffolder extensively against prior state of the art on a new Sensory Scaffolding Suite (S3). S3 contains ten diverse environments and sensor setups, including privileged active visual perception for occluded manipulation policies, privileged touch and pose sensors for dexterous manipulation policies, and privileged audio and sheet music for training “blind and deaf” piano-playing robots. See Figure 2. (4) Through detailed analyses and ablation studies, we study the relative impacts of scaffolded learning mechanisms through which privileged sensing impacts policy learning. We find all components are important and that each component’s contribution depends on task-specific prop- erties. Finally, we show empirically that Scaffolder improves the agent’s estimates of the true RL objective function, thus providing improved learning signals to drive policy improvement. 2 P ROBLEM SETUP, NOTATION , & P RIOR WORK The Sensory Scaffolding Problem Setting:Consider the setup in Figure 1: a robot policy must use only target observations o− t = { proprioception, touch } to pick up a block, whose pose is unknown. It is common to model such problems as partially observable Markov decision process (POMDP) (Kaelbling et al., 1998) and train the policy π−(at | o− t ) to select appropriate robot actions at ∈ A. However, note in Figure 1 that the training process has access to additional privileged camera observations op t : thus, the policy π−(at | o− t ) effectively trains in a different, “scaffolded” POMDP with observations o+ t = \u0002 o− t , op t \u0003 . These scaffolded observations o+ t may still only be partial in that they don’t reveal the full environment state. Indeed they often are partial in our evaluation settings, but they nevertheless contain strictly more information about the environment state st ∈ Sthan the 2Published as a conference paper at ICLR 2024 Figure 2: Sensory Scaffolding Suite (S3). We visualize four out of our ten diverse tasks, each exploring different restricted sensing scenarios such as proprioceptive-only inputs, noisy sensors, images, and occluded or moving viewpoints. We evaluate the enhancement of policy training using privileged sensors like multiple cameras, controllable cameras, object pose, and touch sensors. Refer to Appendix E.4 for details on all environments. target observations o− t . In the picking task, even though the policy will operate blind at test time, the learning procedure can use noisy knowledge of the block pose, as revealed through the privileged camera observations. Prior Work: We now review several prior lines of work on reinforcement learning (RL) with privileged information which may apply to this sensory scaffolding problem. See Appendix A for extended discussion. Perhaps the most straightforward vehicle for privileged information in RL is reward. Analogous to labels in supervised learning, RL rewards specify the task, are only present at training time, and may therefore exploit privileged sensors. For example, Schenck & Fox (2017) instrument privileged thermal cameras to gauge fluid levels to train an image-based pouring policy. In another interesting example, Huang et al. (2022a) exploit privilegedinteractive sensing behaviors for reward estimation, such as pulling at a door to evaluate whether a door locking task is prop- erly completed. Further, nearly all sensorimotor RL in simulation uses privileged low-dimensional simulator state o+ t = st to inform task rewards and thus may be seen as implementing sensory scaf- folding in a limited way. More pertinent to us are methods that exploit privileged observations in other training-specific components of RL beyond just the reward. We discuss these below. • Privileged Critics:Actor-critic methods commonly condition the critic on privileged information (i.e. v(o+)), usually low-dimensional simulator state, while training the actor policy π−(a | o−) on target observations like high dimensional images (Pinto et al., 2018; Andrychowicz et al., 2018). These methods assume that target observations contain full information about the state, and struggle when this assumption is violated (Baisero & Amato, 2021). • Privileged Policies: These methods train a privileged teacher π+ to guide the student target policy π−. A common failure mode here is the “imitation gap”(Weihs et al., 2021; Swamy et al., 2022): the student cannot recover the teacher’s actions given impoverished inputs o−. This is typically mitigated by incorporating an additional reinforcement loss into the student’s learning objective (Rajeswaran et al., 2017; Weihs et al., 2021; Nguyen et al., 2022; Shenfeld et al., 2023). Aside from direct imitation (Chen et al., 2020), privileged teachers can also benefit the student’s exploration by sharing data (Schwab et al., 2019; Shenfeld et al., 2023; Kamienny et al., 2020; Weigand et al., 2021) or defining auxiliary rewards for the student (Walsman et al., 2022). • Privileged World Models: Most privileged sensing strategies employ model-free RL methods, with few exploring enhancements to model-based RL using privileged sensors. Seo et al. (2023) improves DreamerV2 (Hafner et al., 2022) by training the single-view policy representation on multi-view data. Recently, Informed Dreamer (Lambrechts et al., 2023) improve DreamerV3’s (Hafner et al., 2023) representation and world modelling via privileged information prediction. • Privileged Representation Learning Objectives:Privileged observations are commonly used to train representations for high-dimensional, image-based tasks, for example, by leveraging priv- ileged sensors like additional views (Sermanet et al., 2018; Seo et al., 2023) or segmentations (Salter et al., 2021). Several recent works have leveraged privileged simulator state for sim2real applications (Lee et al., 2020; Kumar et al., 2021; Qi et al., 2023), training a policy conditioned on 3Published as a conference paper at ICLR 2024 target observations and predicted simulator states. These methods have demonstrated impressive results in quadruped locomotion and dexterous manipulation. Summarizing, nearly all prior works focus on one route for privileged observations to influence tar- get policy learning, and many suffer from assumptions about the privileged observation op or target observation o− that do not hold in all settings. Scaffolder makes no such assumptions and exploits the scaffolded observations o+ = [op, o−] through multiple routes integrated into one cohesive RL algorithm. We also compare our method empirically against representative methods from each cat- egory of prior work, demonstrating significant and consistent gains on a large suite of tasks. 3 Scaffolder: I MPROVING POLICY TRAINING WITH PRIVILEGED SENSORS Figure 3: Scaffolder uses scaffolded observations to improve all components of training: world modelling, credit assignment, exploration, and policy representation. We now describe Scaffolder, a model-based reinforcement learning (MBRL) approach that uses scaffolded observations to comprehensively improve training mechanisms. MBRL algorithms learn an environment simulator, or world model (WM), from data and then train policies inside the simula- tor, facilitated by learned reward estimators and value functions. In the sensory scaffolding setting, we suggest a conceptually simple improvement: train the WM on scaffolded observations o+ in- stead of impoverished target observations o−. The scaffolded WM is then used to improve target policy training, credit assignment, and exploration. We also improve the target policy representation learning objective with o+. See Figure 3 for a visualization. Scaffolded World Model. We build on DreamerV3 (Hafner et al., 2023), a MBRL method well- known for its generality (see Appendix C.1 for DreamerV3 details). DreamerV3’s WM is imple- mented as a Recurrent State-Space Model (Hafner et al., 2020). The world model serves two pur- poses: it acts as an environment simulator to train the policy and serves as the policy’s recurrent state encoder for aggregating observation history. Scaffolder learns an additional “scaffolded” WM trained on o+ to replace the target WM (trained on o−) for policy training. We still retain and train the original target world model so we can encode states for target policy execution. We define both world models below. Target Scaffolded Target Scaffolded Dynamics: p− ϕ(z− t | z− t−1, at−1) p+ ϕ(z+ t | z+ t−1, at−1) Reward: p− ϕ(rt | z− t ) p+ ϕ(rt | z+ t ) Posterior: q− ϕ (z− t | z− t−1, at−1, o− t ) q+ ϕ (z+ t | z+ t−1, at−1, o+ t ) Decoder: p− ϕ(o− t | z− t ) p+ ϕ(o+ t | z+ t ) Continue: p− ϕ(ct | z− t ) p+ ϕ(ct | z+ t ) Transdecoder: p+ ϕ(o− t | z+ t ) Each WM operates over a latent representation z which is trained to be predictive of future observa- tions and rewards (see Appendix C for full equations). The dynamics predicts the current latent state given only history, while the recurrent posterior network infers the current state given history and the current observation. The decoder, reward, and continue heads reconstruct the observations, rewards, and termination signals from the trajectory data. The “transdecoder” maps scaffolded latent states to target observations; this component is motivated and described in detail under “Nested Latent Imagination” below. Finally, note that in practice, Scaffolder, just like DreamerV3, first embeds all observations into low-dimensional embeddingse−(o−) and e+(o+); to reduce notational clutter, we simply refer to these as o− and o+. 4Published as a conference paper at ICLR 2024 Scaffolded Critic and Reward. DreamerV3, without privileged sensing, optimizes the target policy π− θ (a | z−) by maximizing the policy’s estimated return, which is computed by evaluating the learned reward and critic networks over synthetic trajectories generated by the world model. Scaffolder improves the return estimate by using more accurate synthetic experience from the scaf- folded WM, and better reward and value estimates from the scaffolded rewardp+ ϕ (r | z+) and critic v+ ψ (z−, z+). Note that the critic requires both z−, z+ to remain unbiased, see Appendix C.2 for details. To evaluate the policy, the scaffolded critic and reward networks require trajectories with scaffolded latent states. We describe how to acquire such trajectories in imagination. Figure 4: Nested Latent Imagination. ▶ Nested Latent Imagination (NLI): NLI is a pro- cedure to roll out π− θ inside the scaffolded world model to generate synthetic training data. See Figure 4 for a visualization. To generate imagi- nary rollouts, vanilla DreamerV3 uses the previous timestep’s predicted latent zt ∼ p(zt | zt−1, at−1) as the input to the policy π(zt) for the current timestep. However, in Scaffolder, the scaffolded dy- namics produces scaffolded latents z+ t , whereas the target policy ingests target latentsz− t . To resolve this incompatibility, Scaffolder translates z+ into its cor- responding z− in two steps. First, a “transdecoder” p+ ϕ (ˆo− | z+) decodes z+ t into the corresponding target observation ˆo− t . Then ˆo− t is integrated by the recurrent target posterior network q− ϕ into the desired latent state z− t . This completes one synthetic environment step, When repeated H times, it generates a trajectory τ = \u0002 z+ 1 , z− 1 , a1, . . . z+ H, z− H \u0003 . ▶ Computing the TD(λ) return: Now, we show how to improve target policy training with scaf- folded observations and components. All RL methods aim to maximize the true discounted policy return. Policy updates in Dreamer follow the gradient of the TD( λ) return, an estimate of the true return that is mediated by the world model’s latent state, reward predictions and value function. We modify DreamerV3’s policy objective to use scaffolded dynamics, scaffolded reward estimator, and scaffolded critic instead of the impoverished target components. We highlight the terms that depend on the scaffolded components in blue. The target policy objective is defined as J (θ) = TX t=1 Eπ− θ ,p+ ϕ h Rλ t + ηH \u0002 π− θ (at | z− t ) \u0003i (1) where the first term maximizes the policy’s TD(λ) return, the second term is an entropy regularizer, and the expectation is over the imagined trajectories generated using the above NLI procedure. The TD(λ) return is the weighted average of n-step returns, defined as: Rλ t .= rt + γct \u0000 (1 − λ)v+ ψ (z− t+1, z+ t+1) +λRλ t+1 \u0001 Rλ T .= v+ ψ (z− T , z+ T ) (2) This modified objective now benefits from the scaffolded components in the following ways. First, the imagined trajectories are generated with the scaffolded dynamics, which can better approximate the true dynamics compared to the target dynamics. Next, the reward and value estimates can now use z+, which contains potentially relevant information not available in z− to improve credit as- signment. This all results in a more accurate TD( λ) return estimate of the true return, which in turn better optimizes the policy. Thus, scaffolding the world model directly improves the RL training signals provided to the target policy π− θ at each update. While only the WM and critic are directly involved in the policy update above, they are trained on exploration data, and the policy itself operates on a representation z−. These can both be improved by using privileged observations, as we discuss below. Scaffolded Exploration Policy.We train a scaffolded task policyπ+ θ (at | z+ t ) inside the scaffolded world model. While π+ θ cannot directly run at test time since privileged observations would be missing, it can generate exploratory data to train π− θ . The intuition is that the scaffolded π+ θ learner becomes more performant more quickly, so it explores task-relevant states beyond the current reach of π− θ . We use a 1:1 ratio of π+ θ and π− θ rollouts for exploration. We choose not to enforce any imitation objective between π+ and π− to avoid the “imitation” gap (Section 2). 5Published as a conference paper at ICLR 2024 Scaffolded Representation Learning Objective.Finally, following prior works (Pinto et al., 2018; Lambrechts et al., 2023) that have found it useful to regress privileged information for representation learning, we train an auxiliary decoder from z− to the scaffolded observation o+, to improve z−. We expect this to be helpful whenop is recoverable fromo−, i.e. inferring object poses from images. 4 E XPERIMENTS We aim to answer the following questions. (1) Does Scaffolder successfully exploit privileged sens- ing to improve target policy performance? (2) How does Scaffolder compare to prior work on RL with privileged information? (3) What are the most critical components through which privileged observations influence target policy learning, and how do task properties affect this? 4.1 T HE SENSORY SCAFFOLDING SUITE (S3) OF TASKS We propose Sensory Scaffolding Suite (S3), a suite of 10 robotics-based tasks to evaluateScaffolder, baselines, and future approaches in the sensory scaffolding problem setting. See Figure 5. As mo- tivated in Section 1, robotics is a well-suited domain for studying sensory scaffolding: practical considerations such as cost, robustness, size, compute requirements, and ease of instrumentation of- ten incentivize operating with limited sensing on deployed robots. In addition to standard definitions for RL environments, S3 pre-defines privileged and target sensors. S3 tasks are more general and difficult than prior sensory scaffolding tasks in a variety of ways. S3 tasks have continuous, observa- tion and action spaces with complex dynamics. Furthermore, the privileged sensors are potentially high-dimensional and noisy observations, rather than ground truth, low-dimensional simulator state. S3 defines performance scores for each task: success rate for pen and cube rotation, number of branches swung for Noisy Monkey, and returns computed from dense rewards for the other tasks. Environment/Task Target Sensors Priv. Sensors Properties Blind Pick Proprio, Touch 2 Fixed Cams, Wrist Cam Random object position Blind Locomotion Proprio Cam Random hurdles Blind Deaf Piano Proprio Future Notes, Piano State Deterministic Blind Numb Pen Proprio, Init/Goal Pose Object Pose, Touch Random init / goal pose Blind Numb Cube Proprio, Init/Goal Pose Object Pose, Touch Random init / goal pose Noisy Monkey Noisy proprio, Noisy branch pos. True Proprio, Branch Pos. Random sensor noise Wrist Pick-place Proprio, Touch, Wrist Cam 2 Fixed Cams Random object / bin position Occluded Pick-place Proprio, Touch, Occluded Cam Wrist Cam Random object / bin position Visual Pen Proprio, Init/Goal Pose, Cam Object Pose, Touch Random init / goal pose Visual Cube Proprio, Init/Goal Pose, Cam Object Pose, Touch Random init / goal pose Figure 5: The Sensory Scaffolding Suite (S3) of tasks. See Appendix E.4 for details. We provide a high-level overview here; for more details, see Appendix E.4. Figure 5 provides key information about each task. Tasks 1-5in S3 focus on taking away privileged vision, audio, and touch to train blind, deaf, and numb robot policies that operate mainly from proprioception at test time. The robots span manipulators, legged robots, and dexterous hands, and the tasks include object picking, hurdling, pen rotation, and piano playing. Tasks 6-10provide more target sensing to policies, particularly vision (either as raw RGB images, or as object recognition system outputs). Privileged senses are even more informative, such as multi- camera images, and “active” cameras that move with the robot’s wrist. Equipped with the target sensors, monkey robots must swing between tree branches, dextrous hands must rotate objects, and 2-fingered robot arms must put objects away amidst occlusion on cluttered tables. Baselines. In Section 2, we classified privileged RL methodologies based on the component they enhance with privileged information, such as the critic, policy, world model, and representation learning objective; we select representative baselines from each category to compare to Scaffolder. Wherever possible, we implement baselines over DreamerV3 (Hafner et al., 2023), the base MBRL agent for our method. We overview the baselines below, see Appendix E.3 for more details. 6Published as a conference paper at ICLR 2024 • Unprivileged: DreamerV3. We train DreamerV3 only on target observations. DreamerV3 is known for its consistency across benchmarks, thereby serving as a strong unprivileged baseline. • Critic: AAC.For a privileged critic baseline, we use Asymmetric Actor Critic (AAC) (Pinto et al., 2018). AAC is model-free; we find that it is much more sample-inefficient, so we train it for 100M steps in each environment (20 − 400× more than Scaffolder). • Teacher Policy: DreamerV3+BC.Here, we follow a common strategy from prior works (Weihs et al., 2021; Nguyen et al., 2022; Shenfeld et al., 2023) to train a privileged teacher policy and incorporate a teacher-imitating BC loss alongside RL rewards when training the target policy. We implement this in DreamerV3 and call it DreamerV3+BC. The weights for BC and RL objectives are selected through hyperparameter search for each environment. • World Model: Informed Dreamer. Lambrechts et al. (2023) extends DreamerV3 to exploit privileged observations by incorporating a new objective term to decode privileged observations o+ t from target Dreamer state z− t . • Representation: RMA+. Kumar et al. (2021) train policies with PPO using a privileged state representation analogous to z+ and regress from target observations o− to z+. This is liable to fail when z+ may contain information that is not in o−, but RMA has achieved impressive results for many robotics applications. To facilitate fast RMA training, we found it necessary to provide ground truth simulator state as privileged information; we call this RMA+. Like AAC, RMA+ is model-free, so we run it for 100M steps to permit it to learn meaningful policies. • Exploration Policy: Guided Obs.Finally, Guided Observability (Weigand et al., 2021) collects better exploration data by dropping out the privileged information from policy inputs over time. 4.2 R ESULTS Figure 6: Scaffolder performs better than baselines across all tasks in learning speed and final per- formance, showing its generality. For each method, we report median score and standard error, and each method is run with 4-10 seeds aside from AAC (100M) and RMA+ (100M). We evaluate the training performance and final task scores of each method. The normalized final median scores in the top left of Figure 6 are computed using the performance of the unprivileged DreamerV3 baseline as a lower bound (0.0) and the performance of a privileged DreamerV3 model that is trained and evaluated on o+ as the upper bound (1.0). See Appendix E for more info. 7Published as a conference paper at ICLR 2024 Figure 6 demonstrates that Scaffolder achieves the highest aggregate median performance across all ten tasks. Scaffolder bridges 79% of the gap between o− and o+, just by having temporary access to o+ at training time. In other words, much of the gap between the observations o− and o+ might lie not in whether they support the same behaviors, but in whether they support learning them. A closer look at the learning curves in Figure 6 reveals that Scaffolder learns more quickly in 8 out of 10 tasks. Even with its limited environment sample budget (between 250K to 5M), it outperforms or is competitive with AAC and RMA trained with 100M samples in 9 out of 10 tasks. Scaffolder successfully exploits privileged sensors to learn performant policies despite severely lim- ited target sensors - it plays an imperfect, but recognizable rendition of “Twinkle Twinkle Little Star”, deftly rotates cubes to goal orientations without any object pose information, and actively moves a robot wrist camera to look for objects outside the field of view. See website for video examples of learned behaviors. Base DreamerV3 uniformly performs much worse. The privileged baselines all exhibit a high degree of performance variance. They may excel in certain tasks yet are oftentimes worse than the non-privileged DreamerV3 baseline. Recall from Section 2 that these approaches focus on one route for privileged sensors to influence policy learning, and many make assumptions about the nature of privileged and target observations. This may be ill- suited to performing well on the diverse Sensory Scaffolding Suite. DreamerV3 operating on just the target sensors without any privileged info proves to be a surprisingly strong performer, even outperforming some prior approaches that access privileged information like AAC. For example, RMA excels in Noisy Monkey, where regressing the privileged true stateop from noisy state o− is relatively easy, yet completely fails in Blind Pick. Indeed, we find that in environments like Blind Pick or Blind Locomotion with large information gaps between target and privileged ob- servations, RMA, DreamerV3+BC, and Informed Dreamer tend to suffer. RMA fails because the target observations o− are not predictive of the privileged observations op (e.g. proprioception is not usually predictive of object states). Informed Dreamer similarly fails because it also enforces the target world model to predict privileged observations from target observations. Finally, Dream- erV3+BC fails in such cases due to the large differences in optimal behavior between a privileged teacher and a blind student—the privileged teacher can directly pick up the object with vision, while the student policy needs to learn information gathering behavior to pick up the block. Figure 7: Scaffolder discovers spiral search and robust hurdling for blind policies. The project website showcases several interesting behaviors learned by Scaffolder and baselines. Scaffolder behaviors broadly fall into two categories depending on the task. Sometimes, it per- forms information-gathering strategies - in Blind Picking,Scaffolder performs spiral search over the workspace to find the block from touch alone. At other times, it acquires robust behaviors invariant to unobservables - in Blind Locomotion, Scaffolder discovers robust run-and-jump maneuvers to minimize collisions with unseen randomized hurdles. See Figure 7 for visualizations. 4.3 A BLATIONS AND ANALYSIS OF Scaffolder As evidenced above, Scaffolder works well, but why? We first replace each privileged component with a non-privileged counterpart to assess component-wise contributions. “No Scaff. WM” opts for training the policy in the target world model over training in the privileged world model. “No Scaff. Critic” uses an unprivileged critic v− ψ (s−) in Rλ t for policy learning. “No Scaff. Explore” collects data with only the target policy. For “No Scaff. Repr.”, the representation is trained to reconstruct target observations instead of privileged observations. Figure 8 compares these ablations on 4 tasks. Overall, all ablations perform poorly relative to Scaf- folder, indicating the importance of each component. Interestingly, different scaffolded components 8Published as a conference paper at ICLR 2024 Figure 8: We ablate Scaffolder by replacing components with their non-privileged counterparts. All components are important, and the combined method performs best. prove crucial for different tasks: exploration for Blind Pick, critic for Blind Cube Rotation, world model for Blind Locomotion, and representation for RGB Cube Rotation. These task-specific trends may be due to the task diversity in S3. In Blind Pick, a blind policy struggles to locate the block, but the vision-based exploration policy can easily locate the block and populate the buffer with useful data. In Blind Cube Rotation, the privileged critic, with access to ob- ject pose, can more easily estimate the object-centric reward. Blind Locomotion involves navigating unexpected hurdles, therefore a scaffolded world model, with access to vision, can accurately pre- dict future hurdles. Lastly in Visual Cube, the target policy must encode high-dimensional images, and the scaffolded representation objective encourages encoding of essential object pose and touch information. In all these cases, the combinedScaffolder method benefits from cohesively integrating these many routes for privileged sensing to influence policy learning, and performs best. Figure 9: Comparing TD(λ) estimates. Scaffolder Improves RL Training Signals.We notice in Figure 8 that dropping the scaffolded WM and its corresponding scaffolded value sig- nificantly impact learning. Recall that these components are critical to estimate the policy return (Equation (1)), whose gradients directly deter- mine policy updates. We now examine these policy return estimates. Let the scaffolded TD( λ) return be Equation (1), and the impover- ished “target” TD(λ) return be the original DreamerV3 policy objective, which substitutes the blue terms in Equation (1) with target dynamics p− ϕ (z− t+1 | z− t , at), target rewardsp− ϕ (r− t | z− t ), and target criticv− ψ (z− t ). We compute the mean absolute error (MAE) for each of these two estimates against the ground truth Monte-Carlo return Rt. In Blind Pick, every 25K steps while training, we collect ∼1000 transitions with the policy and compute Rt. Figure 9 plots Improvement = (target return MAE − scaffolded return MAE). The improvements are mostly positive, indicating that scaffolded TD( λ) is more accurate. This provides evidence for why Scaffolder trains better policies than non-privileged counterparts — its policy objective better approximates the true expected return, giving the policy optimizer a better training signal. We find similar trends in other environments in Appendix F. 5 C ONCLUSION We have studied how additional observation streams at training time can aid skill learning, intro- ducing a diverse S3 task suite to aid this study. Scaffolder successfully exploits privileged sensors across these tasks to improve reinforcement learning signals and train better policies. While these results are promising, there is much room for future work. See Appendix B for extended limitations and future work. Empirically, our task suite focuses on simulated robotic tasks, but other domains like real-world robotics and video games, each with their own forms of privileged sensing, warrant study. For real robots, it may be practical toestimate rewards through their sensors (Schenck & Fox, 2017; Fu et al., 2018; Haldar et al., 2023) rather than assume black-box rewards from supervising humans. Here, we expect that improved reward estimates through privileged sensors would offer further advantages (see Appendix G). Our work also opens a window to intriguing deeper questions about the relationship between sensing and learning that it only begins to address: what environment information must a learning agent sense in order to perform a task, and how does this vary with tasks and learning phases? We believe that our findings will prove useful for such future research. 9Published as a conference paper at ICLR 2024 6 A CKNOWLEDGEMENTS This work was supported by the NSF CAREER Award 2239301 and ONR award N00014-22-1- 2677. The authors would like to thank the anonymous reviewers for their constructive feedback and members of the GRASP lab and PAL group for their support. 7 R EPRODUCIBILITY STATEMENT To ensure reproducibility, we will release all code aboutScaffolder, baselines, and Sensory Scaffold- ing Suite on the project website: https://penn-pal-lab.github.io/scaffolder/. Our supplementary has a comprehensive overview of our method in Appendix C, experimentation protocol / resource usage / hyperparameters in Appendix E, baseline details in Appendix E.3, and environmental descriptions in Appendix E.4. REFERENCES Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018. Andrea Baisero and Christopher Amato. Unbiased asymmetric reinforcement learning under partial observability. arXiv preprint arXiv:2105.11674, 2021. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv preprint arXiv:1810.05687, 2018. Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl. Learning by cheating. In Con- ference on Robot Learning, pp. 66–75. PMLR, 2020. Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen Liu. Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. arXiv preprint arXiv:2309.00987, 2023. Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework for data-driven reward definition. Advances in neural information processing systems, 31, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=S1lOTC4tDS. Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from pixels, 2022. URL https://arxiv.org/abs/2206.04114. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Supercharg- ing imitation with regularized optimal transport. In Conference on Robot Learning , pp. 32–43. PMLR, 2023. Edward S. Hu, Kun Huang, Oleh Rybkin, and Dinesh Jayaraman. Know thyself: Transferable visual control policies through robot-awareness. In International Conference on Learning Representa- tions, 2022. URL https://openreview.net/forum?id=o0ehFykKVtr. Kun Huang, Edward S. Hu, and Dinesh Jayaraman. Training robots to evaluate robots: Example- based interactive reward functions for policy learning. In6th Annual Conference on Robot Learn- ing, 2022a. URL https://openreview.net/forum?id=sK2aWU7X9b8. 10Published as a conference paper at ICLR 2024 Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo ˜ao G.M. Ara ´ujo. Cleanrl: High-quality single-file implementations of deep rein- forcement learning algorithms. Journal of Machine Learning Research , 23(274):1–18, 2022b. URL http://jmlr.org/papers/v23/21-1342.html. Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998. Pierre-Alexandre Kamienny, Kai Arulkumaran, Feryal Behbahani, Wendelin Boehmer, and Shi- mon Whiteson. Privileged information dropout in reinforcement learning. arXiv preprint arXiv:2005.09220, 2020. Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. RMA: Rapid Motor Adaptation for Legged Robots. In Robotics: Science and Systems (RSS) , 2021. doi: 10.15607/RSS.2021.XVII. 011. Gaspard Lambrechts, Adrien Bolland, and Damien Ernst. Informed POMDP: Leveraging Additional Information in Model-Based RL. arXiv preprint arXiv:2306.11488, 2023. Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning Quadrupedal Locomotion over Challenging Terrain. Science Robotics , 2020. doi: 10.1126/ scirobotics.abc5986. Hai Nguyen, Andrea Baisero, Dian Wang, Christopher Amato, and Robert Platt. Leveraging fully observable policies for learning under partial observability. In CoRL, 2022. Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example- guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4):1–14, 2018. Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asym- metric actor critic for image-based robot learning. In Robotics: Science and Systems (RSS), 2018. Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-hand object rotation via rapid motor adaptation. In Conference on Robot Learning, pp. 1722–1732. PMLR, 2023. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017. Sasha Salter, Dushyant Rao, Markus Wulfmeier, Raia Hadsell, and Ingmar Posner. Attention- privileged reinforcement learning. In Conference on Robot Learning, pp. 394–408. PMLR, 2021. Connor Schenck and Dieter Fox. Visual closed-loop control for pouring liquids. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 2629–2636. IEEE, 2017. Devin Schwab, Tobias Springenberg, Murilo F Martins, Thomas Lampe, Michael Neunert, Abbas Abdolmaleki, Tim Hertweck, Roland Hafner, Francesco Nori, and Martin Riedmiller. Simultane- ously learning vision and feature-based control policies for real-world ball-in-a-cup. In Robotics: Science and Systems (RSS), 2019. Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view masked world models for visual robotic manipulation. arXiv preprint arXiv:2302.02408, 2023. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. In IEEE International Conference on Robotics and Automation, pp. 1134–1141, 2018. Idan Shenfeld, Zhang-Wei Hong, Aviv Tamar, and Pulkit Agrawal. Tgrl: An algorithm for teacher guided reinforcement learning. In International Conference on Machine Learning, 2023. Yanchao Sun, Ruijie Zheng, Xiyao Wang, Andrew E Cohen, and Furong Huang. Transfer RL across observation feature spaces via model-based regularization. In International Confer- ence on Learning Representations , 2022. URL https://openreview.net/forum?id= 7KdAoOsI81C. 11Published as a conference paper at ICLR 2024 Gokul Swamy, Sanjiban Choudhury, J Bagnell, and Steven Z Wu. Sequence model imitation learn- ing with unobserved contexts. Advances in Neural Information Processing Systems , 35:17665– 17676, 2022. Gyan Tatiya, Jonathan Francis, and Jivko Sinapov. Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies. In 2023 IEEE International Confer- ence on Robotics and Automation (ICRA), pp. 11315–11321. IEEE, 2023. Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for tem- poral difference learning. Journal of Machine Learning Research, 8(9), 2007. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do- main randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 23–30. IEEE, 2017. Vladimir Vapnik and Akshay Vashist. A new learning paradigm: Learning using privileged infor- mation. Neural networks, 22(5-6):544–557, 2009. Lev Vygotsky et al. Interaction between learning and development. Link ¨opings universitet, 2011. Aaron Walsman, Muru Zhang, Sanjiban Choudhury, Dieter Fox, and Ali Farhadi. Impossibly good experts and how to follow them. In The Eleventh International Conference on Learning Repre- sentations, 2022. Stephan Weigand, Pascal Klink, Jan Peters, and Joni Pajarinen. Reinforcement learning using guided observability. arXiv preprint arXiv:2104.10986, 2021. Luca Weihs, Unnat Jain, Iou-Jen Liu, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kembhavi, and Alexander Schwing. Bridging the imitation gap by adaptive insubordination. In NeurIPS, 2021. David Wood, Jerome S Bruner, and Gail Ross. The role of tutoring in problem solving. Journal of child psychology and psychiatry, 17(2):89–100, 1976. Grace Zhang, Linghan Zhong, Youngwoon Lee, and Joseph J Lim. Policy transfer across visual and dynamics domain gaps via iterative grounding. arXiv preprint arXiv:2107.00339, 2021. 12Published as a conference paper at ICLR 2024 A E XTENDED RELATED WORK For the sake of brevity, we gave a broad high level overview of related works on sensory scaffolding. We now give a more extensive overview. First, the sensory scaffolding problem is related to the areas of domain adaptation, domain transfer, and sim-to-real transfer. Such works attempt to transfer an agent trained in a source MDP to a target MDP, differing in observation space, action space, dynamics, rewards, or initial state distribution (i.e. resets) (Taylor et al., 2007; Tobin et al., 2017; Chebotar et al., 2018; Hu et al., 2022; Zhang et al., 2021; Peng et al., 2018; Chen et al., 2023). Sensory scaffolding can be thought of as a special case of observation space transfer, where the source observation space is a superset ( {o−, op}) of the target observation space in a POMDP setting. In contrast, prior work (Sun et al., 2022; Tatiya et al., 2023) focusing on observation transfer typically assume a mapping between the observation spaces to facilitate transfer, i.e. the target image observation space can recover the source simulator state (Pinto et al., 2018), and are formulated with MDPs in mind. Next, the field of privileged imitation learning is related, but not identical, to sensory scaffolding (i.e. privileged reinforcement learning). In the privileged IL setup, one assumes access to a pre-existing expert policy with privileged access to information, and the objective for the partially observed student policy is to imitate the expert policy (Swamy et al., 2022; Weihs et al., 2021). In sensory scaffolding, we study how privileged sensory streams at training time improve RL policy search for a target policy with diminished senses. We do not assume access to any existing target policy, and the target policy’s objective is to maximize reward, not imitate a privileged policy. B E XTENDED LIMITATIONS AND FUTURE WORK Scaffolder trains an additional scaffolded world model and exploration policy on top of the original world model / policy, which adds a sizable amount of compute and resource burden. Training a single dynamics model and policy that is shared across both o+ and o− would reduce this burden, and we believe using ideas from the multi-task learning literature to be a good first step. Through our ablations and analysis, we have a good empirical understanding of howScaffolder ben- efits from privileged observations. On the theoretical side, there is work characterizing privileged imitation learning (Swamy et al., 2022), critics (Baisero & Amato, 2021), world models (Lam- brechts et al., 2023) and representation learning (Vapnik & Vashist, 2009), but a concrete theoretical understanding of how privileged information affects the entire RL process and components, is is lacking. On the practical side, while Scaffolder uses privileged observations to improve multiple training- time only RL mechanisms, future work can investigate better ways of leveraging privileged obser- vations. For example, the use of o+ as a reconstruction target for representation learning can likely be replaced with a better privileged representation learning objective. Next, using additional sensors at training time brings in practical problems. More sensors usu- ally results in increased computation and bandwidth requirements. Furthermore, many multimodal datasets are incomplete, i.e. certain modalities may be missing in subsets of the data. C M ETHOD DETAILS Here, we provide a more in-depth exposition of Scaffolder and DreamerV3. To start, we set up DreamerV3. In the main paper, we chose to depart from DreamerV3 notation and world model naming convention for two reasons. First, they use s for learned world model state, while in our POMDP settings, s refers to the true environment state. To avoid confusion with true environment state, we chose to use z instead of s for world model state. Next, they split up world model state s into two components, h, zto more accurately write out the exact computation graph. In the main paper, we defined world model state as one component, z to be more conceptually ergonomic. Now we default back to the Dreamer notation, and will define Scaffolder in terms of DreamerV3 notation to make comparison easier. 13Published as a conference paper at ICLR 2024 C.1 D REAMER V3 In DreamerV3, the recurrent encoder maps observations xt to stochastic representations zt. Then, the sequence model, a recurrent network with recurrent state ht, predicts the sequence of represen- tations given past at−1. The world model state is the concatenation of ht and zt. The world model state is then used to reconstruct observations, predict rewards rt, continuation flags ct. RSSM    Sequence model: ht = fϕ(ht−1, zt−1, at−1) Encoder: zt ∼ qϕ(zt|ht, xt) Dynamics predictor: ˆzt ∼ pϕ(ˆzt|ht) Reward predictor: ˆrt ∼ pϕ(ˆrt|ht, zt) Continue predictor: ˆct ∼ pϕ(ˆct|ht, zt) Decoder: ˆxt ∼ pϕ(ˆxt|ht, zt) (3) Given the sequence batch of inputsx1:T , actions a1:T , rewards r1:T , and continuation flags c1:T , the world model is trained to minimize prediction loss, dynamics loss, and the representation loss. L(ϕ) .= Eqϕ hPT t=1(βpredLpred(ϕ) +βdynLdyn(ϕ) +βrepLrep(ϕ)) i . (4) We refer interested readers to Hafner et al. (2023) for the particular world model loss definitions. We use the default hyperparameters from DreamerV3 for our method and baselines that use Dreamerv3. Next, DreamerV3 trains Actor-Critic neural networks in imagination. The actor and critic operate over world model states st = ht, zt, and are defined as: Actor: at ∼ πθ(at|st) Critic: vψ(st) ≈ Epϕ,πθ [Rt] (5) The critic is trained to estimate values on imaginary trajectories generated by executing the actor policy with the learned dynamics. The actor policy is trained to maximize the TD( λ) returns of the imagined trajectories, defined below. Rλ t .= rt + γct \u0010 (1 − λ)vψ(st+1) +λRλ t+1 \u0011 Rλ T .= vψ(sT ) (6) Now, with the DreamerV3 components set up, we are ready to define Scaffolder comprehensively, and using DreamerV3 notation. C.2 Scaffolder Scaffolder trains two world models, one on scaffolded observationsx+ = [x−, xp] and one on target observations x−. The scaffolded world model is defined as: RSSM    Sequence model: h+ t = fϕ(h+ t−1, z+ t−1, at−1) Encoder: z+ t ∼ qϕ(z+ t |h+ t , x+ t ) Dynamics predictor: ˆz+ t ∼ pϕ(ˆz+ t |h+ t ) Reward predictor: ˆrt ∼ pϕ(ˆrt|h+ t , z+ t ) Continue predictor: ˆct ∼ pϕ(ˆct|h+ t , z+ t ) Decoder: ˆx+ t ∼ pϕ(ˆx+ t |h+ t , z+ t ) (7) The target world model is defined as: RSSM    Sequence model: h− t = fϕ(h− t−1, z− t−1, at−1) Encoder: z− t ∼ qϕ(z− t |h− t , x− t ) Dynamics predictor: ˆz− t ∼ pϕ(ˆz− t |h− t ) Reward predictor: ˆrt ∼ pϕ(ˆrt|h− t , z− t ) Continue predictor: ˆct ∼ pϕ(ˆct|h− t , z− t ) Decoder: ˆx− t ∼ pϕ(ˆx− t |h− t , z− t ) (8) 14Published as a conference paper at ICLR 2024 The world models are trained as follows: We sample a trajectory containing observations x+ 1:T , actions a1:T , rewards r1:T , and continuation flags c1:T . We then train the scaffolded world model as normal and then convert x+ 1:T to x− 1:T by simply leaving out privileged observations xp 1:T from the data before feeding it to the target world model. Target Actor and Privileged CriticNext, we define the target actor and critic. The target actor operates over target world model state s− t = {h− t , z− t }. Crucially, the critic operates over both the scaffolded world model state s+ t = h+ t , z+ t and the target world model state s− t . Note s− t is still required because s− t contains info about the historical beliefs of the agent, which is not predictable from only the Markovian learned world model state s+ t . See (Baisero & Amato, 2021) for more information. The additional environmental information in s+ t potentially makes value estimation easier for the privileged critic. Furthermore, the critic is trained on trajectories generated by the scaffolded world model, which should better approximate true dynamics. We highlighted in blue the places where privileged components are used instead of their original counterparts. Target Actor: at ∼ π− θ (at|s− t ) Privileged Critic: vψ(s− t , s+ t ) ≈ Ep+ ϕ ,π− θ [Rt] (9) Now, the actor is trained with to maximize TD(λ) returns Rλ t of generated trajectories. We improve all components within this objective with scaffolded observations, and highlight them in blue. First, the policy objective is written as: L(θ) .= PT t=1 Eπ− θ ,p+ ϕ [Rλ t / max(1, S)] − ηH[π− θ (at|s− t )] (10) where we start by using the scaffolded world model to generate imaginary trajectories, using the Nested Latent Imagination procedure described in Section 3. We further incorporate scaffolded components into the TD( λ) return. Because we have access to s+ t from generating trajectories in the scaffolded world model, we can use the scaffolded reward, continue, and critic to compute TD(λ) return. Rλ t .= rt + γct \u0010 (1 − λ)vψ(s− t+1, s+ t+1) +λRλ t+1 \u0011 Rλ T .= vψ(s− T , s+ T ) (11) Additional Exploration Policy We train an additional exploration policy πe(at | s+ t ) that oper- ates over scaffolded world model state during training time, to collect informative trajectories. The intuition is that with better sensing, πe can solve the task more quickly and gather relevant data. To do so, we simply define separate actor critic networks for the exploration policy, that depend ons+. It is trained to maximize reward using standard imagination within the scaffolded world model. Exploration Actor: at ∼ πe θ(at|s+ t ) Exploration Critic: ve ψ(s+ t ) ≈ Ep+ ϕ ,πe θ [Rt] (12) We alternate between collecting episodes with the exploration policy and target policy in a 1:1 ratio. Target Policy Representation We modify the target decoder to pϕ(ˆx+ t | h− t , z− t ) so that it en- forces the representation to be more informative of the privileged observations, which is potentially useful for control. C.3 N ESTED LATENT IMAGINATION Here, we motivate and explain the nested latent imagination procedure more in detail. First, let’s start with a brief overview of DreamerV3’s latent imagination procedure. Recall that DreamerV3 uses the world model components (dynamics, posterior) for two roles: 1) as a environment simulator to generate synthetic experience for policy improvement, and 2) as a latent state encoder that aggregates observation history for the policy. 15Published as a conference paper at ICLR 2024 Figure 10: Left: DreamerV3’s Latent Imagination - note that it only uses impoverished, target dynamics, rewards, and values to generate trajectories for policy improvement. Right: Scaffolder uses scaffolded dynamics, rewards and values. Concretely, the dynamics p−(z− t+1 | z− t , at) is the environment simulator, and the posteriorp−(z− t | z− t−1, at−1, e− t ) encodes present e− t and history z− t−1, at−1 into a compact latent state z− t . Referring to the left side of Figure 10, DreamerV3’s latent imagination generates trajectories by repeating the following three steps. 1. Policy Inference:At timestep t, the latent state z− t is fed into the target policy π−(· |z− t ) to generate an action at. 2. Dynamics Inference:The current latent state and action(z− t , at) are fed into the dynamics p−(z− t+1 | z− t , at) to generate a future state z− t+1. 3. Credit Assignment: With the future state as input, rewards p−(rt+1 | z− t+1) and value estimates v(z− t+1) are computed. After running these 3 steps for H timesteps, a trajectory τ = \u0002 z− 1 , a1, z− 2 , . . . z− H \u0003 is created. The policy is trained to maximize the TD( λ) return of all the trajectories, which is a function of the dynamics, rewards, and values estimates. Consider what happens to this procedure if the target observations are extremely impoverished, i.e. have a high loss of information from the scaffolded observations. Then the target dynamics model p−(z− t+1 | z− t , a− t ), rewards p−(rt+1 | z− t+1) and value estimates v(z− t+1) will be inaccurate. Instead, we propose to train an additional scaffolded world model, that can take advantage of the scaffolded observations to learn a more accurate environmental simulator for generating trajectories, replacing the role of the target dynamics in DreamerV3. However, because we still need to run the target policy, we retain and train the target world model components and use the target posterior to encode latent states from target observations. Following the right side of Figure 10: 1. Policy Inference:At timestep t, we have the target latent statez− t and the scaffolded latent state z+ t . The target latent is fed into the target policy π−(· |z− t ) to generate an action at. 2. Scaffolded Dynamics Inference:The current scaffolded latent state and action (z+ t , at) are fed into the scaffolded dynamics p+(z+ t+1 | z+ t , at) to generate a future state z+ t+1. 3. Scaffolded Credit Assignment: With the scaffolded future state as input, rewards p+(rt+1 | z+ t+1) and value estimates v(z+ t+1, z− t+1) are computed. 4. Target State Inference:At this point, we have the future scaffolded state z+ t+1, but the policy requires z− t+1 to start step 1 for the next iteration. We can’t directly convert or learn a mapping z+ → z−, since they encode fundamentally different information (see Baisero & Amato (2021)). Instead, we can learn a mapping z+ → o−, since z+ → o+ via DreamerV3’s reconstruction objective, and o+ is a superset of o−. 16Published as a conference paper at ICLR 2024 By training the target embedding predictor p+(e− | z+) where e− is the low-level embed- ding of the target observation, we can now infer the target latent state z− t with the target posterior q(z− t+1 | z− t , at, e− t ). With z− t+1 from the target posterior, we now can start step 1 for the next iteration. After running these 4 steps for H timesteps, a trajectory τ = \u0002 z− 1 , z+ 1 , a1, z− 2 , z+ 2 . . . z− H, z+ H \u0003 is created. The policy is trained to maximize the TD(λ) return of all the trajectories, which is a function of the scaffolded dynamics, rewards, and values estimates. Later, we show that these scaffolded TD(λ) estimates using scaffolded components are more accurate than the non-privileged TD( λ) estimates, explaining why Scaffolder trains better policies than DreamerV3. D R UNTIME AND RESOURCE COMPARISON Our method builds off DreamerV3, so it is compute efficient, requiring only 1 GPU and 4 CPUs for each run. Similarly, baselines building off of DreamerV3 like Informed Dreamer, DreamerV3+BC, Guided Observability also are compute efficient. We train on Nvidia 2080ti, 3090, A10, A40, A6000, and L40 GPUs. In contrast, model-free methods like RMA and AAC require large batches for PPO / Actor-Critic training, requiring 128 CPU workers to have fast and stable training. Next, we give the runtime in hours for each method, for all experiments in Figure 11. Note that these are approximate, since we run methods over different types of GPUs and CPUs. In general, Scaffolder takes ∼ 30%-40% longer to run than DreamerV3 due to training an additional world model and exploration policy, but Scaffolder is much more sample efficient and reaches higher per- formance with less environment steps, making up for slower wall time. Similarly, DreamerV3+BC is also slower compared to DreamerV3 because it trains the teacher policy and teacher world model alongside the student policy and world model. In general, RMA and AAC take at least 1 day to train to reach 100M steps. We plot the learning curves over hours rather than environment steps in Figure 12. The trends are largely the same as the learning curves over environment steps in Figure 6. This is becauseScaffolder is much more sample-efficient than baselines, reaching higher performance earlier in walltime even if it takes more time per-step to train the additional models. Env. ScaffolderInformed Dreamer DreamerV3+BC Guided Obs. RMA+(100M) AAC(100M) DreamerV3Blind Pick 18 14 18 14 36 36 14Blind Locomotion 16 12 24 16 24 24 16Blind Deaf Piano 18 14 18 14 36 36 14Blind Pen 12 9 12 9 36 36 9Blind Cube 12 9 12 9 36 36 9Noisy Monkey 20 12 20 12 36 36 12Wrist Pick-place 22 16 22 16 36 36 16Occluded Pick-Place 9 5 9 5 36 36 5RGB Pen 3 2 3 2 36 36 2RGB Cube 3 2 3 2 36 36 2 Figure 11: Approximate runtime in hours for each method, over the tasks. E E XPERIMENT DETAILS E.1 E VALUATION PROTOCOL In the main results in Figure 6, we report the score for each task. The score is a task-specific metric of performance. We use return (sum of rewards) by default. The Pen / Cube rotation tasks use success rate and Noisy Monkey uses number of handholds. While the policy trains, we periodically evaluate the policy every 15000 training steps, and log the mean score over 15 evaluation episodes. We launch 4-10 seeds for each method, and report median and standard error of the evaluation scores in the learning curves over the seeds. We give each task a sample budget ranging from 250K to 5M. To compute the final normalized median scores, we perform the following steps for each task. First, obtain a lower and upper bound on performance by running Dreamerv3 using either o− or o+ as inputs for the task’s training budget. Next, we take each method’s median score over seeds at the end of training and normalize it by the lower and upper bound scores. We do this for each method. 17Published as a conference paper at ICLR 2024 Figure 12: We plot performance over walltime for Scaffolder and baselines. We then average the normalized scores for each method for all tasks, and report this as the final median score. E.2 Scaffolder HYPERPARAMETERS Next, we describe the hyperparameter settings for Scaffolder. In short, we found hyperparameter search to be short and easy, due to the robustness of the DreamerV3 algorithm. For new environ- ments, we found that DreamerV3 only needs tuning for two hyperparameters, the model size and update to data (UTD) ratio. We follow an easy guideline for tuning these - more complicated dy- namics generally require larger models, and tasks with harder exploration require more data and fewer updates (low UTD). As a result, we found hyperparameter settings that work for tasks with similar properties, as seen in Table 1. We did not tune these two DreamerV3 hyperparameters towards Scaffolder- rather, when we used model sizes and UTD ratios from when we ran DreamerV3 with privileged inputs as a reference method used for computing the upper bound scores. These same settings are then used for all DreamerV3 methods (Scaffolder, Informed Dreamer, DreamerV3+BC, Guided Observability) to be consistent. Table 1: DreamerV3 hyperparameters Model, UTD Small, 512 (Simple Dynamics, Easy Exploration) Blind Pick, Blind Locomotion, Wrist Pick-Place, Occluded Pick-Place Large, 512 (Complex dynamics, Easy Exploration) Blind Deaf Piano Large, 16 (Complex Dynamics, Hard Exploration) Noisy Monkey, Blind Pen, Blind Cube, RGB Pen, RGB Cube 18Published as a conference paper at ICLR 2024 E.3 B ASELINES DreamerV3 We run DreamerV3 “out of the box” with only access to target observations for all tasks. See the prior hyperparameters discussion for details. Informed Dreamer Informed Dreamer adds an additional loss term to DreamerV3 that pushes the world model to decode privileged information from the target observations. Informed Dreamer works best when the privileged information is predictable from the impoverished, target observation. Due to the simplicity of the method, which just decodes additional observations, we can just use the DreamerV3 codebase directly to run Informed Dreamer by changing the decoder outputs. DreamerV3+BC DreamerV3+BC trains two policies simultaneously, a teacher policy on privi- leged observations and a student policy on target observations. The teacher policy trains with the same objective as DreamerV3, while the student policy has a combined reinforcement learning and behavior cloning objective that pushes the student to both maximize return and mimic the teacher’s actions. The BC objective assumes the optimal student and teacher have the same action distribu- tions for it to be helpful. This assumption does not always hold true and is violated in multiple of S3’s environments, in- cluding Wrist Camera Pick-and-Place, Occluded Pick-and-Place, and Blind Pick. The student’s RL objective can help with this imitation gap, but DreamerV3+BC is still outperformed by numerous other methods that exploit privileged information at training time. We implement DreamerV3+BC on top of DreamerV3. To find the best balance between RL and BC objective for each task, we try BC weighting values of 0.1, 0.01, 0.001 for all 10 tasks, and report DreamerV3+BC results with the best weight for each task. Guided Observability Guided Observability implements a schedule for removing privileged in- formation during training. The policy is initially given both privileged and target observations, but as training progresses, the privileged information is dropped out with increasing probability. This dropout probability linearly increases with training iterations until only the target observations remain. We used a 50% cutoff for access to privileged information for all tasks, but this hyperpa- rameter can realistically be tuned for each environment. Guided Observability is algorithm agnostic but was implemented on top of DreamerV3 for this base- lines so as to remain as comparable as possible with Scaffolder and other Dreamer-based baselines. Our implementation closely mimicked PO-GRL (Weigand et al., 2021), but had one small modi- fication. In PO-GRL, dropped-out observations are directly placed in the replay buffer. For our implementation, however, only scaffolded observations (o+) are placed in the replay buffer, whether they were dropped out during the data collection process or not. Privileged information sampled from the replay buffer for world model training is then probabilistically dropped out on the same training schedule. This mitigates the risk of privileged information leaking into updates late in the training process. We choose a simple linear annealing schedule that anneals towards 0 by 50% of training for all tasks. RMA RMA shares the same policy between teacher and student, and assumes that the privileged teacher training uses low-dimensional privileged system information as input. Therefore, it is not conceptually or practically easy to incorporate noisy, high dimensional observations into teacher training. So instead, we just give RMA an advantage by giving it access to privileged low dimen- sional states instead of privileged observations. We use the RMA codebase from (Qi et al., 2023) to run our experiments. Asymmetric Actor Critic (AAC) Asymmetric Actor Critic (Pinto et al., 2018) conditions the critic on scaffolded observations, while only conditioning the actor on target observations. AAC assumes that providing privileged information to the critic will allow it to better estimate the value of states and thus, improve policy learning. 19Published as a conference paper at ICLR 2024 AAC was implemented on top of cleanrl’s (Huang et al., 2022b) implementation of PPO. To account for the relative sample inefficiency of PPO and model-free algorithms in general, a 100M sample budget was given to AAC for each task. E.4 S ENSORY SCAFFOLDING SUITE Blind Pick A two-fingered robot arm must pick up a randomly initialized block using only propri- oception (gripper position) and touch sensing. During training, it can use multiple privileged RGB camera inputs (two static cameras and one wrist camera). • Observation Space: – op: Privileged observations include three low-resolution RGB cameras: two static cameras face the workspace from the front and side, while one egocentric camera is located in the wrist. – o−: The target observation space includes the gripper’s position, velocity, and open/close state. Additionally, the robot has two binary touch sensors on each gripper finger. • Action Space: Gripper velocity and gripping force. • Reward: Task reward for successfully picking up the block and auxiliary rewards computed using distances between the gripper, block, and picking goal positions. Blind Locomotion In the Half Cheetah task, a proprioceptive policy must run and jump over randomly sized and positioned hurdles. During training, privileged RGB images of the Half Cheetah and its nearby hurdles are given, allowing the agent to see obstacles before bumping into them (see Figure 2). • Observation Space: – op: RGB camera that tracks the Half Cheetah as it moves. – o−: Joint angles and velocities. • Action Space: Torques for each of the 6 moveable joints on the Half Cheetah. • Reward Function: Reward proportional to the Half Cheetah’s forward-moving velocity with a penalty for incurred control costs. Blind and Deaf Piano A pair of 30-DoF Shadowhands in the Robopianist simulator ( ?) must learn to play “Twinkle Twinkle Little Star” using only proprioception. At training time, the policy has access to future notes, piano key presses, and suggested fingerings, which emulates having vision to see sheet music and hearing to determine which keys were pressed. • Observation Space: – op: Future notes for the next ten control steps, suggested fingerings for each set of notes, current piano key presses, previous action, and previous reward. – o−: Joint angles and forearm position for each Shadowhand. • Action Space: Desired joint angles and forearm position for each Shadowhand. • Reward Function: Task reward for playing correct notes with a penalty incurred for incorrect notes. Additionally, auxiliary rewards are provided to encourage the fingers to stay close to the keys and minimize energy. Blind and Numb PenA proprioceptive policy must control a 30-DoF Shadowhand to rotate a pen from a randomized initial orientation to a randomized desired orientation. The target policy receives joint angles, the initial orientation of the pen, and the desired goal orientation. During training, the policy has access to the object pose and touch sensors. • Observation Space: – op: Pen pose and dense touch sensing. – o−: Joint angles, initial pen pose, goal pen pose. 20Published as a conference paper at ICLR 2024 • Action Space: Joint angles. • Reward Function: Dense positive reward proportional to the similarity between the current and target pen orientations. A negative penalty is incurred for dropping the pen. Blind and Numb Cube Similar to Blind and Numb Pen rotation, a proprioceptive policy must control a 30-DoF Shadowhand to rotate a cube from a randomized initial orientation to a randomized desired orientation. • Observation Space: – op: Cube pose and touch sensing. – o−: Joint angles, initial cube pose, goal cube pose • Action Space: joint angles. • Reward Function: Dense positive reward proportional to the similarity between the current and target cube orientations. A negative penalty is incurred for dropping the cube. Noisy Monkey Bars A 13-link gibbon must swing between a fixed set of handholds in a 2D environment using the brachiation simulator ( ?). Handholds are placed far enough apart such that the gibbon must use its momentum to swing and fly through the air to reach the next handhold. To simulate imperfect sensors on a robotic platform, Gaussian noise is added to the target observations, while privileged observations represent true simulator states. To improve policy learning in this difficult control task, the relative position of the model’s center of mass from a provided reference trajectory is used as an auxiliary reward. • Observation Space: – op: Privileged observations include true simulator states, including the gibbon model’s over- all velocity and pitch, sin and cos of each joint angle, joint velocities, height of the current arm from the model’s center of mass, and whether each hand is currently grabbing or re- leased. Additionally, the relative position to the next handhold and the relative distance to the reference trajectory are provided to the policy. – o−: The target observation space is equivalent to the privileged observation space but with added Gaussian noise. Gaussian noise is applied to the true environment states rather than the observations to more accurately represent noisy sensors. For example, noise is added to joint angles rather than the sin and cos of those angles that the policy receives. • Action Space: Desired joint angle offsets for each joint on the gibbon model and a binary grab/release action for each hand. • Noise: Gaussian noise with 0 mean and 0.05 standard deviation is independently applied to each simulator state component. Angles are measured in radians and distance is measured in meters. For reference, the gibbon is 0.63m tall. • Reward Function: The reward function includes three terms: a style term to encourage the model to stay relatively upright and minimize jerky motions, a tracking term to encourage the model’s center of mass to stay close to the provided reference trajectory, and a sparse task reward term for each additional handhold reached. Wrist Camera Pick-and-Place This task examines the impact of multiple privileged 3rd-person cameras on learning a target active-perception policy with wrist camera input. Given the wrist cam- era’s restricted field of view, the target policy must concurrently learn to move to locate the block and bin while executing the task. This task mirrors real-world scenarios such as mobile manipulation, where a controllable wrist camera with a limited field of view might be the sole visual sensor. • Observation Space: – op: Two static, low-resolution RGB cameras facing the workspace from the front and side. – o−: Proprioceptive information including the gripper’s position, velocity, and open/close state, touch sensing from two binary touch sensors located on each gripper finger, and vision from a low-resolution RGB egocentric camera located in the wrist of the robot. • Action Space: Gripper velocity and force. 21Published as a conference paper at ICLR 2024 • Reward Function: Sparse task reward for placing the block in the bin and auxiliary rewards com- puted using distances between the gripper, block, and bin positions. Occluded Pick-and-Place Rather than employing active visual perception at test time, this task examines the impact of active-perception as privileged sensing at training time. The target policy must use an RGB camera from an occluded viewpoint alongside proprioception and touch sensing to pick up a block behind a shelf and place it into a bin. Both object and bin locations are ran- domly initialized. During training time, the robot gets access to a privileged wrist mounted camera, enabling it to perform active visual perception to locate the block during training (see Figure 2). • Observation Space: – op: Two low-resolution RGB cameras: one static, unoccluded camera facing the workspace from the side and one active-perception camera located in the wrist of the robot. – o−: One occluded low-resolution RGB camera. Although this camera does not reveal any information about the block, it does encode the position of the randomly placed bin. • Action Space: Gripper velocity and force. • Reward Function: Sparse task reward for placing the block in the bin and auxiliary rewards com- puted using distances between the gripper, block, and bin positions. Visual Pen Rotation We carry over the setup from the previous two blind and numb dexterous manipulation tasks with privileged object pose and contact sensors. In this task, however, the target policy gains access to a top-down RGB image (see Figure 5) to rotate a pen from a randomized initial orientation to a randomized desired orientation. At training time, the policy has access to the pen pose and touch sensing. • Observation Space: – op: Pen pose and dense touch sensing. – o−: Joint angles, One top-down, low-resolution RGB camera. desired pen pose • Action Space: Desired joint angles. • Reward Function: Dense positive reward proportional to the similarity between the current and target pen orientations. A negative penalty is incurred for dropping the pen. Visual Cube Rotation Similarly to Visual Pen Rotation, a visual target policy conditioned on a top-down RGB image and proprioception must rotate a block to a desired orientation. • Observation Space: – op: Cube pose and dense touch sensing. – o−: Joint angles, topdown RGB camera, desired cube pose. • Action Space: Desired joint angles. • Reward Function: Dense positive reward proportional to the similarity between the current and target cube orientations. A negative penalty is incurred for dropping the cube. 22Published as a conference paper at ICLR 2024 F A DDITIONAL TD( λ) ERROR EXPERIMENTS . Figure 13: Top: Blind Nav., Bottom: Blind Locomotion Let the scaffolded TD( λ) return be Equation (1), and the impoverished “target” TD( λ) return be the original DreamerV3 policy objective, which substitutes the blue terms in Equation (1) with target dynamics p− ϕ (z− t+1 | z− t , at), target rewards p− ϕ (r− t | z− t ), and target critic v− ψ (z− t ). We compute the mean absolute error (MAE) for each of these two estimates against the ground truth Monte-Carlo return Rt. We evaluate this in two additional environments, Blind Navigation and Blind Locomotion. In Blind Navigation, a randomly initialized agent must find a randomly ini- tialized goal point in a 15x15 gridworld. The privileged observation is the location of the goal in every epsiode, while the target observation is the x,y location of the agent. Every 25K steps while training, we collect ∼1000 transitions with the policy and compute Rt. Figure 9 plots Advantage = (target return MAE − scaf- folded return MAE ). If the target error is higher than the scaffolded estimate’s error, then Advantage is posi- tive. The advantages are mostly positive, indicating that scaffolded TD(λ) is more accurate. This provides evi- dence for why Scaffolder trains better policies than non- privileged counterparts — its policy objective better ap- proximates the true expected return, giving the policy op- timizer a better training signal. G S CAFFOLDER WITH REWARDS ESTIMATED FROM PRIVILEGED SENSORS Figure 14: Scaffolder still outperforms baselines in the estimated reward setting. To simulate realistic real world RL scenarios, we estimate reward signals from noisy sensors rather than using rewards computed from simulator state. We modify the Blind Pick environment’s reward function to use estimated object pose rather than ground truth object pose, acquired via color-segmentation from the privileged cameras. The estimated object pose now has noise due to partial occlusions of the block by the robot gripper. We train Scaffolder and other methods on the estimated reward, and evaluate on the ground truth return. As seen in Figure 14, all meth- ods suffer some performance degradation. Scaffolder gets around 200 return whereas in the original Blind Pick it gets 300. However, the performance trends are consistent. Looking at policy rollouts, we find that only Scaffolder can reliably pick up the block and all other methods fail. 23Published as a conference paper at ICLR 2024 H Scaffolder ON PRE -EXISTING ENVIRONMENTS We ran Scaffolder and DreamerV3 (operating only on target input, consistent with our other results) on two other environments from the COSIL Nguyen et al. (2022) paper, Bumps-2D and Car-Flag. In Bumps-2D, a robot arm must find the bigger bump out of a pair of randomly initialized bumps in a 2D grid, using only proprioception and touch sensing. In Car-Flag, a car using just proprioception must find a randomly initialized green flag. If the car finds a blue flag, the green flag position is revealed. In both environments, the state expert can directly go to the bigger bump / green flag, while the student policy must perform information gathering actions to solve the task. Figure 15: Performance of Scaffolder and DreamerV3 on two environments from COSIL. As seen in Figure 15, the new results are consistent with those reported in our paper on the 10 S3 tasks: Scaffolder clearly outperforms DreamerV3, further establishing the versatility of our approach for utilizing privileged sensory information. In addition, these experiments also enable new comparisons with the prior work evaluated in COSIL. Note that COSIL and its baselines operate in a slightly different setting from Scaffolder: they as- sume access to a perfect, hand-coded scaffolded policy , whereas Scaffolder trains both target and scaffolded policies from scratch, and in tandem, starting at environment step 0. This means that, when comparing learning curves, we must remember that COSIL effectively “starts ahead” ofScaf- folder. Even with this handicap, Scaffolder easily outperforms COSIL on Bumps-2D, and matches it on Car-Flag. These results on COSIL’s own evaluation environments show further evidence of Scaffolder ’s improvements over prior state of the art. 24",
      "references": [
        "Learning dexterous in-hand manipulation",
        "Unbiased asymmetric reinforcement learning under partial observability",
        "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
        "Learning by cheating",
        "Sequential dexterity: Chaining dexterous policies for long-horizon manipulation",
        "Variational inverse control with events: A general framework for data-driven reward definition",
        "Dream to control: Learning behaviors by latent imagination",
        "Deep hierarchical planning from pixels",
        "Mastering diverse domains through world models",
        "Watch and match: Supercharging imitation with regularized optimal transport",
        "Know thyself: Transferable visual control policies through robot-awareness",
        "Training robots to evaluate robots: Example-based interactive reward functions for policy learning",
        "Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms",
        "Planning and acting in partially observable stochastic domains",
        "Privileged information dropout in reinforcement learning",
        "RMA: Rapid Motor Adaptation for Legged Robots",
        "Informed POMDP: Leveraging Additional Information in Model-Based RL",
        "Learning Quadrupedal Locomotion over Challenging Terrain",
        "Leveraging fully observable policies for learning under partial observability",
        "Deepmimic: Example-guided deep reinforcement learning of physics-based character skills",
        "Asymmetric actor critic for image-based robot learning",
        "In-hand object rotation via rapid motor adaptation",
        "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations",
        "Attention-privileged reinforcement learning",
        "Visual closed-loop control for pouring liquids",
        "Simultaneously learning vision and feature-based control policies for real-world ball-in-a-cup",
        "Multi-view masked world models for visual robotic manipulation",
        "Time-contrastive networks: Self-supervised learning from video",
        "Tgrl: An algorithm for teacher guided reinforcement learning",
        "Transfer RL across observation feature spaces via model-based regularization",
        "Sequence model imitation learning with unobserved contexts",
        "Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies",
        "Transfer learning via inter-task mappings for temporal difference learning",
        "Domain randomization for transferring deep neural networks from simulation to the real world",
        "A new learning paradigm: Learning using privileged information",
        "Interaction between learning and development",
        "Impossibly good experts and how to follow them",
        "Reinforcement learning using guided observability",
        "Bridging the imitation gap by adaptive insubordination",
        "The role of tutoring in problem solving",
        "Policy transfer across visual and dynamics domain gaps via iterative grounding"
      ],
      "meta_data": {
        "arxiv_id": "2405.14853v1",
        "authors": [
          "Edward S. Hu",
          "James Springer",
          "Oleh Rybkin",
          "Dinesh Jayaraman"
        ],
        "published_date": "2024-05-23T17:57:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces \"sensory scaffolding,\" a novel problem setting where artificial agents exploit privileged, training-time-only observation streams to learn policies that operate with limited sensors at test time. The main contribution is Scaffolder, a model-based reinforcement learning (MBRL) approach that comprehensively utilizes privileged sensing in critics, world models, reward estimators, exploration policies, and representation learning objectives to enhance target policy training. The research also introduces the \"S3\" (Sensory Scaffolding Suite), a new benchmark of ten diverse simulated robotic tasks for evaluating sensory scaffolding agents. Scaffolder consistently outperforms prior baselines and often achieves performance comparable to policies with test-time access to privileged sensors, demonstrating that the gap between policies operating with and without privileged sensors can be significantly bridged through training-time scaffolding. The paper also provides detailed analyses and ablation studies, showing the importance of each scaffolded component and how their contributions vary with task-specific properties.",
        "methodology": "Scaffolder is an MBRL approach built upon DreamerV3. Its core methodology involves training a \"scaffolded world model\" using privileged observations (o+) instead of impoverished target observations (o-). This scaffolded world model, which includes scaffolded dynamics, reward estimators, and value functions, is used to generate more accurate synthetic training experiences. The target policy operates on target latents (z-) while the scaffolded world model produces scaffolded latents (z+). To reconcile this, Scaffolder employs \"Nested Latent Imagination (NLI).\" NLI involves a \"transdecoder\" that decodes scaffolded latents (z+) into corresponding target observations (o-), which are then integrated by the recurrent target posterior network into the target latent state (z-), allowing for policy rollouts within the scaffolded world model. The policy objective is modified to use these scaffolded components for more accurate TD(λ) return estimates. Additionally, a \"scaffolded exploration policy\" (πe) is trained on scaffolded world model states (s+) to collect more informative trajectories. Lastly, a scaffolded representation learning objective is used, where an auxiliary decoder from the target latent state (z-) to the scaffolded observation (o+) improves the target latent representations.",
        "experimental_setup": "The research introduces the \"S3\" (Sensory Scaffolding Suite), a benchmark comprising ten diverse simulated robotic tasks to evaluate sensory scaffolding agents. These tasks cover various restricted sensing scenarios, including proprioceptive-only inputs, noisy sensors, images, and occluded or moving viewpoints. Privileged sensors used in these tasks include multiple cameras, controllable cameras, object pose, touch sensors, audio, and sheet music. Performance is validated by comparing Scaffolder against several prior baselines: Unprivileged (DreamerV3), Critic (Asymmetric Actor Critic - AAC), Teacher Policy (DreamerV3+BC), World Model (Informed Dreamer), Representation (RMA+), and Exploration Policy (Guided Observability). Scaffolder and DreamerV3-based baselines were run on a sample budget ranging from 250K to 5M steps, while model-free baselines (AAC and RMA+) were given 100M steps due to their higher sample inefficiency. Each method was evaluated with 4-10 seeds, reporting median scores and standard error. Normalized final median scores were computed using the unprivileged DreamerV3 as a lower bound (0.0) and a privileged DreamerV3 (trained and evaluated on o+) as an upper bound (1.0). The study also included ablation studies on Scaffolder's components and analysis of TD(λ) error in environments like Blind Pick, Blind Navigation, and Blind Locomotion.",
        "limitations": "A significant limitation is the increased computational and resource burden of Scaffolder, as it trains an additional scaffolded world model and exploration policy alongside the original models. The current approach also relies on specific methods for leveraging privileged observations (e.g., reconstruction targets for representation learning), suggesting that more effective privileged representation learning objectives could exist. Practical issues with using additional sensors at training time, such as increased computation, bandwidth requirements, and incomplete multimodal datasets, are also acknowledged. Empirically, the task suite is primarily focused on simulated robotic tasks, suggesting a need for validation in other domains like real-world robotics and video games.",
        "future_research_directions": "Future research could explore reducing the computational burden by investigating shared dynamics models and policies across both privileged and target observations, potentially drawing insights from multi-task learning. Developing improved privileged representation learning objectives beyond simple reconstruction targets is another avenue. Extending the evaluation to real-world robotics and video game environments could provide valuable insights into diverse forms of privileged sensing. Investigating methods to estimate rewards through privileged sensors in real-world settings, rather than relying on black-box rewards from human supervision, is also proposed. Finally, the work opens deeper theoretical questions about the fundamental relationship between sensing and learning, and how the informational requirements of a learning agent vary with tasks and learning phases.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "CoT distillation still conflates *learning to reason* with *learning to echo a teacher’s narrative*. Even consequence-aware token weighting (e.g., leave-one-out influence on answer NLL) can remain noisy and brittle because (i) teacher-forcing makes later hidden states depend on earlier tokens in a way that can overestimate a token’s “causal” role, and (ii) nothing explicitly teaches the student that many rationale tokens should be *ignored* (answer should be invariant to them), which is a core human metacognitive skill. The key gap is a feasible training objective that jointly (a) concentrates imitation on decision-critical, uncertain steps **and** (b) actively trains invariance to low-influence (often stylistic/fluff) rationale content, producing shorter and more robust reasoning traces rather than merely reweighting losses.",
    "method": "**Meta-Causal Invariance CoT Distillation (MCI-CoT)**\n\nMCI-CoT refines CI-UF-KCoT by adding a *counterfactual invariance* objective and a *minimal-sufficient* sparsity budget. The method is still a drop-in loss for any causal LM trained with teacher CoT.\n\n### 1) Online causal influence (positive credit assignment)\nFor each rationale token t, estimate its student-conditional influence on the answer by a controlled intervention:\n- Run teacher-forced forward pass on the original sequence to get answer-span NLL.\n- For a small subset of rationale tokens (e.g., top-k by uncertainty), corrupt token t (replace with [UNK] or a random token from a corruption set) and re-run.\n- Define influence as i_t = ReLU(NLL_A(corrupt t) − NLL_A(orig)), detached.\n\n### 2) Uncertainty and optional keypoints (difficulty + structure)\nCompute student uncertainty u_t = 1 − p_θ(y_t) (or entropy), detached. Optionally multiply by KPOD-style or heuristic keypoint weight w_t.\n\n### 3) *New*: Minimal-sufficient imitation via sparsity budget\nInstead of freely scaling weights, MCI-CoT normalizes and caps total “imitation mass” per example to mimic human selective attention:\n- Raw weight: r_t = w_t · u_t · norm(i_t)\n- Attention weight: α_t = r_t / (mean_r + ε)\n- Budget penalty: L_budget = (mean(α_t over rationale) − τ)^2  (τ controls how many tokens get emphasized)\nThis prevents the model from spreading credit widely when influence estimates are noisy.\n\n### 4) *New*: Counterfactual answer invariance for low-influence tokens (negative supervision)\nHumans are robust to stylistic variations once the key computations are fixed. MCI-CoT enforces this directly:\n- Define a low-influence set S_low (e.g., bottom q-quantile of α_t among rationale tokens, excluding keypoint digits/operators).\n- Create a *single* “junked rationale” counterfactual by corrupting **all** tokens in S_low simultaneously.\n- Run one extra forward pass and minimize a consistency loss on the answer distribution:\n  L_inv = Σ_{i∈A} KL( stopgrad(p_orig(·|prefix_i)) || p_junk(·|prefix_i_junk) )\nThis trains the student to keep its answer beliefs stable when non-critical rationale content changes, addressing teacher-forcing confounds by supervising *distributional invariance* rather than only ΔNLL from single-token breaks.\n\n### 5) Final objective\nL = L_ans + L_rat(α-weighted imitation on rationale) + β·L_inv + γ·L_budget\nwhere L_ans is the standard unweighted answer-span CE to prevent degenerate “ignore rationales” solutions.\n\n**What is novel vs KPOD/UF/CI-UF-KCoT**\n- Adds *negative causal supervision*: not just “imitate influential tokens more,” but “make the answer invariant to non-influential tokens,” which explicitly targets the social failure mode of persuasive-but-contentless rationales.\n- Introduces a *minimal-sufficient budget* that regularizes influence weights into a controllable, human-like attention allocation, making the causal signal more stable and less prone to teacher-forcing artifacts.\n- Keeps everything student-conditional, online, and verifiable with pure PyTorch (no extra annotators/models).",
    "experimental_setup": "### Goal\nTest whether MCI-CoT improves answer accuracy and *robustness to rationale style/noise* while reducing fluff imitation, under a fixed training budget.\n\n### Datasets\n1) **Synthetic Arithmetic-with-Fluff+Noise (reproducible)**\n- Generate word problems for +/− (optionally ×/÷) with templated teacher CoTs.\n- Inject two nuisance types:\n  (a) stylistic fluff phrases;\n  (b) *semantically plausible but irrelevant* clauses containing extra numbers/units (to test that the model learns to ignore non-causal numerals, not just generic prose).\n- Create OOD test splits:\n  - More fluff + unseen fluff lexicon\n  - Higher rate of irrelevant-number clauses\n\n2) **GSM8K sanity-check (small subset)**\n- 500 train / 200 dev with cached teacher CoTs.\n\n### Models\n- Synthetic: small decoder-only Transformer (e.g., 6 layers, d=256).\n- GSM8K: optional 1–3B model with LoRA (if available); otherwise keep synthetic as the main claim.\n\n### Baselines (same compute/steps)\nA) Uniform CoT supervision (rationale+answer CE)\nB) KPOD-lite heuristic keypoints\nC) UF-KCoT (keypoint × uncertainty)\nD) CI-UF-KCoT (keypoint × uncertainty × leave-one-out influence)\nE) **MCI-CoT (ours)** = CI-UF-KCoT + invariance loss + budget regularizer\nAblations:\n- E w/o invariance (β=0)\n- E w/o budget (γ=0)\n\n### Metrics\nPrimary:\n- **accuracy**: exact-match on final answer span.\n\nSecondary:\n- OOD accuracy drop (increased fluff / irrelevant-number splits)\n- Fluff-token imitation rate (match rate on known fluff tokens)\n- *Numeric distraction susceptibility*: error rate when irrelevant-number clauses are present\n- Average generated rationale length (when prompted to output CoT)\n- Answer invariance score: KL(p_orig || p_junk) on held-out data under synthetic junking (lower is better)\n\n### Practical compute note\nInfluence i_t computed only for top-k uncertain rationale tokens (k=4–8). Invariance uses one additional forward pass per batch by corrupting S_low simultaneously, keeping overhead modest.",
    "primary_metric": "accuracy",
    "experimental_code": "import torch\nimport torch.nn.functional as F\n\n\ndef _token_ce(logits, targets):\n    B, T, V = logits.shape\n    return F.cross_entropy(logits.reshape(B*T, V), targets.reshape(B*T), reduction='none').view(B, T)\n\n\ndef _kl_token(p_log, q_log):\n    # KL(P||Q) where inputs are log-probs\n    p = p_log.exp()\n    return (p * (p_log - q_log)).sum(dim=-1)\n\n\ndef mci_cot_loss(\n    model,\n    input_ids,                 # [B,T] full teacher-forced sequence: prompt + rationale + answer\n    rationale_mask,            # [B,T] True on rationale token positions (for targets)\n    answer_mask,               # [B,T] True on answer token positions (for targets)\n    keypoint_w=None,           # [B,T] optional\n    unk_id=0,\n    topk_influence=8,\n    low_quantile=0.5,          # fraction of rationale tokens treated as \"low influence\" for invariance\n    beta_inv=0.2,\n    gamma_budget=0.01,\n    tau_budget=1.0,            # target mean weight on rationale tokens (after normalization)\n    eps=1e-8,\n):\n    \"\"\"Meta-Causal Invariance CoT Distillation (MCI-CoT).\n\n    Returns scalar loss = answer CE + weighted rationale imitation + invariance KL + budget penalty.\n    \"\"\"\n    if keypoint_w is None:\n        keypoint_w = torch.ones_like(input_ids, dtype=torch.float32)\n\n    # next-token setup\n    x = input_ids[:, :-1]\n    y = input_ids[:, 1:]\n    rat_m = rationale_mask[:, 1:]\n    ans_m = answer_mask[:, 1:]\n    kp_w = keypoint_w[:, 1:]\n\n    logits = model(x)  # [B,T-1,V]\n    ce = _token_ce(logits, y)  # [B,T-1]\n\n    # base answer loss (always on)\n    ans_loss = (ce * ans_m.float()).sum() / (ans_m.float().sum() + eps)\n\n    # student uncertainty u_t = 1 - p(y_t)\n    with torch.no_grad():\n        logp = F.log_softmax(logits, dim=-1)\n        pt = logp.gather(-1, y.unsqueeze(-1)).squeeze(-1).exp()\n        u = (1.0 - pt).clamp(0.0, 1.0)\n\n        orig_ans_nll = (ce * ans_m.float()).sum(dim=1)  # [B]\n\n        # candidate rationale tokens for influence estimation: top-k uncertain among rationale\n        u_rat = u.masked_fill(~rat_m, -1e9)\n        k = min(topk_influence, u_rat.size(1))\n        idx = torch.topk(u_rat, k=k, dim=1).indices  # [B,k]\n\n    # influence values (only filled at selected indices)\n    influence = torch.zeros_like(u)\n\n    B, Tm1 = y.shape\n    for j in range(idx.size(1)):\n        t_idx = idx[:, j]  # [B]\n        x_corr = x.clone()\n        x_corr[torch.arange(B, device=x.device), t_idx] = unk_id\n\n        with torch.no_grad():\n            logits_corr = model(x_corr)\n            ce_corr = _token_ce(logits_corr, y)\n            corr_ans_nll = (ce_corr * ans_m.float()).sum(dim=1)\n            delta = (corr_ans_nll - orig_ans_nll).clamp(min=0.0)\n\n        influence[torch.arange(B, device=x.device), t_idx] = delta\n\n    # normalize influence within-example\n    with torch.no_grad():\n        infl_mean = (influence * rat_m.float()).sum(dim=1) / (rat_m.float().sum(dim=1) + eps)\n        infl_norm = (influence / (infl_mean.unsqueeze(1) + eps)).clamp(0.0, 5.0)\n\n        # raw positive weights\n        r = kp_w * u * infl_norm\n        r_rat = r.masked_fill(~rat_m, 0.0)\n        mean_r = (r_rat.sum(dim=1) / (rat_m.float().sum(dim=1) + eps)).unsqueeze(1)\n        alpha = (r / (mean_r + eps)).masked_fill(~rat_m, 0.0)  # normalized\n\n    # weighted rationale imitation (positive supervision)\n    rat_loss = (alpha * ce * rat_m.float()).sum() / (alpha * rat_m.float()).sum().clamp(min=eps)\n\n    # budget regularizer to keep attention selective/stable\n    with torch.no_grad():\n        alpha_mean = (alpha * rat_m.float()).sum(dim=1) / (rat_m.float().sum(dim=1) + eps)\n    budget_loss = ((alpha_mean - tau_budget) ** 2).mean()\n\n    # --- invariance loss: corrupt low-influence rationale tokens all-at-once ---\n    # choose low-influence set per example based on alpha quantile (excluding non-rationale positions)\n    with torch.no_grad():\n        alpha_rat = alpha.masked_fill(~rat_m, 1e9)  # non-rationale won't be selected as low\n        # approximate quantile via topk of smallest values among rationale\n        rat_counts = rat_m.float().sum(dim=1).clamp(min=1).long()  # [B]\n        m = torch.clamp((rat_counts.float() * low_quantile).long(), min=1)\n\n        low_mask = torch.zeros_like(rat_m)\n        for b in range(B):\n            # get indices of smallest alpha within rationale\n            vals = alpha[b].masked_fill(~rat_m[b], 1e9)\n            _, low_idx = torch.topk(-vals, k=int(m[b].item()))  # largest of (-vals) => smallest vals\n            low_mask[b, low_idx] = True\n\n    x_junk = x.clone()\n    x_junk[low_mask] = unk_id\n\n    logits_junk = model(x_junk)\n    logp_junk = F.log_softmax(logits_junk, dim=-1)\n\n    # KL on answer token distributions (detach orig to avoid collapse)\n    with torch.no_grad():\n        logp_orig_det = F.log_softmax(logits, dim=-1)\n\n    kl_tok = _kl_token(logp_orig_det, logp_junk)  # [B,T-1]\n    inv_loss = (kl_tok * ans_m.float()).sum() / (ans_m.float().sum() + eps)\n\n    return ans_loss + rat_loss + beta_inv * inv_loss + gamma_budget * budget_loss\n",
    "expected_result": "**Synthetic Arithmetic-with-Fluff+Noise (same model/steps):**\n- A Uniform CE: accuracy 88–91% (OOD irrelevant-number: 72–78%)\n- C UF-KCoT: accuracy 93–95% (OOD: 82–86%)\n- D CI-UF-KCoT: accuracy 95–97% (OOD: 86–90%)\n- **E MCI-CoT (ours): accuracy 96.5–98% (OOD: 90–94%)**\n\nSecondary expectations (E vs D):\n- Fluff-token imitation rate: −25% to −45% relative\n- Numeric distraction susceptibility: −20% to −35% relative error rate\n- Answer invariance KL under synthetic junking: −30% to −50%\n- Rationale length when generating CoT: −10% to −25%\n\n**GSM8K (500 train / 200 dev, LoRA if available):**\n- Expect +1.0 to +2.0 absolute accuracy over CI-UF-KCoT, with larger gains on items whose teacher CoTs contain redundant prose or extra numerals.\n\n(Higher accuracy is better; lower OOD drop / KL / fluff imitation is better.)",
    "expected_conclusion": "MCI-CoT strengthens consequence-aware CoT distillation by adding two human-inspired components that prior token-weighting methods lack:\n1) **Counterfactual invariance training**: explicitly teaches the student that non-decision-critical rationale content should not change the answer distribution, reducing overfitting to persuasive but content-free explanations and improving robustness to style shifts and distractor numerals.\n2) **Minimal-sufficient budget regularization**: turns noisy causal influence estimates into a stable, metacognitive “attention budget,” better matching human selective effort allocation.\n\nAcademically, this reframes CoT distillation as *positive causal credit assignment + negative invariance constraints*, bridging attribution-style counterfactuals with representation robustness in a single training-time objective. Socially, it reduces incentives for models to learn and reproduce verbose rationales that sound convincing but are not causally tied to correctness, improving trustworthiness and lowering compute via shorter rationales. The hypothesis remains highly testable with a single PyTorch training script and synthetic data generation."
  },
  "experimental_design": {
    "experiment_summary": "Task: train a causal language model to solve grade-school math word problems by producing a final numeric answer, while optionally emitting a chain-of-thought (CoT) rationale. We test whether the proposed Meta-Causal Invariance CoT Distillation (MCI-CoT) yields higher answer accuracy and greater robustness when rationale text is corrupted with stylistic fluff or irrelevant-number distractions.\n\nWorkflow:\n1) Data preparation (GSM8K): use a compute-feasible subset (e.g., 2,000 train / 500 dev / 1,000 test from the official splits). For each example, build a teacher-forced sequence: [question prompt] + [teacher CoT rationale] + [final answer].\n2) Teacher CoT creation: generate one teacher rationale per training example using the same base model in “thinking” mode (self-distillation) and cache it to disk; then strip to a bounded length (e.g., max 256 rationale tokens) and keep the final answer span separate.\n3) Fine-tuning (student): LoRA fine-tune the base LM with one of two losses (baseline CI-UF-KCoT vs proposed MCI-CoT). Both always include an unweighted answer-span CE loss to prevent “ignore rationale” collapse.\n4) Online influence estimation: during training, for top-k uncertain rationale tokens, compute a detached influence score via controlled token corruption and measuring delta answer NLL; this supplies positive credit assignment.\n5) New components (MCI-CoT only): (a) a per-example minimal-sufficient sparsity budget regularizer that discourages spreading imitation mass across many rationale tokens; (b) a counterfactual invariance objective that corrupts the low-influence rationale subset in one shot and minimizes KL divergence between original vs junked answer distributions.\n6) Evaluation: report accuracy on the clean test set and on OOD test-time perturbations (extra fluff text, irrelevant-number clauses) applied to the rationale portion (for invariance metrics) and/or prompt (for robustness).\n\nRunner fit: all training is single-GPU (A100/H200 80GB+) with LoRA, mixed precision (bf16), sequence length ≤1024, and a small GSM8K subset. Influence estimation uses small top-k (4–8) and the invariance loss adds only one extra forward pass per batch, keeping overhead modest. Optuna trials are capped and can be run sequentially on the same GPU.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA A100 or H200, VRAM: 80 GB or more, RAM: 2048 GB or more"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Correctness criteria: a prediction is correct if the model’s extracted final numeric answer matches the ground-truth GSM8K answer after normalization (remove commas, currency symbols, trailing periods, and whitespace; convert to canonical decimal/integer string; accept exact numeric equality). If the dataset answer is an integer, accept predictions that parse to the same integer; if a decimal, require exact decimal match after removing trailing zeros.\n\nCalculation method: accuracy = (1/N) * Σ_i 1[normalize(pred_i)==normalize(gold_i)], where pred_i is obtained by greedy decoding (or temperature=0) and extracting the last number in the output (recommended rule used in many GSM8K evaluations).\n\nTask appropriateness: GSM8K is a final-answer math benchmark; exact numeric correctness is the primary objective.\n\nRelevant visualizations: (1) learning curves of dev accuracy vs steps; (2) bar chart of accuracy by problem length bucket (question token length); (3) histogram of absolute numeric error for incorrect cases (optional, computed on parsed floats when possible)."
      },
      {
        "name": "OOD_accuracy_drop",
        "description": "Correctness criteria: same as accuracy, but evaluated under predefined out-of-distribution (OOD) perturbations.\n\nCalculation method: For each perturbation type p, compute accuracy_clean and accuracy_ood(p). Report drop(p)=accuracy_clean-accuracy_ood(p). Aggregate as mean_drop = (1/|P|) Σ_p drop(p).\n\nPerturbations (must be deterministic given a seed):\n- Prompt-fluff: append 1–2 sentences of irrelevant polite/fluffy text to the question.\n- Prompt-irrelevant-numbers: append a clause containing extra numbers/units unrelated to the asked quantity.\n\nTask appropriateness: tests robustness to nuisance text and distractor numerals, matching the hypothesis that MCI-CoT teaches invariance to non-causal rationale/prompt content.\n\nRelevant visualizations: grouped bar chart of clean vs each OOD accuracy; line plot of accuracy vs perturbation strength (e.g., number of added distractor numerals)."
      },
      {
        "name": "Answer_invariance_KL",
        "description": "Correctness criteria: lower KL indicates the model’s answer-token predictive distribution is stable when low-influence rationale tokens are jointly corrupted.\n\nCalculation method:\nFor each held-out example, compute two teacher-forced forward passes over the same target answer span:\n- Original: p_orig(y_i | prefix_i)\n- Junked: p_junk(y_i | prefix_i_junk), where prefix_i_junk is formed by replacing the identified low-influence rationale tokens with [UNK] (or a reserved corruption token).\nThen compute tokenwise KL and average over answer tokens:\nKL_i = (1/|A_i|) Σ_{t∈A_i} KL( stopgrad(p_orig^t) || p_junk^t ).\nReport dataset mean: (1/N) Σ_i KL_i.\n\nTask appropriateness: directly measures the claimed property (answer distribution invariance to low-influence rationale content) rather than relying only on downstream accuracy.\n\nRelevant visualizations: violin/box plot of per-example KL; scatter plot of KL vs correctness (correct/incorrect) to see if invariance correlates with accuracy."
      },
      {
        "name": "Numeric_distraction_error_rate",
        "description": "Correctness criteria: an example counts as a distraction failure if it becomes incorrect under an irrelevant-number perturbation while being correct on the clean version.\n\nCalculation method:\nLet C be the set of examples correct on clean. For a fixed irrelevant-number perturbation, compute error_rate = (1/|C|) Σ_{i∈C} 1[ pred_ood(i) != gold(i) ].\n\nTask appropriateness: isolates susceptibility to added numerals/quantities, which is a core failure mode targeted by invariance training.\n\nRelevant visualizations: bar chart of error_rate across perturbation strengths (0,1,2,3 added numerals)."
      },
      {
        "name": "Generated_rationale_length",
        "description": "Correctness criteria: shorter is not automatically better; interpret alongside accuracy. Measures whether methods reduce verbose/fluff imitation.\n\nCalculation method: On a rationale-required prompt (e.g., \"Explain then answer.\"), decode with fixed max_new_tokens and temperature=0. Count the number of tokens between the rationale start marker (e.g., \"Let's think\") and the answer marker (e.g., \"Answer:\"). Report mean and median length.\n\nTask appropriateness: the hypothesis predicts MCI-CoT produces shorter, more minimal-sufficient reasoning traces.\n\nRelevant visualizations: histogram of rationale lengths; scatter plot of length vs correctness."
      }
    ],
    "models_to_use": [
      "Qwen3-1.7B (1.7B parameters)"
    ],
    "datasets_to_use": [
      "gsm8k"
    ],
    "proposed_method": {
      "method_name": "Meta-Causal Invariance CoT Distillation (MCI-CoT)",
      "description": "A drop-in CoT distillation loss that combines (1) student-conditional online causal influence estimation (token corruption impact on answer NLL) and uncertainty/keypoint weighting for positive credit assignment, with two new components: (2) a minimal-sufficient sparsity budget that normalizes/caps total imitation mass on rationale tokens per example to stabilize noisy influence signals, and (3) a counterfactual answer-invariance objective that jointly corrupts low-influence rationale tokens and minimizes KL divergence between original vs junked answer distributions on the answer span. This explicitly trains the student to ignore non-decision-critical rationale content.",
      "training_config": {
        "learning_rate": 0.0002,
        "batch_size": 16,
        "epochs": 3,
        "optimizer": "adamw",
        "warmup_steps": 200,
        "weight_decay": 0.01,
        "gradient_clip": 1.0,
        "scheduler": "cosine",
        "seed": 42,
        "additional_params": "{\n  \"finetune\": \"LoRA\",\n  \"lora_r\": 16,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.05,\n  \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n  \"precision\": \"bf16\",\n  \"max_seq_len\": 1024,\n  \"train_subset\": {\"train\": 2000, \"dev\": 500},\n  \"teacher_cot\": {\n    \"generation\": \"self-distillation from same base model\",\n    \"max_new_tokens_rationale\": 256,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"seed\": 123\n  },\n  \"mci_cot\": {\n    \"unk_id\": \"tokenizer.unk_token_id\",\n    \"topk_influence\": 8,\n    \"low_quantile\": 0.5,\n    \"beta_inv\": 0.2,\n    \"gamma_budget\": 0.01,\n    \"tau_budget\": 1.0,\n    \"influence_corruption\": \"replace selected token(s) with [UNK]\",\n    \"exclude_from_low_set\": \"digits/operators if using simple regex-based keypoints\"\n  },\n  \"grad_accum_steps\": 8,\n  \"eval_every_steps\": 200\n}"
      },
      "optuna_config": {
        "enabled": true,
        "n_trials": 12,
        "search_spaces": [
          {
            "param_name": "beta_inv",
            "distribution_type": "loguniform",
            "low": 0.03,
            "high": 0.6
          },
          {
            "param_name": "gamma_budget",
            "distribution_type": "loguniform",
            "low": 0.001,
            "high": 0.1
          },
          {
            "param_name": "topk_influence",
            "distribution_type": "categorical",
            "choices": [
              4,
              6,
              8
            ]
          },
          {
            "param_name": "low_quantile",
            "distribution_type": "categorical",
            "choices": [
              0.3,
              0.5,
              0.7
            ]
          },
          {
            "param_name": "learning_rate",
            "distribution_type": "loguniform",
            "low": 5e-05,
            "high": 0.0005
          },
          {
            "param_name": "lora_r",
            "distribution_type": "categorical",
            "choices": [
              8,
              16,
              32
            ]
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "CI-UF-KCoT (baseline)",
        "description": "Consequence-Influence × Uncertainty (× optional keypoints) token-weighted CoT distillation: estimate per-token influence via leave-one-out corruption delta on answer NLL, multiply by student uncertainty (and optional heuristic keypoint weights), and use the resulting weights to scale rationale imitation loss. No explicit counterfactual invariance objective and no minimal-sufficient budget regularizer; serves as the strongest prior token-reweighting baseline aligned with the hypothesis statement.",
        "training_config": {
          "learning_rate": 0.0002,
          "batch_size": 16,
          "epochs": 3,
          "optimizer": "adamw",
          "warmup_steps": 200,
          "weight_decay": 0.01,
          "gradient_clip": 1.0,
          "scheduler": "cosine",
          "seed": 42,
          "additional_params": "{\n  \"finetune\": \"LoRA\",\n  \"lora_r\": 16,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.05,\n  \"target_modules\": [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n  \"precision\": \"bf16\",\n  \"max_seq_len\": 1024,\n  \"train_subset\": {\"train\": 2000, \"dev\": 500},\n  \"teacher_cot\": {\n    \"generation\": \"self-distillation from same base model\",\n    \"max_new_tokens_rationale\": 256,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"seed\": 123\n  },\n  \"ci_uf_kcot\": {\n    \"unk_id\": \"tokenizer.unk_token_id\",\n    \"topk_influence\": 8,\n    \"use_keypoints\": true,\n    \"keypoint_heuristic\": \"upweight digits and arithmetic operators in the rationale\",\n    \"influence_corruption\": \"replace selected token with [UNK]\"\n  },\n  \"grad_accum_steps\": 8,\n  \"eval_every_steps\": 200\n}"
        },
        "optuna_config": {
          "enabled": true,
          "n_trials": 12,
          "search_spaces": [
            {
              "param_name": "topk_influence",
              "distribution_type": "categorical",
              "choices": [
                4,
                6,
                8
              ]
            },
            {
              "param_name": "learning_rate",
              "distribution_type": "loguniform",
              "low": 5e-05,
              "high": 0.0005
            },
            {
              "param_name": "lora_r",
              "distribution_type": "categorical",
              "choices": [
                8,
                16,
                32
              ]
            },
            {
              "param_name": "use_keypoints",
              "distribution_type": "categorical",
              "choices": [
                0,
                1
              ]
            }
          ]
        }
      }
    ]
  }
}